{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Full-fine tuning BERT\n",
        "\n",
        "In this exercise, you will create a BERT sentiment classifier (actually DistilBERT) using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to complete a full fine-tuning and evaluate your model.\n",
        "\n",
        "The IMDB dataset contains movie reviews that are labeled as either positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the required version of datasets in case you have an older version\n",
        "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
        "! pip install -q \"datasets==2.15.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the sms_spam dataset\n",
        "# See: https://huggingface.co/datasets/sms_spam\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
        "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
        "    test_size=0.2, shuffle=True, seed=23\n",
        ")\n",
        "\n",
        "splits = [\"train\", \"test\"]\n",
        "\n",
        "# View the dataset characteristics\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the first example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the first example. Do you think this is spam or not?\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-process datasets\n",
        "\n",
        "Now we are going to process our datasets by converting all the text into tokens for our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Let's use a lambda function to tokenize all the examples\n",
        "tokenized_dataset = {}\n",
        "for split in splits:\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
        "    )\n",
        "\n",
        "# Inspect the available columns in the dataset\n",
        "tokenized_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and set up the model\n",
        "\n",
        "In this case we are doing a full fine tuning, so we will want to unfreeze all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace <MASK> with the code to unfreeze all the model parameters\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},\n",
        ")\n",
        "\n",
        "# Unfreeze all the model parameters.\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "for param in model.parameters():\n",
        "    <MASK>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's train it!\n",
        "\n",
        "Now it's time to train our model. We'll use the `Trainer` class.\n",
        "\n",
        "First we'll define a function to compute our accuracy metreic then we make the `Trainer`.\n",
        "\n",
        "In this instance, we will fill in some of the training arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace <MASK> with the Training Arguments of your choice\n",
        "\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "\n",
        "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
        "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./data/spam_not_spam\",\n",
        "        # Set the learning rate\n",
        "        <MASK>\n",
        "        # Set the per device train batch size and eval batch size\n",
        "        <MASK>\n",
        "        <MASK>\n",
        "        # Evaluate and save the model after each epoch\n",
        "        <MASK>\n",
        "        <MASK>\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "    ),\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Evaluating the model is as simple as calling the evaluate method on the trainer object. This will run the model on the test set and compute the metrics we specified in the compute_metrics function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the performance of the model on the test set\n",
        "# What do you think the evaluation accuracy will be?\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View the results\n",
        "\n",
        "Let's look at a few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a dataframe with the predictions and the text and the labels\n",
        "import pandas as pd\n",
        "\n",
        "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
        "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
        ")\n",
        "\n",
        "results = trainer.predict(items_for_manual_review)\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
        "        \"predictions\": results.predictions.argmax(axis=1),\n",
        "        \"labels\": results.label_ids,\n",
        "    }\n",
        ")\n",
        "# Show all the cell\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of the exercise\n",
        "\n",
        "Great work! Congrats on making it to the end of the exercise!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
