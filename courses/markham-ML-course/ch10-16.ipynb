{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: [Master Machine Learning with scikit-learn](https://courses.dataschool.io/view/courses/master-machine-learning-with-scikit-learn)\n",
    "\n",
    "## Chapters 10-16\n",
    "\n",
    "*Â© 2024 Data School. All rights reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Evaluating and tuning a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Evaluating a Pipeline with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain')\n",
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/MLnewdata')\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "imp_constant = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "ohe = OneHotEncoder()\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ohe = make_pipeline(imp_constant, ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps of 5-fold cross-validation on a Pipeline:**\n",
    "\n",
    "1. Split data into 5 folds (A, B, C, D, E)\n",
    "  - ABCD is training set\n",
    "  - E is testing set\n",
    "2. Pipeline is fit on training set\n",
    "  - ABCD is transformed\n",
    "  - Model is fit on transformed data\n",
    "3. Pipeline makes predictions on testing set\n",
    "  - E is transformed (using step 2 transformations)\n",
    "  - Model makes predictions on transformed data\n",
    "4. Calculate accuracy of those predictions\n",
    "5. Repeat the steps above 4 more times, with a different testing set each time\n",
    "6. Calculate the mean of the 5 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does cross_val_score split the data first?**\n",
    "\n",
    "- **Proper cross-validation:**\n",
    "  - Data is split (step 1) before transformations (steps 2 and 3)\n",
    "  - Imputation values and vocabulary are computed using training set only\n",
    "  - Prevents data leakage\n",
    "- **Improper cross-validation:**\n",
    "  - Transformations are performed before data is split\n",
    "  - Imputation values and vocabulary are computed using full dataset\n",
    "  - Causes data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Tuning a Pipeline with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics terminology:**\n",
    "\n",
    "- **Hyperparameters:** Values that you set\n",
    "  - **Example:** C value of logistic regression\n",
    "- **Parameters:** Values learned from the data\n",
    "  - **Example:** Coefficients of logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn terminology:**\n",
    "\n",
    "- **Hyperparameter tuning:** Tuning a model or a Pipeline\n",
    "- **Parameter:** Anything passed to a class\n",
    "  - **LogisticRegression:** C, random_state\n",
    "  - **SimpleImputer:** strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning with GridSearchCV:**\n",
    "\n",
    "- You define which values to try for each parameter\n",
    "- It cross-validates every combination of those values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of tuning a Pipeline:**\n",
    "\n",
    "- Tunes the model and transformers simultaneously\n",
    "- Prevents data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression tuning parameters:**\n",
    "\n",
    "- **penalty:** Type of regularization\n",
    "  - 'l1'\n",
    "  - 'l2' (default)\n",
    "- **C:** Amount of regularization\n",
    "  - 0.1\n",
    "  - 1 (default)\n",
    "  - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter dictionary for GridSearchCV:**\n",
    "\n",
    "- **Key:** step__parameter\n",
    "  - 'logisticregression__penalty'\n",
    "  - 'logisticregression__C'\n",
    "- **Value:** List of values to try\n",
    "  - ['l1', 'l2']\n",
    "  - [0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params['logisticregression__C'] = [0.1, 1, 10]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (2 parameters):** 0.818 ðŸ‘ˆ\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Tuning the transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for expanding the grid search:**\n",
    "\n",
    "- **Initial idea:** Set C=10 and penalty='l1', then only search transformer parameters\n",
    "- **Better approach:** Search for best combination of C, penalty, and transformer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer'].named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder tuning parameter:**\n",
    "\n",
    "- **drop:** Method for dropping a column of each feature\n",
    "  - None (default)\n",
    "  - 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__pipeline__onehotencoder__drop'] = [None, 'first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pipe.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer tuning parameter:**\n",
    "\n",
    "- **ngram_range:** Selection of word n-grams to be extracted as features\n",
    "  - (1, 1) (default)\n",
    "  - (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SimpleImputer tuning parameter:**\n",
    "\n",
    "- **add_indicator:** Option to add a missing indicator column\n",
    "  - False (default)\n",
    "  - True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__simpleimputer__add_indicator'] = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828 ðŸ‘ˆ\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Using the best Pipeline to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Q&A: How do I save the best Pipeline for future use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'rb') as f:\n",
    "    pipe_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_pickle.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid.best_estimator_, 'pipe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_joblib = joblib.load('pipe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_joblib.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnings for pickle and joblib objects:**\n",
    "\n",
    "- May be version-specific and architecture-specific\n",
    "- Can be poisoned with malicious code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatives to pickle and joblib:**\n",
    "\n",
    "- Examples: ONNX, PMML\n",
    "- Save a model representation for making predictions\n",
    "- Work across environments and architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Q&A: How do I speed up a grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Q&A: How do I tune a Pipeline with randomized search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_params = params.copy()\n",
    "more_params['logisticregression__C'] = [0.01, 0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use RandomizedSearchCV:**\n",
    "\n",
    "- **n_iter:** Specify the number of randomly-chosen parameter combinations to cross-validate\n",
    "- **random_state:** Set to any integer for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand = RandomizedSearchCV(pipe, more_params, cv=5, scoring='accuracy', n_iter=10,\n",
    "                          random_state=1, n_jobs=-1)\n",
    "rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rand.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827 ðŸ‘ˆ\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use RandomizedSearchCV instead of GridSearchCV?**\n",
    "\n",
    "- Similar results in far less time\n",
    "- Easier to control the computational budget\n",
    "- Freedom to tune many more parameters\n",
    "- Can use a much finer grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.linspace(0, 1, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-2, 3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Q&A: What's the target accuracy we are trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When is a model \"good enough\"?**\n",
    "\n",
    "- **Useful model:** Outperforms null accuracy\n",
    "- **Best possible model:** Usually impossible to know the theoretical maximum accuracy\n",
    "- **Practical model:** Continue improving until you run out of resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Null model:** 0.616 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Q&A: Is it okay that our model includes thousands of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer'].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811 ðŸ‘ˆ\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828 ðŸ‘ˆ\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_ct.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_pipe = make_pipeline(no_name_ct, logreg)\n",
    "cross_val_score(no_name_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Baseline excluding Name (no tuning):** 0.783 ðŸ‘ˆ\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we learn?**\n",
    "\n",
    "- Name column contains more predictive signal than noise\n",
    "- More features than samples does not necessarily result in overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.11 Q&A: How do I examine the coefficients of a Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.12 Q&A: Should I split the dataset before tuning the Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals of a grid search:**\n",
    "\n",
    "- Choose the best parameters for the Pipeline\n",
    "- Estimate its performance on new data when using these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it okay to use the same data for both goals?**\n",
    "\n",
    "- **Yes:** If your main objective is to choose the best parameters\n",
    "- **No:** If you need a realistic estimate of performance on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "training_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Grid search (estimate for new data):** 0.816 ðŸ‘ˆ\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Baseline excluding Name (no tuning):** 0.783\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = training_grid.best_estimator_\n",
    "best_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guidelines for using this process:**\n",
    "\n",
    "- **Only use the testing set once:**\n",
    "  - If used multiple times, performance estimates will become less reliable\n",
    "- **You must have enough data:**\n",
    "  - If training set is too small, grid search won't find the optimal parameters\n",
    "  - If testing set is too small, it won't provide a reliable performance estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.13 Q&A: What is regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief explanation of regularization:**\n",
    "\n",
    "- Constrains the size of model coefficients to minimize overfitting\n",
    "- Reduces the variance of an overly complex model to help the model generalize\n",
    "- Decreases model flexibility so that it follows the true patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Comparing linear and non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Trying a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest model:**\n",
    "\n",
    "- Non-linear model\n",
    "- Based on decision trees\n",
    "- Different properties from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = make_pipeline(ct, rf)\n",
    "rf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Tuning random forests with randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = params.copy()\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rf_params['logisticregression__penalty']\n",
    "del rf_params['logisticregression__C']\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_params = {k:v for k, v in params.items() if k.startswith('col')}\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two-step approach to hyperparameter tuning:**\n",
    "\n",
    "1. **Randomized search:** Test a variety of parameters and values, then examine the results for trends\n",
    "2. **Grid search:** Use an optimized set of parameters and values based on what you learned from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestClassifier tuning parameters:**\n",
    "\n",
    "- **n_estimators:** Number of decisions trees in the forest\n",
    "  - 100 (default)\n",
    "  - 300\n",
    "  - 500\n",
    "  - 700\n",
    "- **min_samples_leaf:** Minimum number of samples at a leaf node\n",
    "  - 1 (default)\n",
    "  - 2\n",
    "  - 3\n",
    "- **max_features:** Number of features to consider when choosing a split\n",
    "  - 'sqrt' (default)\n",
    "  - None\n",
    "- **bootstrap:** Whether bootstrap samples are used when building trees\n",
    "  - True (default)\n",
    "  - False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params['randomforestclassifier__n_estimators'] = [100, 300, 500, 700]\n",
    "rf_params['randomforestclassifier__min_samples_leaf'] = [1, 2, 3]\n",
    "rf_params['randomforestclassifier__max_features'] = ['sqrt', None]\n",
    "rf_params['randomforestclassifier__bootstrap'] = [True, False]\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 5 minutes\n",
    "rf_rand = RandomizedSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy',\n",
    "                             n_iter=100, random_state=1, n_jobs=-1)\n",
    "%time rf_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rand.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Randomized search (RF):** 0.825 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Further tuning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rf_rand.cv_results_)\n",
    "results.sort_values('rank_test_score').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trends in the randomized search results:**\n",
    "\n",
    "- **n_estimators:**\n",
    "  - Higher numbers are performing better\n",
    "  - Remove 100, add 900\n",
    "- **min_samples_leaf:**\n",
    "  - Higher numbers are performing better\n",
    "  - Remove 1, add 4 and 5\n",
    "- **max_features:**\n",
    "  - None is performing better\n",
    "  - Remove 'sqrt'\n",
    "- **bootstrap:**\n",
    "  - True is performing better\n",
    "  - Remove False\n",
    "- **Transformer parameters:**\n",
    "  - No clear trends\n",
    "  - Leave as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params['randomforestclassifier__n_estimators'] = [300, 500, 700, 900]\n",
    "rf_params['randomforestclassifier__min_samples_leaf'] = [2, 3, 4, 5]\n",
    "rf_params['randomforestclassifier__max_features'] = [None]\n",
    "rf_params['randomforestclassifier__bootstrap'] = [True]\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 10 minutes\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time rf_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829 ðŸ‘ˆ\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Randomized search (RF):** 0.825\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Q&A: How do I tune two models with a single grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pipe = Pipeline([('preprocessor', ct), ('classifier', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {}\n",
    "params1['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params1['classifier__penalty'] = ['l1', 'l2']\n",
    "params1['classifier__C'] = [0.1, 1, 10]\n",
    "params1['classifier'] = [logreg]\n",
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {}\n",
    "params2['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params2['classifier__n_estimators'] = [300, 500]\n",
    "params2['classifier__min_samples_leaf'] = [3, 4]\n",
    "params2['classifier'] = [rf]\n",
    "params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "both_params = [params1, params2]\n",
    "both_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_grid = GridSearchCV(both_pipe, both_params, cv=5, scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "both_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(both_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "both_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extensions of this approach:**\n",
    "\n",
    "- Tune different preprocessing parameters for each model\n",
    "- Tune two different preprocessor objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Q&A: How do I tune two models with a single randomized search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_rand = RandomizedSearchCV(both_pipe, both_params, cv=5, scoring='accuracy',\n",
    "                               n_iter=10, random_state=1, n_jobs=-1)\n",
    "both_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(both_rand.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Ensembling multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Introduction to ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to create an ensemble:**\n",
    "\n",
    "- **Regression:** Average the predictions\n",
    "- **Classification:** Average the predicted probabilities, or let the classifiers vote on the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does ensembling work?**\n",
    "\n",
    "- \"One-off\" errors made by each model will be discarded when ensembling\n",
    "- Ensemble has a lower variance than any individual model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Ensembling logistic regression and random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)\n",
    "pipe = make_pipeline(ct, logreg)\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, n_jobs=-1)\n",
    "rf_pipe = make_pipeline(ct, rf)\n",
    "cross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR):** 0.811 ðŸ‘ˆ\n",
    "- **Baseline (RF):** 0.811 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voting options for VotingClassifier:**\n",
    "\n",
    "- **soft:** Average the predicted probabilities\n",
    "- **hard:** Majority vote using class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vc_pipe = make_pipeline(ct, vc)\n",
    "vc_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Combining predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)\n",
    "pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.fit(X, y)\n",
    "rf_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.fit(X, y)\n",
    "vc_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.predict(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC soft voting):** 0.818 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Combining class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='hard', n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820 ðŸ‘ˆ\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is this result misleading?**\n",
    "\n",
    "- In the case of a tie, hard voting always chooses class 0\n",
    "- Thus hard voting is performing better than soft voting by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Choosing a voting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft voting vs hard voting:**\n",
    "\n",
    "- **Soft voting:**\n",
    "  - Preferred if you have an even number of models (especially two)\n",
    "  - Preferred if all models are well-calibrated\n",
    "  - Only works if all models have the predict_proba method\n",
    "- **Hard voting:**\n",
    "  - Preferred if some models are not well-calibrated\n",
    "  - Does not require the predict_proba method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft', n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6 Tuning an ensemble with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params = {k:v for k, v in params.items() if k.startswith('col')}\n",
    "vc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.named_steps['votingclassifier'].named_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params['votingclassifier__clf1__penalty'] = ['l1', 'l2']\n",
    "vc_params['votingclassifier__clf1__C'] = [1, 10]\n",
    "vc_params['votingclassifier__clf2__n_estimators'] = [100, 300]\n",
    "vc_params['votingclassifier__clf2__min_samples_leaf'] = [2, 3]\n",
    "vc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 1 minute\n",
    "vc_grid = GridSearchCV(vc_pipe, vc_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time vc_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC soft voting):** 0.834 ðŸ‘ˆ\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7 Q&A: When should I use ensembling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should you ensemble?**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Improves model performance\n",
    "- **Disadvantages:**\n",
    "  - Adds more complexity\n",
    "  - Decreases interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice for ensembling:**\n",
    "\n",
    "- Include at least 3 models\n",
    "- Models should be performing well on their own\n",
    "- Ideal if they generate predictions using different processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.8 Q&A: How do I apply different weights to the models in an ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft',\n",
    "                      weights=[2, 1], n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.fit(X, y)\n",
    "vc_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC soft voting):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (VC soft voting with LR weighted):** 0.816 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params['votingclassifier__weights'] = [(1, 1), (2, 1), (1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Introduction to feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential benefits of feature selection:**\n",
    "\n",
    "- Higher accuracy\n",
    "- Greater interpretability\n",
    "- Faster training\n",
    "- Lower costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection methods:**\n",
    "\n",
    "- Human intuition\n",
    "- Domain knowledge\n",
    "- Data exploration\n",
    "- Automated methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods for automated feature selection:**\n",
    "\n",
    "- Intrinsic methods\n",
    "- Filter methods\n",
    "- Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Intrinsic methods: L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are intrinsic methods?**\n",
    "\n",
    "- Feature selection happens automatically during model building\n",
    "- Also called: implicit methods, embedded methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression tuning parameters:**\n",
    "\n",
    "- **penalty:** Type of regularization\n",
    "- **C:** Amount of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does L1 regularization do feature selection?**\n",
    "\n",
    "- Regularization shrinks model coefficients to help the model to generalize\n",
    "- L1 regularization shrinks some coefficients to zero, which removes those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['logisticregression'].coef_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(grid.best_estimator_.named_steps['logisticregression'].coef_[0] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['logisticregression'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pipe.named_steps['logisticregression'].coef_[0] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages and disadvantages of intrinsic methods:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - No added computation\n",
    "  - No added steps\n",
    "- **Disadvantages:**\n",
    "  - Model-dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Filter methods: Statistical test-based scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How filter methods work:**\n",
    "\n",
    "1. Each feature is scored by its relationship to the target\n",
    "2. Top scoring features (most informative features) are provided to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811 ðŸ‘ˆ\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How SelectPercentile works:**\n",
    "\n",
    "1. Scores each feature using the statistical test you specify\n",
    "2. Passes to the model the percentage of features you specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "selection = SelectPercentile(chi2, percentile=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe = make_pipeline(ct, selection, logreg)\n",
    "fs_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectPercentile):** 0.819 ðŸ‘ˆ\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SelectPercentile vs SelectKBest:**\n",
    "\n",
    "- **SelectPercentile:** Specify percentage of features to keep\n",
    "- **SelectKBest:** Specify number of features to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Filter methods: Model-based scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How SelectFromModel works:**\n",
    "\n",
    "1. Scores each feature using the model you specify\n",
    "  - Model is fit on all features\n",
    "  - Coefficients or feature importances are used as scores\n",
    "2. Passes to the prediction model features that score above a threshold you specify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models that can be used by SelectFromModel:**\n",
    "\n",
    "- Logistic regression\n",
    "- Linear SVC\n",
    "- Tree-based models\n",
    "- Any other model with coefficients or feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_selection = LogisticRegression(solver='liblinear', penalty='l1',\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "selection = SelectFromModel(logreg_selection, threshold='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe = make_pipeline(ct, selection, logreg)\n",
    "fs_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectFromModel LR):** 0.826 ðŸ‘ˆ\n",
    "- **Baseline (LR with SelectPercentile):** 0.819\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et_selection = ExtraTreesClassifier(n_estimators=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = SelectFromModel(et_selection, threshold='mean')\n",
    "fs_pipe = make_pipeline(ct, selection, logreg)\n",
    "fs_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectFromModel LR):** 0.826\n",
    "- **Baseline (LR with SelectPercentile):** 0.819\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR with SelectFromModel ET):** 0.815 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_params = params.copy()\n",
    "fs_params['selectfrommodel__threshold'] = ['mean', '1.5*mean', -np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 1 minute\n",
    "fs_grid = GridSearchCV(fs_pipe, fs_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time fs_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832 ðŸ‘ˆ\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectFromModel LR):** 0.826\n",
    "- **Baseline (LR with SelectPercentile):** 0.819\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR with SelectFromModel ET):** 0.815\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Filter methods: Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages and disadvantages of filter methods:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Runs quickly (usually)\n",
    "- **Disadvantages:**\n",
    "  - Scores are not always correlated with predictive value\n",
    "  - Scores are calculated only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6 Wrapper methods: Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter methods vs wrapper methods:**\n",
    "\n",
    "- **Filter methods:** Features are scored once\n",
    "- **Wrapper methods:** Features are scored multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How RFE works:**\n",
    "\n",
    "1. Scores each feature using the model you specify\n",
    "  - Model is fit on all features\n",
    "  - Coefficients or feature importances are used as scores\n",
    "2. Removes the single worst scoring feature\n",
    "3. Repeats steps 1 and 2 until it reaches the number of features you specify\n",
    "4. Passes the remaining features to the prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SelectFromModel vs RFE:**\n",
    "\n",
    "- **SelectFromModel:** Scores your features a single time\n",
    "- **RFE:** Scores your features many times\n",
    "  - More computationally expensive\n",
    "  - May better capture the relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "selection = RFE(logreg_selection, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe = make_pipeline(ct, selection, logreg)\n",
    "fs_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectFromModel LR):** 0.826\n",
    "- **Baseline (LR with SelectPercentile):** 0.819\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR with SelectFromModel ET):** 0.815\n",
    "- **Baseline (LR with RFE LR):** 0.814 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_params = params.copy()\n",
    "fs_params['rfe__n_features_to_select'] = [None, 500]\n",
    "fs_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 2 minutes\n",
    "fs_grid = GridSearchCV(fs_pipe, fs_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time fs_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with SelectFromModel LR):** 0.826\n",
    "- **Grid search (LR with RFE LR):** 0.822 ðŸ‘ˆ\n",
    "- **Baseline (LR with SelectPercentile):** 0.819\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR with SelectFromModel ET):** 0.815\n",
    "- **Baseline (LR with RFE LR):** 0.814\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages and disadvantages of RFE:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Captures the relationships between features\n",
    "- **Disadvantages:**\n",
    "  - Scores are not always correlated with predictive value\n",
    "  - Computationally expensive\n",
    "  - Does not look ahead when removing features (\"greedy\" approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.7 Q&A: How do I see which features were selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe[0].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe[0:2].fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe[1].get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fs_pipe[1].get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe[1].get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pipe[0].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.8 Q&A: Are the selected features the \"most important\" features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection vs feature importance:**\n",
    "\n",
    "- Multiple sets of features may perform similarly\n",
    "- Especially likely if there are many more features than samples (\"p >> n\")\n",
    "- Thus, feature selection does not necessarily determine feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.9 Q&A: Is it okay for feature selection to remove one-hot encoded categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection of one-hot encoded categories:**\n",
    "\n",
    "- Feature selection examines each feature column independently (regardless of its \"origin\")\n",
    "- Each one-hot encoded column is conceptually independent from the others\n",
    "- Thus, it's acceptable for feature selection to ignore the origin of each column when removing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Standardizing numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is feature standardization useful?**\n",
    "\n",
    "- Some models assume that features are centered around zero and have similar variances\n",
    "- Those models may perform poorly if that assumption is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_scaler = make_pipeline(imp, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp_scaler, ['Age', 'Fare', 'Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pipe = make_pipeline(ct, logreg)\n",
    "scaler_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(scaler_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811\n",
    "- **Baseline (LR with numerical features standardized):** 0.810 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why didn't feature standardization help?**\n",
    "\n",
    "- Regularized linear models often benefit from standardization\n",
    "- However, the liblinear solver is robust to unscaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Standardizing all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not use StandardScaler?**\n",
    "\n",
    "- Our ColumnTransformer outputs a sparse matrix\n",
    "- Centering would cause memory issues by creating a dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pipe = make_pipeline(ct, scaler, logreg)\n",
    "scaler_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(scaler_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (LR with all features standardized):** 0.811 ðŸ‘ˆ\n",
    "- **Baseline (RF):** 0.811\n",
    "- **Baseline (LR with numerical features standardized):** 0.810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Q&A: How do I see what scaling was applied to each feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pipe.fit(X, y)\n",
    "scaler_pipe.named_steps['maxabsscaler'].scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Q&A: How do I turn off feature standardization within a grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_params = {}\n",
    "scaler_params['logisticregression__C'] = [0.1, 1, 10]\n",
    "scaler_params['maxabsscaler'] = ['passthrough', MaxAbsScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_grid = GridSearchCV(scaler_pipe, scaler_params, cv=5, scoring='accuracy',\n",
    "                           n_jobs=-1)\n",
    "scaler_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 Q&A: Which models benefit from standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When is feature standardization likely to be useful?**\n",
    "\n",
    "- **Useful:**\n",
    "  - Distance-based models (KNN, SVM)\n",
    "  - Regularized models (linear or logistic regression with L1/L2)\n",
    "- **Not useful:**\n",
    "  - Tree-based models (random forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Feature engineering with custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 Why not use pandas for feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for feature engineering:**\n",
    "\n",
    "- **pandas:** Create features on original dataset, pass updated dataset to scikit-learn\n",
    "- **scikit-learn:** Create features using custom transformers\n",
    "  - Requires more work\n",
    "  - All transformations can be included in a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Transformer 1: Rounding numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(df[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "ceiling = FunctionTransformer(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling.fit_transform(df[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ceiling, ['Fare']))\n",
    "ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Transformer 2: Clipping numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(df[['Age']], a_min=5, a_max=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = FunctionTransformer(np.clip, kw_args={'a_min':5, 'a_max':60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.fit_transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ceiling, ['Fare']),\n",
    "    (clip, ['Age']))\n",
    "ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Transformer 3: Extracting string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].str.slice(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Cabin']].apply(lambda x: x.str.slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter(df):\n",
    "    return df.apply(lambda x: x.str.slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter(df):\n",
    "    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter(df[['Cabin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = FunctionTransformer(first_letter)\n",
    "letter.fit_transform(df[['Cabin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ceiling, ['Fare']),\n",
    "    (clip, ['Age']),\n",
    "    (letter, ['Cabin']))\n",
    "ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 Rules for transformer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input and output of transformer functions:**\n",
    "\n",
    "- **Input:**\n",
    "  - 1D is allowed\n",
    "  - 2D is preferred: Enables it to accept multiple columns\n",
    "- **Output:**\n",
    "  - 2D is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 Transformer 4: Combining two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['SibSp', 'Parch']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df[['SibSp', 'Parch']]).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_cols(df):\n",
    "    return np.array(df).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cols(df[['SibSp', 'Parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = FunctionTransformer(sum_cols)\n",
    "total.fit_transform(df[['SibSp', 'Parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ceiling, ['Fare']),\n",
    "    (clip, ['Age']),\n",
    "    (letter, ['Cabin']),\n",
    "    (total, ['SibSp', 'Parch']))\n",
    "ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7 Revising the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ceiling, ['Fare']),\n",
    "    (clip, ['Age']),\n",
    "    (letter, ['Cabin']),\n",
    "    (total, ['SibSp', 'Parch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issues to handle when updating the ColumnTransformer:**\n",
    "\n",
    "1. **Cabin** and **SibSp** weren't originally included\n",
    "2. **Fare** and **Age** have missing values\n",
    "3. **Cabin** is non-numeric and has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin', 'SibSp']\n",
    "X = df[cols]\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ceiling = make_pipeline(imp, ceiling)\n",
    "imp_clip = make_pipeline(imp, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X['Cabin'].str.slice(0, 1).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are rare categories problematic for cross-validation?**\n",
    "\n",
    "- Rare category values may all show up in the same testing fold\n",
    "- The rare category won't be learned during fit and will be treated as an unknown category\n",
    "- OneHotEncoder will error when it encounters an unknown category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_ignore = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_imp_ohe = make_pipeline(letter, imp_constant, ohe_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp_ceiling, ['Fare']),\n",
    "    (imp_clip, ['Age']),\n",
    "    (letter_imp_ohe, ['Cabin']),\n",
    "    (total, ['SibSp', 'Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC):** 0.834\n",
    "- **Grid search (LR with SelectFromModel ET):** 0.832\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR with more features):** 0.826 ðŸ‘ˆ\n",
    "- **Baseline (VC):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)\n",
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Q&A: How do I fix incorrect data types within a Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.DataFrame({'A': ['10', '20', '30'],\n",
    "                     'B': ['40', '50', '60'],\n",
    "                     'C': [70, 80, 90],\n",
    "                     'D': ['x', 'y', 'z']})\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[['A', 'B']].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[['A', 'B']].astype('int').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_integer(df):\n",
    "    return pd.DataFrame(df).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer = FunctionTransformer(make_integer)\n",
    "integer.fit_transform(demo[['A', 'B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer.fit_transform(demo[['A', 'B']]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.loc[2, 'B'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer.fit_transform(demo[['A', 'B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[['A', 'B']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[['A', 'B']].apply(pd.to_numeric).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_number(df):\n",
    "    return pd.DataFrame(df).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = FunctionTransformer(make_number)\n",
    "number.fit_transform(demo[['A', 'B']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9 Q&A: How do I create features from datetime data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('http://bit.ly/ufosample', parse_dates=['Date'])\n",
    "ufo = pd.read_csv('ufo.csv', parse_dates=['Date'])\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_month(df):\n",
    "    return df.apply(lambda x: x.dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_month(ufo[['Date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_month(df):\n",
    "    return pd.DataFrame(df).apply(lambda x: pd.to_datetime(x).dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_month(df):\n",
    "    return pd.DataFrame(df, dtype=np.datetime64).apply(lambda x: x.dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('http://bit.ly/ufosample')\n",
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_month(ufo[['Date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = FunctionTransformer(day_of_month)\n",
    "day.fit_transform(ufo[['Date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.10 Q&A: How do I create feature interactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When are feature interactions useful?**\n",
    "\n",
    "- When the combined impact of features is different from their independent impacts\n",
    "- **Example:**\n",
    "  - A and B (individually) each have a small positive impact\n",
    "  - A and B (combined) has a larger positive impact than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['Fare', 'SibSp', 'Parch']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(include_bias=False, interaction_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.fit_transform(X[['Fare', 'SibSp', 'Parch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output columns:**\n",
    "\n",
    "1. Fare\n",
    "2. SibSp\n",
    "3. Parch\n",
    "4. Fare * SibSp\n",
    "5. Fare * Parch\n",
    "6. SibSp * Parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to choose feature interactions:**\n",
    "\n",
    "- Use expert knowledge\n",
    "- Explore the data\n",
    "- Create all possible interactions\n",
    "  - Not practical with a large number of features\n",
    "  - Increases risk of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When are feature interactions not useful?**\n",
    "\n",
    "- Tree-based models can learn feature interactions on their own\n",
    "- Linear models can sometimes replace the information supplied by interaction terms\n",
    "- **Conclusion:** Evaluate the model with and without interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.11 Q&A: How do I save a Pipeline with custom transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter(df):\n",
    "    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_cols(df):\n",
    "    return np.array(df).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pipe.pickle', 'rb') as f:\n",
    "    pipe_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin', 'SibSp']\n",
    "df_new = pd.read_csv('http://bit.ly/MLnewdata')\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_pickle.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "with open('pipe.pickle', 'wb') as f:\n",
    "    cloudpickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'rb') as f:\n",
    "    pipe_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_pickle.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.12 Q&A: Can FunctionTransformer be used with any transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stateless transformations:**\n",
    "\n",
    "- **ceiling:** Rounding up to the next integer\n",
    "- **clip:** Limiting values to a range\n",
    "- **letter:** Extracting the first letter\n",
    "- **total:** Adding two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stateful transformations:**\n",
    "\n",
    "- **OneHotEncoder:** fit learns the categories\n",
    "- **CountVectorizer:** fit learns the vocabulary\n",
    "- **SimpleImputer:** fit learns the value to impute\n",
    "- **MaxAbsScaler:** fit learns the scale of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Workflow review #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 Recap of our workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin', 'SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain')\n",
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/MLnewdata')\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "imp_constant = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "ohe = OneHotEncoder()\n",
    "ohe_ignore = OneHotEncoder(handle_unknown='ignore')\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter(df):\n",
    "    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_cols(df):\n",
    "    return np.array(df).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling = FunctionTransformer(np.ceil)\n",
    "clip = FunctionTransformer(np.clip, kw_args={'a_min':5, 'a_max':60})\n",
    "letter = FunctionTransformer(first_letter)\n",
    "total = FunctionTransformer(sum_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ohe = make_pipeline(imp_constant, ohe)\n",
    "imp_ceiling = make_pipeline(imp, ceiling)\n",
    "imp_clip = make_pipeline(imp, clip)\n",
    "letter_imp_ohe = make_pipeline(letter, imp_constant, ohe_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp_ceiling, ['Fare']),\n",
    "    (imp_clip, ['Age']),\n",
    "    (letter_imp_ohe, ['Cabin']),\n",
    "    (total, ['SibSp', 'Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe.fit(X, y)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 What's the role of pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uses for pandas in the data science workflow:**\n",
    "\n",
    "- **All projects:** Data exploration and visualization\n",
    "- **ML projects:** Testing out data transformations\n",
    "- **Non-ML projects:** Executing data transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
