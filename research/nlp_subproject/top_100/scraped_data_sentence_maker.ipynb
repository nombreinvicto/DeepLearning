{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# normal nlp initialisations\n",
    "stopwords_english = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_dir_raw = f\"{cwd}//combined_dataset\"\n",
    "dump_dir = f\"{data_dir_raw}//mcmaster_processed_lemmatised\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "part_types = ['bearing', 'bolt', 'collet', 'spring', 'sprocket']\n",
    "\n",
    "for part_type in part_types:\n",
    "\n",
    "    with open(f\"{data_dir_raw}//{part_type}.txt\", 'r') as f:\n",
    "        contents = f.read()\n",
    "    lines = contents.split(\"\\n\")\n",
    "\n",
    "    # get the sentences\n",
    "    sentences = []\n",
    "    for line in lines: # each line can be multiple sentences\n",
    "        temp_sentences = line.split(\".\")\n",
    "\n",
    "        #\n",
    "        for sent_ in temp_sentences:\n",
    "            if sent_ and len(sent_) > 2:\n",
    "                # remove any leading space\n",
    "                sent_ = sent_.lstrip(\" \").lower()\n",
    "                words_in_sent_ = sent_.split(\" \")\n",
    "\n",
    "                # now remove stopwords and punctuation from sentence\n",
    "                words_to_allow_from_sent_ = []\n",
    "                for word in words_in_sent_:\n",
    "                    if (word not in stopwords_english and word not in punctuation):\n",
    "                        words_to_allow_from_sent_.append(word)\n",
    "\n",
    "                # now stem the words\n",
    "                words_to_allow_from_sent_stemmed = []\n",
    "                for word in words_to_allow_from_sent_:\n",
    "                    stem_word = stemmer.stem(word)\n",
    "                    words_to_allow_from_sent_stemmed.append(stem_word)\n",
    "\n",
    "                # finally reconstruct the sentence\n",
    "                final_cleaned_sentence = \" \".join(words_to_allow_from_sent_stemmed)\n",
    "                sentences.append(final_cleaned_sentence)\n",
    "\n",
    "    sentences = np.asarray(sentences)\n",
    "\n",
    "    # now store them in new text file\n",
    "    with open(f\"{dump_dir}//{part_type}.txt\", \"a\") as file:\n",
    "        for sentence in sentences:\n",
    "            file.write(sentence+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}