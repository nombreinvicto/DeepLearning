{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install opencv-python==4.5.5.64\n",
    "! pip install colab-xterm\n",
    "%load_ext colabxterm\n",
    "%xterm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "sys.path.append(r\"/content/drive/MyDrive/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:15:45.213309700Z",
     "start_time": "2023-05-29T02:15:45.182393400Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FCHeadNet' from 'loader_util.nn.conv' (C:\\Users\\mhasa\\GitHub\\deeplearning\\loader_util\\nn\\conv\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18412\\3463297458.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mloader_util\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAspectAwarePreprocessor\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mloader_util\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSimpleDatasetLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mloader_util\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mFCHeadNet\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImageDataGenerator\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRMSprop\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'FCHeadNet' from 'loader_util.nn.conv' (C:\\Users\\mhasa\\GitHub\\deeplearning\\loader_util\\nn\\conv\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from loader_util.preprocessing import ImageToArrayPreprocessor\n",
    "from loader_util.preprocessing import AspectAwarePreprocessor\n",
    "from loader_util.datasets import SimpleDatasetLoader\n",
    "from loader_util.nn.conv import FCHeadNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# script constants\n",
    "dataset_path = r\"\"\n",
    "model_save_dir = r\"\"\n",
    "batch_size = 32\n",
    "num_epochs = 25"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:15:36.176908200Z",
     "start_time": "2023-05-29T02:15:36.118066800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct the imagegenerator\n",
    "aug = ImageDataGenerator(rotation_range=30,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=0.2,\n",
    "                         horizontal_flip=True,\n",
    "                         fill_mode=\"nearest\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:32.763171Z",
     "start_time": "2023-05-29T02:18:32.758184500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"[INFO] loading images......\")\n",
    "image_paths = list(paths.list_images(dataset_path))\n",
    "all_class_names = [pth.split(os.path.sep)[-2] for pth in image_paths]\n",
    "class_names = np.unique(all_class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.028385400Z",
     "start_time": "2023-05-29T02:18:33.024396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init the image processors\n",
    "aap = AspectAwarePreprocessor(224, 224)\n",
    "iap = ImageToArrayPreprocessor()\n",
    "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
    "data, labels = sdl.load(image_paths=image_paths, verbose=500)\n",
    "data = data.astype(\"float\") / 255.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.153182800Z",
     "start_time": "2023-05-29T02:18:33.125261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainx, testx, trainy, testy = train_test_split(data, labels,\n",
    "                                                test_size=0.25,\n",
    "                                                random_state=42)\n",
    "\n",
    "# encode the labels\n",
    "lb = LabelBinarizer()\n",
    "trainy_encoded = lb.fit_transform(trainy)\n",
    "testy_encoded = lb.transform(testy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.326352700Z",
     "start_time": "2023-05-29T02:18:33.322364400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initilise the base model\n",
    "base_model = VGG16(weights=\"imagenet\",\n",
    "                   include_top=False,\n",
    "                   input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "head_model = FCHeadNet.build(base_model=base_model,\n",
    "                             output_classes=len(class_names),\n",
    "                             dense_layer_nodes=[256])\n",
    "model = Model(inputs=base_model.input, outputs=head_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.604871900Z",
     "start_time": "2023-05-29T02:18:33.597890600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.852776300Z",
     "start_time": "2023-05-29T02:18:33.835821700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now train the model\n",
    "print(f\"[INFO] compiling the model......\")\n",
    "opt = RMSprop(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.693691300Z",
     "start_time": "2023-05-29T02:18:33.644764800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training the head for a few epochs\n",
    "print(f\"[INFO] training the head......\")\n",
    "H = model.fit(aug.flow(trainx, trainy, batch_size=batch_size),\n",
    "              validation_data=(testx, testy),\n",
    "              epochs=num_epochs,\n",
    "              steps_per_epoch=len(trainx) // batch_size,\n",
    "              verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:18:33.947060900Z",
     "start_time": "2023-05-29T02:18:33.910645200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testx, batch_size=batch_size)\n",
    "print(classification_report(testy.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), target_names=class_names))\n",
    "\n",
    "# plot the performance\n",
    "epochs = range(1, num_epochs + 1)\n",
    "loss = H.history['loss']\n",
    "accuracy = H.history['accuracy']\n",
    "val_loss = H.history['val_loss']\n",
    "val_accuracy = H.history['val_accuracy']\n",
    "plot_df = pd.DataFrame(data=np.c_[epochs, loss, accuracy, val_loss, val_accuracy],\n",
    "                       columns=['epochs', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "# do the actual plots\n",
    "sns.set(font_scale=1)\n",
    "f, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "sns.lineplot(data=plot_df, x='epochs', y='loss', ax=ax, label='train loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='accuracy', ax=ax, label='train accuracy', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='val_loss', ax=ax, label='val loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='val_accuracy', ax=ax, label='val_accuracy', linewidth=3)\n",
    "ax.set_ylabel('Loss or Accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='18');  # for legend text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:42:42.471007500Z",
     "start_time": "2023-05-29T02:42:42.452951300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now unfreeze the last few layers\n",
    "for layer in base_model.layers[15:]\n",
    "    layer.trainable = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T02:42:42.612744500Z",
     "start_time": "2023-05-29T02:42:42.593794900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now train the model\n",
    "print(f\"[INFO] re-compiling the model......\")\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training the head for a few epochs\n",
    "print(f\"[INFO] fine tuning the model......\")\n",
    "H = model.fit(aug.flow(trainx, trainy, batch_size=batch_size),\n",
    "              validation_data=(testx, testy),\n",
    "              epochs=100,\n",
    "              steps_per_epoch=len(trainx) // batch_size,\n",
    "              verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating network after finetuning...\")\n",
    "predictions = model.predict(testx, batch_size=batch_size)\n",
    "print(classification_report(testy.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), target_names=class_names))\n",
    "\n",
    "# plot the performance\n",
    "epochs = range(1, 100 + 1)\n",
    "loss = H.history['loss']\n",
    "accuracy = H.history['accuracy']\n",
    "val_loss = H.history['val_loss']\n",
    "val_accuracy = H.history['val_accuracy']\n",
    "plot_df = pd.DataFrame(data=np.c_[epochs, loss, accuracy, val_loss, val_accuracy],\n",
    "                       columns=['epochs', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "# do the actual plots\n",
    "sns.set(font_scale=1)\n",
    "f, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "sns.lineplot(data=plot_df, x='epochs', y='loss', ax=ax, label='train loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='accuracy', ax=ax, label='train accuracy', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='val_loss', ax=ax, label='val loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='val_accuracy', ax=ax, label='val_accuracy', linewidth=3)\n",
    "ax.set_ylabel('Loss or Accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='18');"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
