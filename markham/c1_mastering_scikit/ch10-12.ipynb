{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: [Master Machine Learning with scikit-learn](https://courses.dataschool.io/view/courses/master-machine-learning-with-scikit-learn)\n",
    "\n",
    "## Chapters 10-12\n",
    "\n",
    "*Â© 2023 Data School. All rights reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Evaluating and tuning a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Evaluating a Pipeline with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain')\n",
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/MLnewdata')\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "imp_constant = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "ohe = OneHotEncoder()\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ohe = make_pipeline(imp_constant, ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps of 5-fold cross-validation on a Pipeline:**\n",
    "\n",
    "1. Split data into 5 folds (A, B, C, D, E)\n",
    "  - ABCD is training set\n",
    "  - E is testing set\n",
    "2. Pipeline is fit on training set\n",
    "  - ABCD is transformed\n",
    "  - Model is fit on transformed data\n",
    "3. Pipeline makes predictions on testing set\n",
    "  - E is transformed (using step 2 transformations)\n",
    "  - Model makes predictions on transformed data\n",
    "4. Calculate accuracy of those predictions\n",
    "5. Repeat the steps above 4 more times, with a different testing set each time\n",
    "6. Calculate the mean of the 5 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does cross_val_score split the data first?**\n",
    "\n",
    "- **Proper cross-validation:**\n",
    "  - Data is split (step 1) before transformations (steps 2 and 3)\n",
    "  - Imputation values and vocabulary are computed using training set only\n",
    "  - Prevents data leakage\n",
    "- **Improper cross-validation:**\n",
    "  - Transformations are performed before data is split\n",
    "  - Imputation values and vocabulary are computed using full dataset\n",
    "  - Causes data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Tuning a Pipeline with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics terminology:**\n",
    "\n",
    "- **Hyperparameters:** Values that you set\n",
    "  - **Example:** C value of logistic regression\n",
    "- **Parameters:** Values learned from the data\n",
    "  - **Example:** Coefficients of logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn terminology:**\n",
    "\n",
    "- **Hyperparameter tuning:** Tuning a model or a Pipeline\n",
    "- **Parameter:** Anything passed to a class\n",
    "  - **LogisticRegression:** C, random_state\n",
    "  - **SimpleImputer:** strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning with GridSearchCV:**\n",
    "\n",
    "- You define which values to try for each parameter\n",
    "- It cross-validates every combination of those values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of tuning a Pipeline:**\n",
    "\n",
    "- Tunes the model and transformers simultaneously\n",
    "- Prevents data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression tuning parameters:**\n",
    "\n",
    "- **penalty:** Type of regularization\n",
    "  - 'l1'\n",
    "  - 'l2' (default)\n",
    "- **C:** Amount of regularization\n",
    "  - 0.1\n",
    "  - 1 (default)\n",
    "  - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter dictionary for GridSearchCV:**\n",
    "\n",
    "- **Key:** step__parameter\n",
    "  - 'logisticregression__penalty'\n",
    "  - 'logisticregression__C'\n",
    "- **Value:** List of values to try\n",
    "  - ['l1', 'l2']\n",
    "  - [0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params['logisticregression__C'] = [0.1, 1, 10]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (2 parameters):** 0.818 ðŸ‘ˆ\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Tuning the transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for expanding the grid search:**\n",
    "\n",
    "- **Initial idea:** Set C=10 and penalty='l1', then only search transformer parameters\n",
    "- **Better approach:** Search for best combination of C, penalty, and transformer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer'].named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder tuning parameter:**\n",
    "\n",
    "- **drop:** Method for dropping a column of each feature\n",
    "  - None (default)\n",
    "  - 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__pipeline__onehotencoder__drop'] = [None, 'first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pipe.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer tuning parameter:**\n",
    "\n",
    "- **ngram_range:** Selection of word n-grams to be extracted as features\n",
    "  - (1, 1) (default)\n",
    "  - (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SimpleImputer tuning parameter:**\n",
    "\n",
    "- **add_indicator:** Option to add a missing indicator column\n",
    "  - False (default)\n",
    "  - True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__simpleimputer__add_indicator'] = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828 ðŸ‘ˆ\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Using the best Pipeline to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Q&A: How do I save the best Pipeline for future use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pickle', 'rb') as f:\n",
    "    pipe_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_pickle.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid.best_estimator_, 'pipe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_joblib = joblib.load('pipe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_from_joblib.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnings for pickle and joblib objects:**\n",
    "\n",
    "- May be version-specific and architecture-specific\n",
    "- Can be poisoned with malicious code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatives to pickle and joblib:**\n",
    "\n",
    "- Examples: ONNX, PMML\n",
    "- Save a model representation for making predictions\n",
    "- Work across environments and architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Q&A: How do I speed up a grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Q&A: How do I tune a Pipeline with randomized search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_params = params.copy()\n",
    "more_params['logisticregression__C'] = [0.01, 0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use RandomizedSearchCV:**\n",
    "\n",
    "- **n_iter:** Specify the number of randomly-chosen parameter combinations to cross-validate\n",
    "- **random_state:** Set to any integer for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand = RandomizedSearchCV(pipe, more_params, cv=5, scoring='accuracy', n_iter=10,\n",
    "                          random_state=1, n_jobs=-1)\n",
    "rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rand.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827 ðŸ‘ˆ\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use RandomizedSearchCV instead of GridSearchCV?**\n",
    "\n",
    "- Similar results in far less time\n",
    "- Easier to control the computational budget\n",
    "- Freedom to tune many more parameters\n",
    "- Can use a much finer grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.linspace(0, 1, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-2, 3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Q&A: What's the target accuracy we are trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When is a model \"good enough\"?**\n",
    "\n",
    "- **Useful model:** Outperforms null accuracy\n",
    "- **Best possible model:** Usually impossible to know the theoretical maximum accuracy\n",
    "- **Practical model:** Continue improving until you run out of resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Null model:** 0.616 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Q&A: Is it okay that our model includes thousands of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['columntransformer'].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811 ðŸ‘ˆ\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828 ðŸ‘ˆ\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_ct.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name_pipe = make_pipeline(no_name_ct, logreg)\n",
    "cross_val_score(no_name_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Baseline excluding Name (no tuning):** 0.783 ðŸ‘ˆ\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we learn?**\n",
    "\n",
    "- Name column contains more predictive signal than noise\n",
    "- More features than samples does not necessarily result in overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.11 Q&A: How do I examine the coefficients of a Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['columntransformer'].transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.12 Q&A: Should I split the dataset before tuning the Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals of a grid search:**\n",
    "\n",
    "- Choose the best parameters for the Pipeline\n",
    "- Estimate its performance on new data when using these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it okay to use the same data for both goals?**\n",
    "\n",
    "- **Yes:** If your main objective is to choose the best parameters\n",
    "- **No:** If you need a realistic estimate of performance on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "training_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (5 parameters):** 0.828\n",
    "- **Randomized search (more C values):** 0.827\n",
    "- **Grid search (2 parameters):** 0.818\n",
    "- **Grid search (estimate for new data):** 0.816 ðŸ‘ˆ\n",
    "- **Baseline (no tuning):** 0.811\n",
    "- **Baseline excluding Name (no tuning):** 0.783\n",
    "- **Null model:** 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = training_grid.best_estimator_\n",
    "best_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guidelines for using this process:**\n",
    "\n",
    "- **Only use the testing set once:**\n",
    "  - If used multiple times, performance estimates will become less reliable\n",
    "- **You must have enough data:**\n",
    "  - If training set is too small, grid search won't find the optimal parameters\n",
    "  - If testing set is too small, it won't provide a reliable performance estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.13 Q&A: What is regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief explanation of regularization:**\n",
    "\n",
    "- Constrains the size of model coefficients to minimize overfitting\n",
    "- Reduces the variance of an overly complex model to help the model generalize\n",
    "- Decreases model flexibility so that it follows the true patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Comparing linear and non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Trying a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest model:**\n",
    "\n",
    "- Non-linear model\n",
    "- Based on decision trees\n",
    "- Different properties from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = make_pipeline(ct, rf)\n",
    "rf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Tuning random forests with randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = params.copy()\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rf_params['logisticregression__penalty']\n",
    "del rf_params['logisticregression__C']\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_params = {k:v for k, v in params.items() if k.startswith('col')}\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two-step approach to hyperparameter tuning:**\n",
    "\n",
    "1. **Randomized search:** Test a variety of parameters and values, then examine the results for trends\n",
    "2. **Grid search:** Use an optimized set of parameters and values based on what you learned from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestClassifier tuning parameters:**\n",
    "\n",
    "- **n_estimators:** Number of decisions trees in the forest\n",
    "  - 100 (default)\n",
    "  - 300\n",
    "  - 500\n",
    "  - 700\n",
    "- **min_samples_leaf:** Minimum number of samples at a leaf node\n",
    "  - 1 (default)\n",
    "  - 2\n",
    "  - 3\n",
    "- **max_features:** Number of features to consider when choosing a split\n",
    "  - 'sqrt' (default)\n",
    "  - None\n",
    "- **bootstrap:** Whether bootstrap samples are used when building trees\n",
    "  - True (default)\n",
    "  - False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params['randomforestclassifier__n_estimators'] = [100, 300, 500, 700]\n",
    "rf_params['randomforestclassifier__min_samples_leaf'] = [1, 2, 3]\n",
    "rf_params['randomforestclassifier__max_features'] = ['sqrt', None]\n",
    "rf_params['randomforestclassifier__bootstrap'] = [True, False]\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 5 minutes\n",
    "rf_rand = RandomizedSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy',\n",
    "                             n_iter=100, random_state=1, n_jobs=-1)\n",
    "%time rf_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rand.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Randomized search (RF):** 0.825 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Further tuning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rf_rand.cv_results_)\n",
    "results.sort_values('rank_test_score').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trends in the randomized search results:**\n",
    "\n",
    "- **n_estimators:**\n",
    "  - Higher numbers are performing better\n",
    "  - Remove 100, add 900\n",
    "- **min_samples_leaf:**\n",
    "  - Higher numbers are performing better\n",
    "  - Remove 1, add 4 and 5\n",
    "- **max_features:**\n",
    "  - None is performing better\n",
    "  - Remove 'sqrt'\n",
    "- **bootstrap:**\n",
    "  - True is performing better\n",
    "  - Remove False\n",
    "- **Transformer parameters:**\n",
    "  - No clear trends\n",
    "  - Leave as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params['randomforestclassifier__n_estimators'] = [300, 500, 700, 900]\n",
    "rf_params['randomforestclassifier__min_samples_leaf'] = [2, 3, 4, 5]\n",
    "rf_params['randomforestclassifier__max_features'] = [None]\n",
    "rf_params['randomforestclassifier__bootstrap'] = [True]\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 10 minutes\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time rf_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829 ðŸ‘ˆ\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Randomized search (RF):** 0.825\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Q&A: How do I tune two models with a single grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pipe = Pipeline([('preprocessor', ct), ('classifier', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {}\n",
    "params1['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params1['classifier__penalty'] = ['l1', 'l2']\n",
    "params1['classifier__C'] = [0.1, 1, 10]\n",
    "params1['classifier'] = [logreg]\n",
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {}\n",
    "params2['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params2['classifier__n_estimators'] = [300, 500]\n",
    "params2['classifier__min_samples_leaf'] = [3, 4]\n",
    "params2['classifier'] = [rf]\n",
    "params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "both_params = [params1, params2]\n",
    "both_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_grid = GridSearchCV(both_pipe, both_params, cv=5, scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "both_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(both_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "both_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extensions of this approach:**\n",
    "\n",
    "- Tune different preprocessing parameters for each model\n",
    "- Tune two different preprocessor objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Q&A: How do I tune two models with a single randomized search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_rand = RandomizedSearchCV(both_pipe, both_params, cv=5, scoring='accuracy',\n",
    "                               n_iter=10, random_state=1, n_jobs=-1)\n",
    "both_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(both_rand.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Ensembling multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Introduction to ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to create an ensemble:**\n",
    "\n",
    "- **Regression:** Average the predictions\n",
    "- **Classification:** Average the predicted probabilities, or let the classifiers vote on the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does ensembling work?**\n",
    "\n",
    "- \"One-off\" errors made by each model will be discarded when ensembling\n",
    "- Ensemble has a lower variance than any individual model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Ensembling logistic regression and random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)\n",
    "pipe = make_pipeline(ct, logreg)\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, n_jobs=-1)\n",
    "rf_pipe = make_pipeline(ct, rf)\n",
    "cross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (LR):** 0.811 ðŸ‘ˆ\n",
    "- **Baseline (RF):** 0.811 ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voting options for VotingClassifier:**\n",
    "\n",
    "- **soft:** Average the predicted probabilities\n",
    "- **hard:** Majority vote using class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vc_pipe = make_pipeline(ct, vc)\n",
    "vc_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Combining predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)\n",
    "pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.fit(X, y)\n",
    "rf_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.fit(X, y)\n",
    "vc_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.predict(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.predict_proba(X_new)[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC soft voting):** 0.818 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Combining class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='hard', n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820 ðŸ‘ˆ\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is this result misleading?**\n",
    "\n",
    "- In the case of a tie, hard voting always chooses class 0\n",
    "- Thus hard voting is performing better than soft voting by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Choosing a voting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft voting vs hard voting:**\n",
    "\n",
    "- **Soft voting:**\n",
    "  - Preferred if you have an even number of models (especially two)\n",
    "  - Preferred if all models are well-calibrated\n",
    "  - Only works if all models have the predict_proba method\n",
    "- **Hard voting:**\n",
    "  - Preferred if some models are not well-calibrated\n",
    "  - Does not require the predict_proba method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft', n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6 Tuning an ensemble with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params = {k:v for k, v in params.items() if k.startswith('col')}\n",
    "vc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.named_steps['votingclassifier'].named_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params['votingclassifier__clf1__penalty'] = ['l1', 'l2']\n",
    "vc_params['votingclassifier__clf1__C'] = [1, 10]\n",
    "vc_params['votingclassifier__clf2__n_estimators'] = [100, 300]\n",
    "vc_params['votingclassifier__clf2__min_samples_leaf'] = [2, 3]\n",
    "vc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: 1 minute\n",
    "vc_grid = GridSearchCV(vc_pipe, vc_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%time vc_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC soft voting):** 0.834 ðŸ‘ˆ\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7 Q&A: When should I use ensembling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should you ensemble?**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Improves model performance\n",
    "- **Disadvantages:**\n",
    "  - Adds more complexity\n",
    "  - Decreases interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice for ensembling:**\n",
    "\n",
    "- Include at least 3 models\n",
    "- Models should be performing well on their own\n",
    "- Ideal if they generate predictions using different processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.8 Q&A: How do I apply different weights to the models in an ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft',\n",
    "                      weights=[2, 1], n_jobs=-1)\n",
    "vc_pipe = make_pipeline(ct, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_pipe.fit(X, y)\n",
    "vc_pipe.predict_proba(X_new)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline accuracy scores:**\n",
    "\n",
    "- **Grid search (VC soft voting):** 0.834\n",
    "- **Grid search (RF):** 0.829\n",
    "- **Grid search (LR):** 0.828\n",
    "- **Baseline (VC hard voting):** 0.820\n",
    "- **Baseline (VC soft voting):** 0.818\n",
    "- **Baseline (VC soft voting with LR weighted):** 0.816 ðŸ‘ˆ\n",
    "- **Baseline (LR):** 0.811\n",
    "- **Baseline (RF):** 0.811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_params['votingclassifier__weights'] = [(1, 1), (2, 1), (1, 2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
