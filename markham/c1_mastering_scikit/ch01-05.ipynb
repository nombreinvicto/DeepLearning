{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: [Master Machine Learning with scikit-learn](https://courses.dataschool.io/view/courses/master-machine-learning-with-scikit-learn)\n",
    "\n",
    "## Chapters 1-5\n",
    "\n",
    "*Â© 2022 Data School. All rights reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Course overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High-level topics:**\n",
    "\n",
    "- Handling missing values, text data, categorical data, and class imbalance\n",
    "- Building a reusable workflow\n",
    "- Feature engineering, selection, and standardization\n",
    "- Avoiding data leakage\n",
    "- Tuning your entire workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How you will benefit from this course:**\n",
    "\n",
    "- Knowledge of best practices\n",
    "- Confidence when tackling new ML problems\n",
    "- Ability to anticipate and solve problems\n",
    "- Improved code quality\n",
    "- Better, faster results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 scikit-learn vs Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of scikit-learn:**\n",
    "\n",
    "- Consistent interface to many models\n",
    "- Many tuning parameters (but sensible defaults)\n",
    "- Workflow-related functionality\n",
    "- Exceptional documentation\n",
    "- Active community support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawbacks of deep learning:**\n",
    "\n",
    "- More computational resources\n",
    "- Higher learning curve\n",
    "- Less interpretable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prerequisite skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn prerequisites:**\n",
    "\n",
    "- Loading a dataset\n",
    "- Defining the features and target\n",
    "- Training and evaluating a model\n",
    "- Making predictions with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New to scikit-learn?**\n",
    "\n",
    "- Enroll in \"Introduction to Machine Learning with scikit-learn\" (free)\n",
    "- Available at https://courses.dataschool.io\n",
    "- Complete lessons 1 through 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Course setup and software versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to install scikit-learn and pandas:**\n",
    "\n",
    "- **Option 1:** Install together\n",
    "  - **Anaconda:** https://www.anaconda.com/products/distribution\n",
    "- **Option 2:** Install separately\n",
    "  - **scikit-learn:** https://scikit-learn.org\n",
    "  - **pandas:** https://pandas.pydata.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:06.324643600Z",
     "start_time": "2023-09-20T01:14:59.364501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'1.0.2'"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn version:**\n",
    "\n",
    "- **Course version:** 0.23.2\n",
    "- **Minimum version:** 0.20.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to install scikit-learn 0.23.2:**\n",
    "\n",
    "- **Option 1:** conda install scikit-learn==0.23.2\n",
    "- **Option 2:** pip install -U scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:07.117525500Z",
     "start_time": "2023-09-20T01:14:59.418358500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'1.1.5'"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Google Colab with the course:**\n",
    "\n",
    "- Similar to the Jupyter Notebook\n",
    "- Runs in your browser\n",
    "- Free (but requires a Google account)\n",
    "- Available at https://colab.research.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Course outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapters:**\n",
    "\n",
    "1. Introduction\n",
    "2. Review of the Machine Learning workflow\n",
    "3. Encoding categorical features\n",
    "4. Improving your workflow with ColumnTransformer and Pipeline\n",
    "5. Workflow review #1\n",
    "6. Encoding text data\n",
    "7. Handling missing values\n",
    "8. Fixing common workflow problems\n",
    "9. Workflow review #2\n",
    "10. Evaluating and tuning a Pipeline\n",
    "11. Comparing linear and non-linear models\n",
    "12. Ensembling multiple models\n",
    "13. Feature selection\n",
    "14. Feature standardization\n",
    "15. Feature engineering with custom transformers\n",
    "16. Workflow review #3\n",
    "17. High-cardinality categorical features\n",
    "18. Class imbalance\n",
    "19. Class imbalance walkthrough\n",
    "20. Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson types:**\n",
    "\n",
    "- Core lessons\n",
    "- Q&A lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not focus on algorithms?**\n",
    "\n",
    "- Workflow will have a greater impact on your results\n",
    "- Reusable workflow enables you to try many different algorithms\n",
    "- Hard to know (in advance) which algorithm will work best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Course datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets:**\n",
    "\n",
    "- Titanic\n",
    "- US census\n",
    "- Mammography scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use smaller datasets?**\n",
    "\n",
    "- Easier and faster access to files\n",
    "- Reduced computational time\n",
    "- Greater understanding of the course material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Meet your instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About me:**\n",
    "\n",
    "- Founder of Data School\n",
    "- Teaching data science for 7+ years\n",
    "- Passionate about teaching people who are new to data science\n",
    "- Live in Asheville, North Carolina\n",
    "- Degree in Computer Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Review of the Machine Learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading and exploring a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:08.762170500Z",
     "start_time": "2023-09-20T01:15:00.069573400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\n",
    "df = pd.read_csv('titanic_train.csv', nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:08.763167700Z",
     "start_time": "2023-09-20T01:15:01.420212500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Survived  Pclass                                               Name  \\\n0         0       3                            Braund, Mr. Owen Harris   \n1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n2         1       3                             Heikkinen, Miss. Laina   \n3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n4         0       3                           Allen, Mr. William Henry   \n5         0       3                                   Moran, Mr. James   \n6         0       1                            McCarthy, Mr. Timothy J   \n7         0       3                     Palsson, Master. Gosta Leonard   \n8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n9         1       2                Nasser, Mrs. Nicholas (Adele Achem)   \n\n      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n3  female  35.0      1      0            113803  53.1000  C123        S  \n4    male  35.0      0      0            373450   8.0500   NaN        S  \n5    male   NaN      0      0            330877   8.4583   NaN        Q  \n6    male  54.0      0      0             17463  51.8625   E46        S  \n7    male   2.0      3      1            349909  21.0750   NaN        S  \n8  female  27.0      0      2            347742  11.1333   NaN        S  \n9  female  14.0      1      0            237736  30.0708   NaN        C  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Moran, Mr. James</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>1</td>\n      <td>McCarthy, Mr. Timothy J</td>\n      <td>male</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Palsson, Master. Gosta Leonard</td>\n      <td>male</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>2</td>\n      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n      <td>female</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning terminology:**\n",
    "\n",
    "- **Target:** Goal of prediction\n",
    "- **Classification:** Problem with a categorical target\n",
    "- **Feature:** Input to the model (column)\n",
    "- **Sample:** Single observation (row)\n",
    "- **Training data:** Data with known target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection methods:**\n",
    "\n",
    "- Human intuition\n",
    "- Domain knowledge\n",
    "- Data exploration\n",
    "- Automated methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Currently selected features:**\n",
    "\n",
    "- **Parch:** Number of parents or children aboard with that passenger\n",
    "- **Fare:** Amount the passenger paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:09.844991400Z",
     "start_time": "2023-09-20T01:15:01.466090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Parch     Fare\n0      0   7.2500\n1      0  71.2833\n2      0   7.9250\n3      0  53.1000\n4      0   8.0500\n5      0   8.4583\n6      0  51.8625\n7      1  21.0750\n8      2  11.1333\n9      0  30.0708",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7.2500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>71.2833</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>7.9250</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>53.1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>8.0500</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>8.4583</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>51.8625</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>21.0750</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>11.1333</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>30.0708</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Parch', 'Fare']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:10.013058Z",
     "start_time": "2023-09-20T01:15:01.734359800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    0\n1    1\n2    1\n3    1\n4    0\n5    0\n6    0\n7    0\n8    1\n9    1\nName: Survived, dtype: int64"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Survived']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:10.146108700Z",
     "start_time": "2023-09-20T01:15:01.883979700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 2)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:10.259805300Z",
     "start_time": "2023-09-20T01:15:02.266955400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10,)"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Building and evaluating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:10.504153Z",
     "start_time": "2023-09-20T01:15:02.485371900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined X and y, our next step is to build and evaluate a model.\n",
    "\n",
    "To start, we're going to use logistic regression as our model. It's a good default choice for classification problems because it's both fast and interpretable.\n",
    "\n",
    "We import it from the linear_model module, and then we create an instance called logreg. This is our model object.\n",
    "\n",
    "The default solver for logistic regression has changed between different scikit-learn versions, but in this course I'm going to set the solver to liblinear. I'm specifying the solver explicitly and setting a value for random_state so that if you run the same code at home, you will most likely get the same results as me.\n",
    "\n",
    "Let's now talk about model evaluation. The goal of model evaluation is to simulate how a model will perform on future data so that we can choose between models today. To do model evaluation, we need both an evaluation procedure and an evaluation metric.\n",
    "\n",
    "The procedure we will use is K-fold cross-validation. Another option is to use train/test split, but cross-validation is generally superior because it gives a lower variance estimate of model performance.\n",
    "\n",
    "The metric we will use is classification accuracy. There are many other classification metrics we could have chosen, but accuracy is suitable for this problem for two reasons:\n",
    "\n",
    "- First, there is not significant class imbalance.\n",
    "- And second, predicting the positive class correctly is just as important to us as predicting the negative class correctly.\n",
    "\n",
    "That being said, I will cover other classification metrics in the chapters on class imbalance.\n",
    "\n",
    "With such a small dataset, we're going to use 3-fold cross-validation, rather than 5 or 10 folds which is more typical. Let me briefly review what happens during 3-fold cross-validation:\n",
    "\n",
    "- The rows are split into 3 subsets, which we'll call A, B, and C.\n",
    "- First, A and B together become the training set, and C becomes the testing set. The model is trained on the training set, the trained model makes predictions for the testing set, and those predictions are evaluated.\n",
    "- Next, A and C together become the training set, and B becomes the testing set. Again, the model is trained, it makes predictions, and the predictions are evaluated.\n",
    "- Finally, B and C together become the training set, and A becomes the testing set. The training, predicting, and evaluation process happens one final time.\n",
    "- Because the evaluation process occurred 3 times, it returns 3 scores, and we will usually take the mean of those scores.\n",
    "\n",
    "Let's go ahead and use cross-validation to evaluate our model:\n",
    "\n",
    "- First we import the cross_val_score function from the model_selection module.\n",
    "- Then we pass it the model object, X and y, the number of cross-validation folds, and the evaluation metric. Although the default metric for classification problems is accuracy, I recommend specifying it explicitly so that there's no ambiguity.\n",
    "- When we run cross_val_score, it does the dataset splitting, training, predicting, and evaluation. 3 accuracy scores are returned, and the mean of those scores is 69%.\n",
    "\n",
    "If you received a different result, that's not a problem. The results can vary based on your scikit-learn version due to changes in the default parameters, algorithm changes, bug fixes, and so on.\n",
    "\n",
    "Unfortunately, we can't take these results seriously because the dataset is so small. There's actually no reliable evaluation procedure when your training data only contains 10 rows, but I did want to demonstrate it anyway to emphasize that model evaluation is a normal part of the Machine Learning workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements for model evaluation:**\n",
    "\n",
    "- **Procedure:** K-fold cross-validation\n",
    "- **Metric:** Classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps of 3-fold cross-validation:**\n",
    "\n",
    "1. Split rows into 3 subsets (A, B, C)\n",
    "2. A & B is training set, C is testing set\n",
    "  - Train model on training set\n",
    "  - Make predictions on testing set\n",
    "  - Evaluate predictions\n",
    "3. Repeat with A & C as training set, B as testing set\n",
    "4. Repeat with B & C as training set, A as testing set\n",
    "5. Calculate the mean of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:10.832071100Z",
     "start_time": "2023-09-20T01:15:02.745675800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6944444444444443"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(logreg, X, y, cv=3, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using the model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the workflow, we would typically try making changes in order to achieve a better accuracy, such as:\n",
    "\n",
    "- Tuning the the model's hyperparameters\n",
    "- Adding or removing features\n",
    "- Or trying a different classification model other than logistic regression.\n",
    "\n",
    "We'll cover these topics in detail later in the course, but for now, let's assume that we're happy with the model as-is. Thus, our next steps are to train the model and then use it to make predictions on new data.\n",
    "\n",
    "We use the model's fit method, which instructs the model to try to learn the relationship between X and y.\n",
    "\n",
    "There are four important points I want to note here:\n",
    "\n",
    "- First, you should train your model on the entire dataset before using it to make predictions, otherwise you are throwing away valuable training data. In truth, we do have more than 10 rows, but for now we are considering our entire training dataset to be these 10 rows.\n",
    "- Second, the model object is modified in-place when you run the fit method, and so there's no need to overwrite the logreg object using an assignment statement.\n",
    "- Third, scikit-learn understands how to work with pandas objects, and so we can pass X and y directly to the fit method.\n",
    "- And finally, if you're using scikit-learn 0.23 or later, you will only see the parameters that have changed from the defaults when you print or fit a model. That's why it only displays the random_state and solver parameters, whereas in previous versions of scikit-learn, all model parameters would have been displayed.\n",
    "\n",
    "Now, let's read in a new dataset for which we don't know the target values. You can read it from a URL or from your local computer, so choose whichever option you prefer. Again, we are only going to keep the first 10 rows.\n",
    "\n",
    "You'll notice that it has the same columns as the df DataFrame, except that there's no Survived column, which is the column that we're going to predict.\n",
    "\n",
    "Before we make predictions, we have to define X_new. It has to have the same columns as X, and those columns have to be in the same order.\n",
    "\n",
    "Finally, we'll use the trained model to make predictions by passing X_new to the predict method, which outputs a NumPy array. There are 10 predictions because it makes 1 prediction for each sample in X_new.\n",
    "\n",
    "The predictions are in the same order as the samples in X_new, meaning the first prediction is for the first row in X_new, the second prediction is for the second row in X_new, and so on.\n",
    "\n",
    "Note that we can't actually evaluate the accuracy of these predictions because we don't know the true target values for the samples in X_new.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ways to improve the model:**\n",
    "\n",
    "- Hyperparameter tuning\n",
    "- Adding or removing features\n",
    "- Trying a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:11.557822900Z",
     "start_time": "2023-09-20T01:15:03.197892800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(random_state=1, solver='liblinear')"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points about model fitting:**\n",
    "\n",
    "- Train your model on the entire dataset before making predictions\n",
    "- Assignment statement is unnecessary\n",
    "- Passing pandas objects is fine\n",
    "- Only prints parameters that have changed (version 0.23 or later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.119763800Z",
     "start_time": "2023-09-20T01:15:03.386535400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Pclass                                          Name     Sex   Age  SibSp  \\\n0       3                              Kelly, Mr. James    male  34.5      0   \n1       3              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n2       2                     Myles, Mr. Thomas Francis    male  62.0      0   \n3       3                              Wirz, Mr. Albert    male  27.0      0   \n4       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1   \n5       3                    Svensson, Mr. Johan Cervin    male  14.0      0   \n6       3                          Connolly, Miss. Kate  female  30.0      0   \n7       2                  Caldwell, Mr. Albert Francis    male  26.0      1   \n8       3     Abrahim, Mrs. Joseph (Sophie Halaut Easu)  female  18.0      0   \n9       3                       Davies, Mr. John Samuel    male  21.0      2   \n\n   Parch     Ticket     Fare  Cabin Embarked  \n0      0     330911   7.8292    NaN        Q  \n1      0     363272   7.0000    NaN        S  \n2      0     240276   9.6875    NaN        Q  \n3      0     315154   8.6625    NaN        S  \n4      1    3101298  12.2875    NaN        S  \n5      0       7538   9.2250    NaN        S  \n6      0     330972   7.6292    NaN        Q  \n7      1     248738  29.0000    NaN        S  \n8      0       2657   7.2292    NaN        C  \n9      0  A/4 48871  24.1500    NaN        S  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>Svensson, Mr. Johan Cervin</td>\n      <td>male</td>\n      <td>14.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7538</td>\n      <td>9.2250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>Connolly, Miss. Kate</td>\n      <td>female</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330972</td>\n      <td>7.6292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>Caldwell, Mr. Albert Francis</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>248738</td>\n      <td>29.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>Abrahim, Mrs. Joseph (Sophie Halaut Easu)</td>\n      <td>female</td>\n      <td>18.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2657</td>\n      <td>7.2292</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>Davies, Mr. John Samuel</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>A/4 48871</td>\n      <td>24.1500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/MLnewdata', nrows=10)\n",
    "df_new = pd.read_csv('titanic_new.csv', nrows=10)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.141700Z",
     "start_time": "2023-09-20T01:15:05.750366100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Parch     Fare\n0      0   7.8292\n1      0   7.0000\n2      0   9.6875\n3      0   8.6625\n4      1  12.2875\n5      0   9.2250\n6      0   7.6292\n7      1  29.0000\n8      0   7.2292\n9      0  24.1500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7.8292</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>7.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>9.6875</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>8.6625</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>12.2875</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>9.2250</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>7.6292</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>29.0000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>7.2292</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>24.1500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = df_new[['Parch', 'Fare']]\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.142737800Z",
     "start_time": "2023-09-20T01:15:05.766323700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 1, 0, 0, 1, 0, 1], dtype=int64)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Q&A: How do I adapt this workflow to a regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we're going to be focusing on classification problems, which means that the target you're trying to predict is categorical. The other main type of prediction problem is regression, in which your target value is continuous.\n",
    "\n",
    "If you're planning to work on a regression problem, the good news is that the workflow I'm teaching will work just as well to solve classification or regression problems. There are only two changes you will need to make to adapt this workflow for regression:\n",
    "\n",
    "- First, you will need to choose a different Machine Learning model. For example, you might choose linear regression instead of logistic regression, since linear regression predicts continuous values whereas logistic regression predicts class values.\n",
    "- Second, you will need to choose a different model evaluation metric. For example, you might choose mean squared error instead of accuracy, since mean squared error is appropriate for continuous values whereas accuracy is only appropriate for categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapting this workflow for regression:**\n",
    "\n",
    "1. Choose a different model\n",
    "2. Choose a different evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Q&A: How do I adapt this workflow to a multiclass problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we worked on a binary classification problem, which means there are only two possible output classes.\n",
    "\n",
    "Multiclass problems are ones in which there are more than two output classes. The classic example of this is the iris dataset, in which each iris plant can be classified as one of three possible species.\n",
    "\n",
    "Thankfully, in scikit-learn, all classifiers automatically handle multiclass problems with no changes to the workflow. It automatically detects the number of classes from the data, thus you don't even have to inform scikit-learn that you're working on a multiclass problem.\n",
    "\n",
    "So how do classifiers handle multiclass problems?\n",
    "\n",
    "- Many classifiers are inherently multiclass, meaning that they work exactly the same regardless of the number of classes.\n",
    "- For classifiers that only work in the binary case, they can be extended to the multiclass case using the so-called \"one-vs-one\" or \"one-vs-rest\" strategies, in which multiple models are fit and the results are combined. You can research more about these strategies if you're interested, but the key point is that this is handled for you automatically by scikit-learn without you needing to do anything special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of classification problems:**\n",
    "\n",
    "- **Binary:** Two output classes\n",
    "- **Multiclass:** More than two output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How classifiers handle multiclass problems:**\n",
    "\n",
    "- Many are inherently multiclass\n",
    "- Others can be extended using \"one-vs-one\" or \"one-vs-rest\" strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Q&A: Why should I select a Series for the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our DataFrame, there are two ways you could imagine selecting the target variable of Survived:\n",
    "\n",
    "- The first way is as a pandas Series, which we can do using a single set of brackets.\n",
    "- The second way is as a pandas DataFrame with one column, which we can do using two sets of brackets.\n",
    "\n",
    "These two objects look similar, but they actually have different shapes. The Series is a one-dimensional object, while the DataFrame is a two-dimensional object.\n",
    "\n",
    "The difference between these two objects is more clear if you convert them to NumPy arrays using the to_numpy method.\n",
    "\n",
    "Now that you've seen that these two objects are different, the question is: Why does it matter which object you use for the target?\n",
    "\n",
    "To answer this question, I have to briefly explain multilabel classification. A multilabel classification problem is one in which each sample can simultaneously have more than one label. The classic example of this is classifying the topic of a document. For example, a document might be about politics, religion, or law, or it might fit into multiple topics at once.\n",
    "\n",
    "This is different from multiclass classification because multiclass only allows a sample to have a single label, whereas multilabel allows a single sample to have multiple labels.\n",
    "\n",
    "Anyway, scikit-learn supports multilabel classification by allowing you to represent the target as a two-dimensional object. For example, if you had 10 documents and there were 3 possible labels, you would actually use a 10 by 3 DataFrame as your y value.\n",
    "\n",
    "That is all to say that using a two-dimensional DataFrame as your y value signals to scikit-learn that you are working on a multilabel problem. Our classification problem is not multilabel, and thus we use a one-dimensional Series as our y value to signal that we are working on a single-label problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.886795Z",
     "start_time": "2023-09-20T01:15:05.915948100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    0\n1    1\n2    1\n3    1\n4    0\n5    0\n6    0\n7    0\n8    1\n9    1\nName: Survived, dtype: int64"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.887788500Z",
     "start_time": "2023-09-20T01:15:07.508503500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Survived\n0         0\n1         1\n2         1\n3         1\n4         0\n5         0\n6         0\n7         0\n8         1\n9         1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.888785900Z",
     "start_time": "2023-09-20T01:15:07.732909300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10,)"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.889783400Z",
     "start_time": "2023-09-20T01:15:07.896013700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 1)"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Survived']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.890781Z",
     "start_time": "2023-09-20T01:15:08.105924800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1], dtype=int64)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.890781Z",
     "start_time": "2023-09-20T01:15:08.344287200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1]], dtype=int64)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Survived']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilabel vs multiclass problems:**\n",
    "\n",
    "- **Multilabel:** Each sample can have more than one label\n",
    "- **Multiclass:** Each sample can have one label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilabel vs multiclass targets:**\n",
    "\n",
    "- **Multilabel:** 2-dimensional y (DataFrame)\n",
    "- **Multiclass:** 1-dimensional y (Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Q&A: How do I add the model's predictions to a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that you wanted to match up the 10 predictions output by the model with the 10 rows of the X_new DataFrame so that you can see the predictions next to the features.\n",
    "\n",
    "To do this, you would first convert the predictions from a NumPy array to a pandas Series. Note that we are setting the Series index to match the index of X_new, and we're giving the Series a name.\n",
    "\n",
    "Then, you use the concat function to concatenate the X_new DataFrame and the predictions Series along the columns axis, which outputs a DataFrame. Note that the name of the Series became the name of that DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:14.891778Z",
     "start_time": "2023-09-20T01:15:08.526799200Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.Series(logreg.predict(X_new), index=X_new.index,\n",
    "                        name='Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.007468900Z",
     "start_time": "2023-09-20T01:15:08.704325800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Parch     Fare  Prediction\n0      0   7.8292           0\n1      0   7.0000           0\n2      0   9.6875           0\n3      0   8.6625           0\n4      1  12.2875           1\n5      0   9.2250           0\n6      0   7.6292           0\n7      1  29.0000           1\n8      0   7.2292           0\n9      0  24.1500           1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>9.2250</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>7.6292</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>29.0000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>7.2292</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>24.1500</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([X_new, predictions], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Q&A: How do I determine the confidence level of each prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some classification problems, you are only interested in the predicted class labels. However, sometimes it's useful to output the predicted probabilities of class membership using the predict_proba method.\n",
    "\n",
    "The output array has 10 rows because the model made predictions for 10 samples, and it has 2 columns because there are 2 possible classes. The left column represents class 0, and the right column represents class 1.\n",
    "\n",
    "Let's talk about how to interpret this array, using the first row as an example:\n",
    "\n",
    "- The model calculated a likelihood of 58% that the first sample in X_new was class 0 and a 42% likelihood that it was class 1.\n",
    "- Because class 0 had a higher likelihood, the model predicted class 0 for this sample.\n",
    "- And as you might imagine, the values in every row will always add up to 1.\n",
    "\n",
    "If you need just the second column, meaning the predicted probabilities of class 1, then you can extract it using NumPy's slicing notation. The colon means select all rows, and the 1 means select the column in the 1 position, which is the second column.\n",
    "\n",
    "Some classifiers, such as logistic regression, are known as well-calibrated classifiers, which means that their predicted probabilities can be directly interpreted as the model's confidence level in that prediction. So for example, it's more confident that the 8th sample is class 1 than it is that the 10th sample is class 1.\n",
    "\n",
    "Knowing these confidence levels can be useful if you're most interested in the samples with the highest predicted probabilities. For example, if you were trying to predict who might be interested in purchasing a specific product, you might focus all of your marketing budget on reaching those customers with the highest predicted probabilities of purchase.\n",
    "\n",
    "Keep in mind that there are other classifiers which are not as well-calibrated, such as Naive Bayes. In those cases, it's less appropriate to interpret their predicted probabilities as confidence levels. Thus if you know you're going to be interested in these confidence levels, it's best to use a well-calibrated classifier like logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.008466Z",
     "start_time": "2023-09-20T01:15:08.927727600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 1, 0, 0, 1, 0, 1], dtype=int64)"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.009463600Z",
     "start_time": "2023-09-20T01:15:09.168084400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.57804075, 0.42195925],\n       [0.58275546, 0.41724454],\n       [0.56742414, 0.43257586],\n       [0.57328835, 0.42671165],\n       [0.48357081, 0.51642919],\n       [0.57007262, 0.42992738],\n       [0.57917926, 0.42082074],\n       [0.38795132, 0.61204868],\n       [0.58145374, 0.41854626],\n       [0.48342837, 0.51657163]])"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Array of predicted probabilities:**\n",
    "\n",
    "- One row for each sample\n",
    "- One column for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.010463600Z",
     "start_time": "2023-09-20T01:15:09.442363700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.42195925, 0.41724454, 0.43257586, 0.42671165, 0.51642919,\n       0.42992738, 0.42082074, 0.61204868, 0.41854626, 0.51657163])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(X_new)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Q&A: How do I check the accuracy of the model's predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this chapter, we made predictions for the 10 samples in X_new, though we couldn't check the accuracy of these predictions because we don't know the true target values for those samples. That will sometimes be the case in the real world.\n",
    "\n",
    "For example, if you built a model to predict what medical conditions someone might develop based on their genetic information, you may not ever find out whether the model's predictions were correct, either because that data is not being collected or because that data is protected by privacy laws.\n",
    "\n",
    "In other cases, you can actually check the accuracy of your predictions. For example, if you built a model to predict the outcome of all US Supreme Court cases, you would make those predictions before those cases were decided, and then you could check the model's accuracy once the court's rulings were publicly announced.\n",
    "\n",
    "Ideally, the actual accuracy of your model will be close to the accuracy that you estimated using your training data during model evaluation. If it's not close, that could indicate a problem with your model evaluation procedure, or it could indicate that there are some important differences between your training data and the new data.\n",
    "\n",
    "In all cases, you can incorporate this new data into your training data since you know the true target values, which should help the model to make better predictions in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking model accuracy:**\n",
    "\n",
    "- **Not possible:** Target value is unknown or is private data\n",
    "- **Possible:** Target value is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Q&A: What do the \"solver\" and \"random_state\" parameters do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, when creating the model object, I set the logistic regression's solver to liblinear, and I set the random_state to 1. I set these values so that if you ran the same code at home, you would most likely get the same results as me. In this lesson, I'll explain what these two parameters actually do.\n",
    "\n",
    "The solver is the algorithm used to solve the optimization problem of calculating the logistic regression's coefficients. In other words, given the features and the target, the solver figures out the coefficients. The solvers have different strengths and weaknesses, different properties, and ultimately may come up with different results. Here's a comparison chart from the scikit-learn documentation.\n",
    "\n",
    "I recommend reviewing this chart and reading the documentation to decide which solver to use for your particular problem, but ultimately it's fine to just try each one and see what happens.\n",
    "\n",
    "liblinear used to be the default solver for logistic regression, but in scikit-learn version 0.22, they changed the default to lbfgs instead. I'm having all of us use liblinear in this course so that we will tend to get the same results, regardless of scikit-learn version. Keep in mind that we still aren't guaranteed to get the exact same results, because with each new version, bugs are fixed and other algorithm parameters are sometimes changed.\n",
    "\n",
    "One final note about the solver is that if you ever get a convergence warning when using logistic regression, the best solution is usually just to use a different solver.\n",
    "\n",
    "Next, let's talk about the random_state parameter. It happens that there is some randomness involved in three of the solvers, including liblinear. That means you may get different results each time you fit the model. By setting the random_state, you ensure that your model will output the same results every time.\n",
    "\n",
    "More generally, any time you're running a scikit-learn function that involves a random process, I recommend setting the random_state parameter to any integer. That allows your code to be reproducible, both by you and others. Keep in mind that the only way to know whether a given function involves a random process is by reading the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.010463600Z",
     "start_time": "2023-09-20T01:15:09.607424Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataschool.io/files/solver_comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default solver for logistic regression:**\n",
    "\n",
    "- **Before version 0.22:** liblinear\n",
    "- **Starting in version 0.22:** lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice for random_state:**\n",
    "\n",
    "- Set random_state to any integer when a random process is involved\n",
    "- Allows your code to be reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Q&A: How do I show all of the model parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in scikit-learn version 0.23, when you print any estimator (such as a model, a transformer, or a pipeline), it will only show you the parameters that are not set to their default values. For example, when we print out the logreg model object, it only shows the random_state and solver parameters because we set those explicitly.\n",
    "\n",
    "However, you can still see all parameters by running the get_params method.\n",
    "\n",
    "If you like, you can restore the behavior from previous scikit-learn versions by importing the set_config function and then setting the print_changed_only parameter to False. Now, all parameters will be printed, regardless of whether you've changed them.\n",
    "\n",
    "I prefer the new behavior, so I'm going to set print_changed_only back to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.134131200Z",
     "start_time": "2023-09-20T01:15:09.799989700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(random_state=1, solver='liblinear')"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.246828700Z",
     "start_time": "2023-09-20T01:15:10.060931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'auto',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': 1,\n 'solver': 'liblinear',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.246828700Z",
     "start_time": "2023-09-20T01:15:10.305683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.281734500Z",
     "start_time": "2023-09-20T01:15:10.475234900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=1, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.345563700Z",
     "start_time": "2023-09-20T01:15:10.713592200Z"
    }
   },
   "outputs": [],
   "source": [
    "set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Q&A: Should I shuffle the samples when using cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran cross_val_score earlier in this chapter, I passed an integer, 3 in this case, that specified the number of cross-validation folds.\n",
    "\n",
    "This code shows you what happens \"under the hood\" when you specify cv=3 for a classification problem. I'm going to walk through this code so you understand what's happening and you can modify it when needed.\n",
    "\n",
    "First, you'll notice that we're importing a class called StratifiedKFold. It's known as a cross-validation splitter, which means that its role is to split datasets. We create an instance of this class and pass it a 3 so that it will create 3 folds. And then we can pass this instance to cross_val_score instead of an integer.\n",
    "\n",
    "It's called StratifiedKFold because it uses stratified sampling to ensure that the class proportions are approximately equal in each fold. For example, if 40% of the passengers in the dataset survived, then stratified sampling ensures that about 40% of each fold is survived passengers.\n",
    "\n",
    "In other words, it ensures that each fold is representative of the entire dataset. Stratified sampling is desirable because it produces more reliable cross-validation scores, and again, scikit-learn will do this for you by default.\n",
    "\n",
    "Another good thing to know about StratifiedKFold is that by default, it does not shuffle the samples before splitting. Thus, there is nothing random about this process, and as such you will get the same results every time you run cross_val_score.\n",
    "\n",
    "In most cases, it doesn't matter whether you shuffle the samples before splitting. However, if the order of the samples in your dataset is not arbitrary, then it's important to randomly shuffle the samples when cross-validating.\n",
    "\n",
    "For example, you could imagine that if your dataset was sorted by one of the features, then some folds would only have high values of that feature and other folds would only have low values of that feature, which could result in unreliable cross-validation scores.\n",
    "\n",
    "If you do need to shuffle the samples, you simply modify the cross-validation splitter by setting shuffle to True, and then pass that splitter object to cross_val_score. Note that because you are introducing randomness into the process by shuffling, you should also set a random_state to ensure reproducibility.\n",
    "\n",
    "In summary, if you have a classification problem and the samples are in an arbitrary order, you can just pass an integer to the cv parameter of cross_val_score, and it will use stratified sampling without shuffling.\n",
    "\n",
    "If your samples are not in an arbitrary order, you should use StratifiedKFold as your splitter and set shuffle to True, and then pass the splitter object to the cv parameter of cross_val_score, as I did above.\n",
    "\n",
    "Finally, it's worth mentioning that if you're working on a regression problem instead, and you need to shuffle the samples, you should use the KFold class instead of the StratifiedKFold class, because stratified sampling does not apply to regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.345563700Z",
     "start_time": "2023-09-20T01:15:10.930835900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.75      , 0.66666667, 0.66666667])"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, X, y, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.346561400Z",
     "start_time": "2023-09-20T01:15:11.108890200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.75      , 0.66666667, 0.66666667])"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(3)\n",
    "cross_val_score(logreg, X, y, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified sampling:**\n",
    "\n",
    "- Ensures that each fold is representative of the dataset\n",
    "- Produces more reliable cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.358528600Z",
     "start_time": "2023-09-20T01:15:11.364218800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.75      , 0.33333333, 0.66666667])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = StratifiedKFold(3, shuffle=True, random_state=1)\n",
    "cross_val_score(logreg, X, y, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to shuffle your samples:**\n",
    "\n",
    "- **Samples in arbitrary order:** Shuffling not needed\n",
    "- **Samples are ordered:** Shuffling needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to shuffle your samples:**\n",
    "\n",
    "- **Classification:** StratifiedKFold\n",
    "- **Regression:** KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction to one-hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to run the code above:**\n",
    "\n",
    "- **Jupyter Notebook:**\n",
    "  - Select this cell\n",
    "  - Click \"Cell\" menu, then \"Run All Above\"\n",
    "- **JupyterLab:**\n",
    "  - Select this cell\n",
    "  - Click \"Run\" menu, then \"Run All Above Selected Cell\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this chapter, we're going to focus on one of the most important data preprocessing steps, which is the encoding of categorical features.\n",
    "\n",
    "Before we start, it's important to note that this chapter builds on the objects and the imports from the previous chapter. So if you just opened your notebook a moment ago, you need to run all of the code above before starting this chapter.\n",
    "\n",
    "- If you're using the Jupyter Notebook, the easiest way to do this is to select this cell, click on the \"Cell\" menu and then select \"Run All Above\", which runs all of the cells above the currently selected cell.\n",
    "- If you're using JupyterLab, you would select this cell, click the \"Run\" menu and then select \"Run All Above Selected Cell\", which does the same thing.\n",
    "\n",
    "You should repeat this process every time you open the notebook.\n",
    "\n",
    "Now that that's complete, let's take a look at our Titanic DataFrame. In the last chapter, the only features we used were Parch and Fare. In this chapter, we want to add Embarked and Sex as additional features, in case they improve our model.\n",
    "\n",
    "As a reminder, Parch is the number of parents or children aboard with that passenger, and Fare is the amount the passenger paid. Our first new feature, Embarked, is the port that each passenger embarked from, and the possible values are C, Q, or S. Our other new feature, Sex, is simply male or female.\n",
    "\n",
    "Both Embarked and Sex are known as unordered categorical features because there are distinct categories and there's no inherent logical ordering to the categories. This type of data is also known as nominal data.\n",
    "\n",
    "All scikit-learn models expect features to be numeric, and so Embarked and Sex can't actually be passed directly to a model. Instead, we're going to encode them using a process called one-hot encoding, also known as dummy encoding.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.380471400Z",
     "start_time": "2023-09-20T01:15:11.546849800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Survived  Pclass                                               Name  \\\n0         0       3                            Braund, Mr. Owen Harris   \n1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n2         1       3                             Heikkinen, Miss. Laina   \n3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n4         0       3                           Allen, Mr. William Henry   \n5         0       3                                   Moran, Mr. James   \n6         0       1                            McCarthy, Mr. Timothy J   \n7         0       3                     Palsson, Master. Gosta Leonard   \n8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n9         1       2                Nasser, Mrs. Nicholas (Adele Achem)   \n\n      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n3  female  35.0      1      0            113803  53.1000  C123        S  \n4    male  35.0      0      0            373450   8.0500   NaN        S  \n5    male   NaN      0      0            330877   8.4583   NaN        Q  \n6    male  54.0      0      0             17463  51.8625   E46        S  \n7    male   2.0      3      1            349909  21.0750   NaN        S  \n8  female  27.0      0      2            347742  11.1333   NaN        S  \n9  female  14.0      1      0            237736  30.0708   NaN        C  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Moran, Mr. James</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>1</td>\n      <td>McCarthy, Mr. Timothy J</td>\n      <td>male</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Palsson, Master. Gosta Leonard</td>\n      <td>male</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>2</td>\n      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n      <td>female</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Currently selected features:**\n",
    "\n",
    "- **Parch:** Number of parents or children aboard with that passenger\n",
    "- **Fare:** Amount the passenger paid\n",
    "- **Embarked:** Port the passenger embarked from\n",
    "- **Sex:** Male or Female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unordered categorical data:**\n",
    "\n",
    "- Contains distinct categories\n",
    "- No inherent logical ordering to the categories\n",
    "- Also called \"nominal data\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the code for one-hot encoding. First, we import the OneHotEncoder class from the preprocessing module. Then, we create an instance of it and set sparse to False.\n",
    "\n",
    "By default, OneHotEncoder will output a sparse matrix, which is the most efficient and performant data structure for this type of data. By setting sparse to False, it will instead output a dense matrix, which is just the normal way of representing a matrix. This representation will allow us to examine the output so that we can understand the encoding scheme.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.380471400Z",
     "start_time": "2023-09-20T01:15:12.086067600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix representations:**\n",
    "\n",
    "- **Sparse:** More efficient and performant\n",
    "- **Dense:** More readable"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll encode the Embarked column by passing it to the fit_transform method of the OneHotEncoder. We'll talk about the fit_transform method in the next lesson, but for now I just want to highlight that we'll use double brackets around Embarked to pass it as a single-column DataFrame instead of using single brackets to pass it as a Series.\n",
    "\n",
    "This is important because OneHotEncoder expects to receive a two-dimensional object (such as a DataFrame) since a one-dimensional object is considered ambiguous. A one-dimensional Series could be interpreted either as a single feature or a single sample, whereas our two-dimensional DataFrame signals to scikit-learn that this is indeed a single feature.\n",
    "\n",
    "Running the fit_transform method outputs this 10 by 3 array. This is the encoded version of the Embarked column, and it is exactly what we will pass to the model instead of the strings C, Q, and S.\n",
    "\n",
    "Let's talk about how we interpret this output. There are 3 columns because there were 3 unique values in Embarked. Each row contains a single 1, and the rest of the values in the row are 0. 100 means \"C\", 010 means \"Q\", and 001 means \"S\", which you can confirm by comparing it to the Embarked column in our DataFrame.\n",
    "\n",
    "As an aside, this is called one-hot encoding because in each row there is one \"hot\" level, meaning one non-zero level.\n",
    "\n",
    "This is also the same output you would get by using the get_dummies function in pandas, though we'll talk later in the course why it's best to do all of your preprocessing in scikit-learn instead of pandas.\n",
    "\n",
    "Let's now look at the categories attribute of the OneHotEncoder. You can think of it as the column header for our 10 by 3 array. In other words, the categories attribute tells you that the first column represents C, the second column represents Q, and the third column represents S. Because the categories are always in alphabetical order from left to right, I didn't actually have to examine the categories attribute in order to know how to interpret it.\n",
    "\n",
    "As an aside, you'll notice a lot of attributes in scikit-learn end in an underscore. This is scikit-learn's convention for any attribute that is learned or estimated from the data during the fit step.\n",
    "\n",
    "We've now seen how the OneHotEncoder encodes the Embarked feature. But why is this a reasonable way to encode a categorical feature?\n",
    "\n",
    "You can think of it this way: OneHotEncoder creates a feature from each level so that the model can learn the relationship between each level and the target value. In this case, the model can learn the relationship between the target value of Survived and whether or not a passenger embarked at a given port.\n",
    "\n",
    "For example, the model might learn from the first feature that passengers who embarked at C have a higher survival rate than passengers who didn't embark at C. This is similar to how a model might learn from a numeric feature like Fare that passengers with a higher Fare have a higher survival rate than passengers with a lower Fare.\n",
    "\n",
    "At this point, you might be wondering whether we could have instead encoded Embarked as a single numeric feature with the values 0, 1, and 2 representing C, Q, and S. The answer is that yes, we can do this, but it's generally not a good idea to do this with unordered categories because it would imply an ordering that doesn't inherently exist.\n",
    "\n",
    "To see why it's not a good idea, let's pretend that passengers who embarked at C and S had high survival rates, and passengers who embarked at Q had low survival rates. There would be no way for a linear model like logistic regression to learn that relationship if Embarked is encoded as a single feature since a single feature can't be assigned both a negative coefficient to represent the impact of Q with respect to C and a positive coefficient to represent the impact of S with respect to Q.\n",
    "\n",
    "In summary, encoding Embarked as a single feature would prohibit a linear model from learning a non-linear relationship in the data, which is why encoding it as multiple features is generally the better choice.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use double brackets?**\n",
    "\n",
    "- **Single brackets:**\n",
    "  - Outputs a Series\n",
    "  - Could be interpreted as a single feature or a single sample\n",
    "- **Double brackets:**\n",
    "  - Outputs a single-column DataFrame\n",
    "  - Interpreted as a single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.380471400Z",
     "start_time": "2023-09-20T01:15:12.243049800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.]])"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(df[['Embarked']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output of OneHotEncoder:**\n",
    "\n",
    "- One column for each unique value\n",
    "- One non-zero value in each row:\n",
    "  - 1, 0, 0 means \"C\"\n",
    "  - 0, 1, 0 means \"Q\"\n",
    "  - 0, 0, 1 means \"S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.381468500Z",
     "start_time": "2023-09-20T01:15:12.446506400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[array(['C', 'Q', 'S'], dtype=object)]"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can be considered as the column headr of our ohe output\n",
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use one-hot encoding?**\n",
    "\n",
    "- Model can learn the relationship between each level and the target value\n",
    "- Example: Model might learn that \"C\" passengers have a higher survival rate than \"not C\" passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not encode as a single feature?**\n",
    "\n",
    "- **Pretend:**\n",
    "  - C: high survival rate\n",
    "  - Q: low survival rate\n",
    "  - S: high survival rate\n",
    "- **Single feature would need two coefficients:**\n",
    "  - Negative coefficient for impact of Q (with respect to C)\n",
    "  - Positive coefficient for impact of S (with respect to Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transformer methods: fit, transform, fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generic transformer methods:**\n",
    "\n",
    "- **fit:** Transformer learns something\n",
    "- **transform:** Transformer uses what it learned to do the data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder methods:**\n",
    "\n",
    "- **fit:** Learn the categories\n",
    "- **transform:** Create the feature matrix using those categories"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's discuss the fit_transform method, since that's the method we used with OneHotEncoder to encode the Embarked feature.\n",
    "\n",
    "OneHotEncoder is known as a transformer, meaning its role is to perform data transformations. Transformers usually have a \"fit\" method and always have a \"transform\" method. The fit method is when the transformer learns something, and the transform method is when it uses what it learned to do the data transformation.\n",
    "\n",
    "Using OneHotEncoder as an example, the fit method is when it learns the categories from the data in alphabetical order, and the transform method is when it creates the feature matrix using those categories.\n",
    "\n",
    "The fit_transform method, which is what we used above, just combines those two steps into a single method call. You can actually do those steps as two separate calls of fit then transform, but the single method call of fit_transform is better because it's more computationally efficient and also more readable in my opinion."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 One-hot encoding of multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We saw how to use OneHotEncoder to encode the Embarked column, but we actually need to encode both Embarked and Sex. Thankfully, OneHotEncoder can be applied to multiple features at once.\n",
    "\n",
    "To do this, we simply pass a two-column DataFrame to the fit_transform method, whereas previously we had passed a one-column DataFrame. It outputs 5 columns, in which the first 3 columns represent Embarked and the last 2 columns represent Sex.\n",
    "\n",
    "Looking at the categories attribute, we first see the 3 categories that were learned from Embarked in alphabetical order, and then we see the 2 categories that were learned from Sex in alphabetical order. Thus, we know that a 10 in the last two columns means \"female\", and a 01 in the last two columns means \"male\".\n",
    "\n",
    "So for example, the first sample in the output array is 00101, which means they embarked from S and they are male. The second sample in the array is 10010, which means they embarked from C and they are female. And so on.\n",
    "\n",
    "Recall that our goal in this chapter was to numerically encode Embarked and Sex so we could include them in our model along with Parch and Fare. How might we do that?\n",
    "\n",
    "One idea would be to manually stack Parch and Fare side-by-side with the 5 columns output by OneHotEncoder, and then train the model using all 7 columns. However, we would need to repeat the same exact process of encoding and stacking with the new data, since if you train a model with 7 features, you need the same 7 features in the new data in order to make predictions.\n",
    "\n",
    "This process would work, but it's less than ideal, since repeating the same steps twice is both inefficient and error-prone. Not only that, but the complexity of this process will continue to increase as you preprocess additional features.\n",
    "\n",
    "In the next chapter, I'll introduce you to the ColumnTransformer and Pipeline classes. We'll use these two classes to accomplish our goal of adding Embarked and Sex to our model, but we'll do it in a way that is both reliable and efficient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.381468500Z",
     "start_time": "2023-09-20T01:15:12.610068900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., 0., 1.],\n       [1., 0., 0., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 0., 1.],\n       [0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 1., 0.],\n       [1., 0., 0., 1., 0.]])"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(df[['Embarked', 'Sex']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.382466Z",
     "start_time": "2023-09-20T01:15:12.759670400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[array(['C', 'Q', 'S'], dtype=object), array(['female', 'male'], dtype=object)]"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding the output array:**\n",
    "\n",
    "- **First three columns:**\n",
    "  - 1, 0, 0 means \"C\"\n",
    "  - 0, 1, 0 means \"Q\"\n",
    "  - 0, 0, 1 means \"S\"\n",
    "- **Last two columns:**\n",
    "  - 1, 0 means \"female\"\n",
    "  - 0, 1 means \"male\"\n",
    "- **Example:**\n",
    "  - 0, 0, 1, 0, 1 means \"S, male\"\n",
    "  - 1, 0, 0, 1, 0 means \"C, female\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to manually add Embarked and Sex to the model:**\n",
    "\n",
    "1. Stack Parch and Fare side-by-side with OneHotEncoder output\n",
    "2. Repeat the same process with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems with a manual approach:**\n",
    "\n",
    "- Repeating steps is inefficient and error-prone\n",
    "- Complexity will increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Q&A: When should I use transform instead of fit_transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Earlier in this chapter, we used the fit_transform method of OneHotEncoder to encode two categorical features. In this lesson, I'll show you when it's appropriate to just use the transform method instead of fit_transform. The example below will use OneHotEncoder, but the principles I'm teaching here apply the same way to all transformers.\n",
    "\n",
    "We'll start by creating a DataFrame of training data with just 1 categorical feature. Let's run fit_transform on the entire DataFrame.\n",
    "\n",
    "Recall that fit_transform is really 2 steps. During the first step, which is fit, the OneHotEncoder learns the 3 categories. During the second step, which is transform, the OneHotEncoder creates the feature matrix using those categories. It outputs a 4 by 3 array, since there are 4 samples and 3 categories.\n",
    "\n",
    "Now, we'll create a DataFrame of testing data. It contains the same feature, but that feature includes one less category. What would happen if we ran fit_transform on the testing data?\n",
    "\n",
    "The output array only includes two columns, because the testing data only included two categories. The first column represents the A category, and the second column represents the C category.\n",
    "\n",
    "This is problematic, because if we trained a model using the 3-column feature matrix, and then tried to make predictions on the 2-column feature matrix, it would error due to a shape mismatch. That makes sense because if you train a model such as logistic regression using 3 features, it will learn 3 coefficients, and it expects to use all 3 of those coefficients when making predictions.\n",
    "\n",
    "The solution is to run fit_transform on the training data, and only run transform on the testing data. Let's take a look at the output arrays.\n",
    "\n",
    "Notice that the categories are represented the same way in both arrays: the first column represents A, the second column represents B, and the third column represents C.\n",
    "\n",
    "This happened because we only ran the fit method once, on the training data, and the fit method is when the OneHotEncoder learns the categories.\n",
    "\n",
    "Then we ran the transform method twice, both on the training data and the testing data. Because we didn't run the fit method on the testing data, the categories learned from the training data were applied to both the training and testing data. This is critically important because it means that both our training and testing feature matrices have 3 columns, and those 3 columns mean the same thing.\n",
    "\n",
    "In summary, when using any transformer, you will always use the fit_transform method on the training data and only the transform method on the testing data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.382466Z",
     "start_time": "2023-09-20T01:15:12.955147800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  letter\n0      A\n1      B\n2      C\n3      B",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_train = pd.DataFrame({'letter':['A', 'B', 'C', 'B']})\n",
    "demo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.383464100Z",
     "start_time": "2023-09-20T01:15:13.164586800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of fit_transform on training data:**\n",
    "\n",
    "- **fit:** Learn 3 categories (A, B, C)\n",
    "- **transform:** Create feature matrix with 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.383464100Z",
     "start_time": "2023-09-20T01:15:13.300571500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  letter\n0      A\n1      C\n2      A",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_test = pd.DataFrame({'letter':['A', 'C', 'A']})\n",
    "demo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.384460300Z",
     "start_time": "2023-09-20T01:15:13.449471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0.],\n       [0., 1.],\n       [1., 0.]])"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(demo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of fit_transform on testing data:**\n",
    "\n",
    "- **fit:** Learn 2 categories (A, C)\n",
    "- **transform:** Create feature matrix with 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.384460300Z",
     "start_time": "2023-09-20T01:15:13.615028700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.407398500Z",
     "start_time": "2023-09-20T01:15:13.764824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.]])"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform(demo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correct process:**\n",
    "\n",
    "1. Run fit_transform on training data:\n",
    "  - **fit:** Learn 3 categories (A, B, C)\n",
    "  - **transform:** Create feature matrix with 3 columns\n",
    "2. Run transform on testing data:\n",
    "  - **transform:** Create feature matrix with 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Q&A: What happens if the testing data includes a new category?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the previous lesson, we created this example DataFrame of training data. When we passed that DataFrame to fit_transform, the output array included three columns. We know from the categories attribute that those columns represent the categories A, B, and C.\n",
    "\n",
    "Now we'll create a new DataFrame of testing data that includes a category, D, which was not seen in the training data.\n",
    "\n",
    "If you pass this new DataFrame to the transform method, it will throw an error because it doesn't know how to represent the D category. It only knows how to represent the A, B, and C categories because those are the ones that were seen by the OneHotEncoder during the fit step.\n",
    "\n",
    "There are two possible solutions to this problem.\n",
    "\n",
    "The first solution is to specify the categories manually to the OneHotEncoder when creating an instance. Then, when fit_transform is run on the training data, a column is reserved for each of the four categories. As a result, the transform on the testing data will no longer error.\n",
    "\n",
    "However, specifying the categories manually is only a useful solution if you know all possible categories that might ever appear in your data. But in the real world, you don't always know the full set of categories ahead of time.\n",
    "\n",
    "For example, there might be rare categories that aren't present in your set of samples, or new categories might be added in the future. For example, if one of your categorical features was medical billing codes, you could imagine that new billing codes are added over time.\n",
    "\n",
    "If you don't know all possible categories, then the solution is to set the handle_unknown parameter of the OneHotEncoder to ignore, which overrides the default value of error.\n",
    "\n",
    "Let's use the fit_transform method on our training data one more time. The output array includes three columns representing A, B, and C. Now, when you use the transform method on the testing data, the third sample is encoded as all zeros because D is an unknown category.\n",
    "\n",
    "Although this might seem strange, this is actually quite a reasonable approach since you don't have any information from the training data about the relationship between the D category and the target value.\n",
    "\n",
    "One limitation of this approach, however, is that all unknown categories will be encoded the same way, which means that an E value in the testing data would also be encoded as all zeros.\n",
    "\n",
    "Here's my overall advice:\n",
    "\n",
    "1. When starting a project, keep the handle_unknown parameter set to its default value of error so that you will know if you are encountering new categories in your testing data.\n",
    "\n",
    "2. If you do find that you're encountering new categories, but you can determine the full set of categories through research, then define the categories manually when creating the OneHotEncoder instance.\n",
    "\n",
    "3. If you can't determine the full set of categories, then set the handle_unknown parameter to ignore. However, you should retrain your model as soon as possible using data that includes those new categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.409393700Z",
     "start_time": "2023-09-20T01:15:13.929751100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  letter\n0      A\n1      B\n2      C\n3      B",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.548024200Z",
     "start_time": "2023-09-20T01:15:14.093830200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.738515200Z",
     "start_time": "2023-09-20T01:15:14.244402200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[array(['A', 'B', 'C'], dtype=object)]"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.740508900Z",
     "start_time": "2023-09-20T01:15:14.378301500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  letter\n0      A\n1      C\n2      D",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_test_unknown = pd.DataFrame({'letter':['A', 'C', 'D']})\n",
    "demo_test_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:15:15.910054300Z",
     "start_time": "2023-09-20T01:15:14.512764Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['D'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17284\\3333860145.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mohe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdemo_test_unknown\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\GitHub\\venvs\\DeepLearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    511\u001B[0m             \u001B[0mhandle_unknown\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle_unknown\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m             \u001B[0mforce_all_finite\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"allow-nan\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 513\u001B[1;33m             \u001B[0mwarn_on_unknown\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwarn_on_unknown\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    514\u001B[0m         )\n\u001B[0;32m    515\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\GitHub\\venvs\\DeepLearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001B[0m in \u001B[0;36m_transform\u001B[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001B[0m\n\u001B[0;32m    140\u001B[0m                         \u001B[1;34m\" during transform\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdiff\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m                     )\n\u001B[1;32m--> 142\u001B[1;33m                     \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    143\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mwarn_on_unknown\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Found unknown categories ['D'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "ohe.transform(demo_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.692798100Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, categories=[['A', 'B', 'C', 'D']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.697784800Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.702773100Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe.transform(demo_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why you might not know all possible categories:**\n",
    "\n",
    "- Rare categories aren't present in your set of samples\n",
    "- New categories are added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.707759Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.712745200Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.716733700Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe.transform(demo_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice for OneHotEncoder:**\n",
    "\n",
    "1. Start with handle_unknown set to 'error'\n",
    "2. If possible, specify the categories manually\n",
    "3. If necessary, set handle_unknown to 'ignore' and then retrain your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Q&A: Should I drop one of the one-hot encoded categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the example training data that we've used in the past few lessons. And here's the default one-hot encoding of this DataFrame.\n",
    "\n",
    "When one-hot encoding, it's somewhat common to drop the first column of the output array because it contains redundant information and because it avoids collinearity between features.\n",
    "\n",
    "If you want to drop the first column, you can set the OneHotEncoder's drop parameter to first, though this option only exists in scikit-learn version 0.21 and later. When you run the fit_transform, you can see that the output array contains 1 less column. However, the new encoding retains the same information, since each category is still represented by a unique code: 00 means A, 10 means B, and 01 means C.\n",
    "\n",
    "Dropping the first column will work regardless of the number of categories, but you're only ever allowed to drop a single column. And it doesn't actually matter which column you drop, though the convention is to drop the first column.\n",
    "\n",
    "You've now seen that you can drop the first column, but the question is, should you drop the first column? Here's my advice.\n",
    "\n",
    "If you know that perfectly collinear features will cause problems, such as when feeding the resulting data into a neural network or an unregularized regression, then it's a good idea to drop the first column. However, for most scikit-learn models, perfectly collinear features will not cause any problems, and thus dropping the first column will not benefit the model.\n",
    "\n",
    "There are also some significant downsides to dropping the first column that you need to be aware of.\n",
    "\n",
    "Number one, dropping the first column is incompatible with ignoring unknown categories, which is the handle_unknown='ignore' option that we saw in the previous lesson, since the dropped category and unknown categories would both be encoded as all zeros. You are allowed to do this starting in scikit-learn 1.0, but I still don't recommend it.\n",
    "\n",
    "Number two, dropping the first column can introduce bias into the model if you standardize your features, such as with StandardScaler, or if you use a regularized model, such as logistic regression, since the dropped category will be exempt from standardization and regularization.\n",
    "\n",
    "In summary, I recommend that you drop the first column only if you know that perfectly collinear features will cause problems, otherwise I don't recommend dropping the first column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.723715300Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.728702300Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can drop the first column:**\n",
    "\n",
    "- Contains redundant information\n",
    "- Avoids collinearity between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.732691500Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe.fit_transform(demo_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding the output array (after dropping the first column):**\n",
    "\n",
    "- 0, 0 means \"A\"\n",
    "- 1, 0 means \"B\"\n",
    "- 0, 1 means \"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should you drop the first column?**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Useful if perfectly collinear features will cause problems (does not apply to most models)\n",
    "- **Disadvantages:**\n",
    "  - Incompatible with handle_unknown='ignore'\n",
    "  - Introduces bias if you standardize features or use a regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Q&A: How do I encode an ordinal feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of categorical data:**\n",
    "\n",
    "- Unordered (nominal data)\n",
    "- Ordered (ordinal data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Throughout this chapter, we used one-hot encoding to encode unordered categorical features, also known as nominal data. But how should you encode categorical features with an inherent logical ordering, also known as ordinal data? That's the subject of this lesson.\n",
    "\n",
    "Let's take a look at our Titanic DataFrame.\n",
    "\n",
    "Pclass, which stands for passenger class, is an ordinal feature. Although it's already numeric, the numbers 1, 2, and 3 represent the categories 1st class, 2nd class, and 3rd class. It's considered ordinal data because there is a logical ordering to the categories.\n",
    "\n",
    "Our intuition is that there may be a relationship between Pclass values increasing and survival rate decreasing, because passengers in the lower-numbered classes may have gotten priority access to lifeboats. Thus if we were going to include Pclass in the model, we would keep the existing numeric encoding so that the model can learn the relationship between Pclass and Survived with a single feature. You could use one-hot encoding with Pclass instead, but the model wouldn't be able to learn that relationship as effectively because that information would be spread out across three features.\n",
    "\n",
    "Let's create an example DataFrame to see how to handle ordinal features that are stored as strings.\n",
    "\n",
    "In this DataFrame, we have two ordinal features, Class and Size. If you have ordinal data, you should use the OrdinalEncoder class to do the encoding. First, you import it from the preprocessing module. Then, you create an instance of OrdinalEncoder, and when you do so, you define the logical order of the categories.\n",
    "\n",
    "We pass a list of lists to the categories parameter, in which the first inner list is the categories for the Class feature, and the second inner list is the categories for the Size feature. I put the two lists in that order because that is the order in which I'll be passing the features to the fit_transform method.\n",
    "\n",
    "One important note is that I included the M category for Size even though it wasn't present in this DataFrame because I knew that it would occur in the dataset at some point.\n",
    "\n",
    "Next, we pass the DataFrame to the OrdinalEncoder's fit_transform method in order to do the encoding. You'll notice that each input feature became a single feature in the output array.\n",
    "\n",
    "For the Class feature, first was encoded as 0, second was encoded as 1, and third was encoded as 2.\n",
    "\n",
    "For the Size feature, S was encoded as 0, L was encoded as 2, and XL was encoded as 3. And if M appears in the data at some point, it will be encoded as 1.\n",
    "\n",
    "Again, we encoded each input feature as a single column so that the model can learn the relationship between the target and an increase or decrease in each feature.\n",
    "\n",
    "Let's briefly contrast this with the output you would get if you used OneHotEncoder with these same two features.\n",
    "\n",
    "OneHotEncoder would create 7 columns in the output array, since Class has 3 categories and Size has 4 categories. These 7 columns contain the same information as the 2 columns output by OrdinalEncoder, but the model would have a comparatively harder time learning from the 7 columns since the information is expressed in a less compact form.\n",
    "\n",
    "Here's a summary of my advice on this topic:\n",
    "\n",
    "1. If you have an ordinal feature that's already encoded numerically, then leave it as-is.\n",
    "2. If you have an ordinal feature that's stored as strings, then encode it using OrdinalEncoder.\n",
    "3. If you have a nominal feature, then encode it using OneHotEncoder.\n",
    "\n",
    "In chapter 17, we'll explore this topic further and see if there are cases in which you should diverge from this advice.\n",
    "\n",
    "One final note about OrdinalEncoder is that unlike OneHotEncoder, it does not allow for new categories in the testing data that were not seen during training. However, that functionality is available beginning in scikit-learn version 0.24 using a handle_unknown parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.737678Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for encoding Pclass:**\n",
    "\n",
    "- **Ordinal encoding:** Creates one feature\n",
    "- **One-hot encoding:** Creates three features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.751640900Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ordinal = pd.DataFrame({'Class': ['third', 'first', 'second', 'third'],\n",
    "                           'Size': ['S', 'S', 'L', 'XL']})\n",
    "df_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.756140Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder(categories=[['first', 'second', 'third'],\n",
    "                                ['S', 'M', 'L', 'XL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.759132100Z"
    }
   },
   "outputs": [],
   "source": [
    "oe.fit_transform(df_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding the output array:**\n",
    "\n",
    "- **First column:**\n",
    "  - 0 means \"first\"\n",
    "  - 1 means \"second\"\n",
    "  - 2 means \"third\"\n",
    "- **Second column:**\n",
    "  - 0 means \"S\"\n",
    "  - 1 means \"M\"\n",
    "  - 2 means \"L\"\n",
    "  - 3 means \"XL\"\n",
    "- **Example:**\n",
    "  - 2, 0 means \"third, S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.763121700Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, categories=[['first', 'second', 'third'],\n",
    "                                              ['S', 'M', 'L', 'XL']])\n",
    "ohe.fit_transform(df_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice for encoding categorical data:**\n",
    "\n",
    "- **Ordinal feature stored as numbers:** Leave as-is\n",
    "- **Ordinal feature stored as strings:** Use OrdinalEncoder\n",
    "- **Nominal feature:** Use OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Q&A: What's the difference between OrdinalEncoder and LabelEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many similarities between the OrdinalEncoder and LabelEncoder classes, so in this lesson I'll explain how they're different and why you should be using OrdinalEncoder, not LabelEncoder.\n",
    "\n",
    "The first main difference is that OrdinalEncoder allows you to define the order of the categories, whereas LabelEncoder does not. LabelEncoder simply uses the alphabetical order of the values you pass to it to determine which value to encode as 0, which value to encode as 1, and so on.\n",
    "\n",
    "The second main difference is that OrdinalEncoder can be used to encode multiple features at once, whereas LabelEncoder can only encode one column of data at once.\n",
    "\n",
    "Because of these differences, OrdinalEncoder is much better suited than LabelEncoder for encoding ordinal features. And in fact, LabelEncoder is only intended for the encoding of class labels, hence its name.\n",
    "\n",
    "You might be asking why LabelEncoder even exists, given its limitations. There are two reasons.\n",
    "\n",
    "First, in older versions of scikit-learn, some classification models were not able to handle string-based labels. LabelEncoder was used to encode those strings as integers so that they could be passed to the model. That limitation was removed a few years ago, and so all scikit-learn classifiers can now handle string-based labels. Therefore, you should never need to use LabelEncoder for encoding your class labels.\n",
    "\n",
    "Second, also in order versions of scikit-learn, OneHotEncoder did not accept strings as input. Thus if you had categorical data stored as strings, you actually had to use LabelEncoder to encode the strings as integers before passing them to the OneHotEncoder. Again, that limitation was removed a few years ago, and so you can pass string-based categorical data directly to OneHotEncoder.\n",
    "\n",
    "Because of this legacy from older versions of scikit-learn, many people are familiar with LabelEncoder and thus use it to encode features. However, the best practice is to use OrdinalEncoder to encode ordinal features. In fact, it's rare that you will ever need to use LabelEncoder, which is why I'm not using it in this course."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; | OrdinalEncoder | LabelEncoder\n",
    ":--- | :---: | :---:\n",
    "Can you define the category order? | Yes | No\n",
    "Can you encode multiple features? | Yes | No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outdated uses for LabelEncoder:**\n",
    "\n",
    "- Encoding string-based labels for some classifiers\n",
    "- Encoding string-based features for OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Q&A: Should I encode numeric features as ordinal features?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normally, when you have a continuous numeric feature such as Fare, you pass that feature directly to your Machine Learning model. However, one strategy that is sometimes used with numeric features is to \"discretize\" or \"bin\" them into categorical features. In scikit-learn, we can do this using KBinsDiscretizer.\n",
    "\n",
    "When creating an instance of KBinsDiscretizer, you define the number of bins, the binning strategy, and the method used for encoding the result. Here's the output when we pass the Fare feature to the fit_transform method.\n",
    "\n",
    "Because we specified 3 bins, every sample has been assigned to bin 0 or 1 or 2. The smallest values were assigned to bin 0, the largest values were assigned to bin 2, and the values in between were assigned to bin 1. Thus, we've taken a continuous numeric feature and encoded it as an ordinal feature, and this ordinal feature could be passed to the model in place of the numeric feature.\n",
    "\n",
    "The obvious follow-up question is: Should we discretize our numeric features? Theoretically, discretization can benefit linear models by helping them to learn non-linear trends. However, my general recommendation is to not use discretization, for three main reasons.\n",
    "\n",
    "First, discretization removes all nuance from the data, which makes it harder for a model to learn the actual trends that are present in the data.\n",
    "\n",
    "Second, discretization reduces the variation in the data, which makes it easier to find trends that don't actually exist.\n",
    "\n",
    "Third, any possible benefits of discretization are highly dependent on the parameters used with KBinsDiscretizer. Making those decisions by hand creates a risk of overfitting the training data, and making those decisions during a tuning process adds both complexity and processing time, and so neither of those options is particularly attractive to me."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.767111500Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['Fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.774091500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.777084200Z"
    }
   },
   "outputs": [],
   "source": [
    "kb = KBinsDiscretizer(n_bins=3, strategy='quantile', encode='ordinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T01:15:14.781073500Z"
    }
   },
   "outputs": [],
   "source": [
    "kb.fit_transform(df[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not discretize numeric features?**\n",
    "\n",
    "- Makes it harder to learn the actual trends\n",
    "- Makes it easier to discover non-existent trends\n",
    "- May result in overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Improving your workflow with ColumnTransformer and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing features with ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the last chapter, our goal was to include two numeric features and two categorical features in our model. We saw how to numerically encode the categorical features using OneHotEncoder, but we lacked an efficient process for stacking those encoded features next to the numerical features, and we lacked an efficient way to apply this same preprocessing to our new data.\n",
    "\n",
    "In this chapter, we're going to solve both of those problems using the ColumnTransformer and Pipeline classes:\n",
    "\n",
    "- ColumnTransformer will make it easy to apply different preprocessing steps to different columns.\n",
    "- Pipeline will make it easy to apply the same workflow to training data and new data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems from Chapter 3:**\n",
    "\n",
    "- Need to stack categorical features next to numerical features\n",
    "- Need to apply the same preprocessing to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to solve those problems:**\n",
    "\n",
    "- **ColumnTransformer:** Apply different preprocessing steps to different columns\n",
    "- **Pipeline:** Apply the same workflow to training data and new data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start, we'll create a Python list of the four columns we've been working with, and use that to create our X object.\n",
    "\n",
    "We're still going to be one-hot encoding the Embarked and Sex columns, so we'll create an instance of OneHotEncoder. We're using the default options for OneHotEncoder, which means it will output a sparse matrix, but that's fine because we're not going to examine the output directly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:41:38.278094900Z",
     "start_time": "2023-09-20T01:41:38.179600400Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T01:41:41.481439200Z",
     "start_time": "2023-09-20T01:41:41.230439600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Parch     Fare Embarked     Sex\n0      0   7.2500        S    male\n1      0  71.2833        C  female\n2      0   7.9250        S  female\n3      0  53.1000        S  female\n4      0   8.0500        S    male\n5      0   8.4583        Q    male\n6      0  51.8625        S    male\n7      1  21.0750        S    male\n8      2  11.1333        S  female\n9      0  30.0708        C  female",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>8.4583</td>\n      <td>Q</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>51.8625</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>21.0750</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>11.1333</td>\n      <td>S</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>30.0708</td>\n      <td>C</td>\n      <td>female</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[cols]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:42:09.645369500Z",
     "start_time": "2023-09-20T01:42:09.526195600Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it's time to create our first ColumnTransformer, which will take care of any data transformations that we specify. We'll start by importing the make_column_transformer function from the compose module.\n",
    "\n",
    "In general, you use make_column_transformer by passing it one or more tuples, and each tuple should have two elements:\n",
    "\n",
    "1. The first element is a transformer.\n",
    "2. The second element is a list of columns to which that transformer should be applied. Note that in most cases, this element should be a list even if you are only specifying a single column.\n",
    "\n",
    "In our case, we'll pass it a single tuple in which the first element is our OneHotEncoder object and the second element is a list of the two columns we want to one-hot encode.\n",
    "\n",
    "After all tuples, we'll set the remainder parameter to drop, which means that all columns which are not explicitly mentioned in the ColumnTransformer should be dropped. Drop is actually the default value for remainder, but I'm including it here just for clarity.\n",
    "\n",
    "Note that I could have defined the ColumnTransformer on a single line, but I prefer breaking the lines in this way for readability.\n",
    "\n",
    "When we run this code, the make_column_transformer code returns a ColumnTransformer object, which we'll save as ct.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:43:24.757141600Z",
     "start_time": "2023-09-20T01:43:22.999002700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuple elements for make_column_transformer:**\n",
    "\n",
    "1. Transformer object\n",
    "2. List of columns to which the transformer should be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll perform the transformation by passing X, which is our four-column DataFrame, to the fit_transform method of the ct object. It outputs a 10 by 5 array that represents the one-hot encoding of the Embarked and Sex columns. The first three columns represent Embarked and the other two columns represent Sex, and they're in that order because that's the order in which they were listed in the ColumnTransformer.\n",
    "\n",
    "Note that even though the Parch and Fare columns are part of X, they're excluded from the output array because we told the ColumnTransformer to drop all unspecified columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:47:17.586877700Z",
     "start_time": "2023-09-20T01:47:17.180889800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., 0., 1.],\n       [1., 0., 0., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 0., 1.],\n       [0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 1., 0.],\n       [1., 0., 0., 1., 0.]])"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output columns:**\n",
    "\n",
    "- **Columns 1-3:** Embarked\n",
    "- **Columns 4-5:** Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is nice, but our actual goal was to create a matrix that includes the Parch and Fare columns alongside the encoded versions of Embarked and Sex. To accomplish that, we'll simply change the value of remainder from drop to passthrough. This means that all columns which are not mentioned in the ColumnTransformer should be passed through to the output unmodified. In other words, include the Parch and Fare columns in the output, but don't transform them in any way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:48:32.328824800Z",
     "start_time": "2023-09-20T01:48:32.132282300Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we run the fit_transform method this time, it outputs a 10 by 7 array. The first five columns represent the encoded Embarked and Sex columns, and the sixth and seventh columns are the Parch and Fare columns. The column order is based on the order in which the columns were listed in the ColumnTransformer, followed by any you passthrough."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:48:49.599892400Z",
     "start_time": "2023-09-20T01:48:49.420373800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output columns:**\n",
    "\n",
    "- **Columns 1-3:** Embarked\n",
    "- **Columns 4-5:** Sex\n",
    "- **Column 6:** Parch\n",
    "- **Column 7:** Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We were able to figure out on our own what each column represents, but you can also use the ColumnTransformer's get_feature_names method to confirm the meanings of these 7 features. The x0 simply means feature 0 that was passed to the OneHotEncoder, and the x1 means feature 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:49:20.485121900Z",
     "start_time": "2023-09-20T01:49:20.156001500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhasa\\GitHub\\venvs\\DeepLearning\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "['onehotencoder__x0_C',\n 'onehotencoder__x0_Q',\n 'onehotencoder__x0_S',\n 'onehotencoder__x1_female',\n 'onehotencoder__x1_male',\n 'Parch',\n 'Fare']"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we move on, I have two quick asides about the get_feature_names method:\n",
    "\n",
    "- First, the get_feature_names method didn't work with passthrough columns prior to scikit-learn version 0.23, so you'll get an error if you run the code with previous versions.\n",
    "- Second, the get_feature_names method has been replaced with a similar method called get_feature_names_out beginning in scikit-learn 1.0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes about get_feature_names:**\n",
    "\n",
    "- **Before version 0.23:** Didn't work with passthrough columns\n",
    "- **Starting in version 1.0:** Has been replaced with get_feature_names_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuple elements for make_column_transformer (revised):**\n",
    "\n",
    "1. Transformer object or \"drop\" or \"passthrough\"\n",
    "2. List of columns to which the transformer should be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To wrap up this lesson, I want to show you one other way to specify this same ColumnTransformer.\n",
    "\n",
    "As I mentioned before, make_column_transformer accepts tuples, and the first element of each tuple is usually a transformer object (like our \"ohe\" object). However, the first element of the tuple can also be the special strings \"drop\" or \"passthrough\", which tells the ColumnTransformer to drop or passthrough specific columns.\n",
    "\n",
    "So, we're going to add a second tuple in which the transformer is the string \"passthrough\", and we want to apply this passthrough transformer to the columns Parch and Fare. This ColumnTransformer will do the exact same thing as the previous one, but I actually prefer this notation any time I have a small number of passthrough columns, since it reminds me of which columns I'm passing through.\n",
    "\n",
    "It's still important to remember that the default value for the remainder parameter is \"drop\", which means that any unspecified columns will be dropped, though we don't have any unspecified columns in this case.\n",
    "\n",
    "We'll run the fit_transform method one more time, and you can see that it outputs the same 7 columns as before. And to be clear, this is the feature matrix that we will pass to our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:51:14.733275800Z",
     "start_time": "2023-09-20T01:51:14.620119600Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('passthrough', ['Parch', 'Fare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T01:51:17.105251700Z",
     "start_time": "2023-09-20T01:51:16.976991600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Chaining steps with Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the previous lesson, we accomplished our first goal, which was to apply different preprocessing to different columns using ColumnTransformer. In this lesson, we're moving on to our second goal, which is to apply the same workflow to training data and new data using the Pipeline class.\n",
    "\n",
    "A Pipeline is used to chain together sequential steps. In this case, we want to chain together two steps, namely data preprocessing followed by model building.\n",
    "\n",
    "We'll start by importing the make_pipeline function. Then, we can create a Pipeline instance by passing it two objects: our ColumnTransformer instance for data preprocessing, and our logistic regression instance for model building. We'll save it as an object called \"pipe\", which is a 2-step Pipeline.\n",
    "\n",
    "You might remember that back in Chapter 2, we used cross-validation to evaluate our model when it only included the Parch and Fare features. Now that we've added the Embarked and Sex features, it would normally make sense to cross-validate the updated model to see whether the adding those features made our model better or worse. And in fact, you can (and should) cross-validate an entire Pipeline.\n",
    "\n",
    "However, any model evaluation procedure is highly unreliable with only 10 rows of data, and so any change in the cross-validated accuracy would be misleading. Thus we're going to skip the cross-validation step for the moment, though we'll return to it in a later chapter once we're using the full dataset.\n",
    "\n",
    "Since we're skipping cross-validation, our next step is just to run the fit method on the Pipeline, and pass it X and y. Here's what happens when we fit the Pipeline:\n",
    "\n",
    "- First, it runs the ColumnTransformer step, meaning that it takes X, which is a 4-column DataFrame that contains both numbers and strings, and transforms it into the 7-column feature matrix that only includes numbers.\n",
    "- Second, it runs the LogisticRegression step, meaning that the model is fit to this 7-column feature matrix. In other words, it learns the relationship between those 7 features and the y values.\n",
    "\n",
    "Note that when you fit a Pipeline, it will actually print out the steps. You can see that step 1 is a ColumnTransformer that includes a OneHotEncoder and a passthrough transformer, and step 2 is a LogisticRegression model.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:23:17.624267900Z",
     "start_time": "2023-09-20T02:23:17.474850700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(ct, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline steps:**\n",
    "\n",
    "1. Data preprocessing with ColumnTransformer\n",
    "2. Model building with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:24:50.239238400Z",
     "start_time": "2023-09-20T02:24:49.934689300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Pipeline:**\n",
    "\n",
    "1. ColumnTransformer converts X (4 columns) into a numeric feature matrix (7 columns)\n",
    "2. LogisticRegression model is fit to the feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case it helps you to understand the Pipeline better, I'm going to show you what happens \"under the hood\" when you fit this Pipeline. To be clear, you should not actually write the following code, rather it is just for teaching purposes.\n",
    "\n",
    "First, X is transformed by the ColumnTransformer into X_t, which stands for X transformed. Second, the LogisticRegression model is fit on X_t and y. And as you would expect, X has the shape 10 by 4, and X_t has the shape 10 by 7."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:26:48.927130700Z",
     "start_time": "2023-09-20T02:26:48.770908300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(random_state=1, solver='liblinear')"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = ct.fit_transform(X)\n",
    "logreg.fit(X_t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:27:00.275985900Z",
     "start_time": "2023-09-20T02:27:00.144246800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Using the Pipeline to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've fit our Pipeline, we want to use it to make predictions on new data.\n",
    "\n",
    "The first step is to update the X_new DataFrame so that it contains the same columns as X. Recall that the cols object contains the names of our four columns, and so we can use it to select those four columns from the df_new DataFrame.\n",
    "\n",
    "Now, we can pass X_new to the Pipeline's predict method to make predictions for these ten samples. When we run it, the Pipeline applies the same transformations to X_new that it applied to X, and the transformed version of X_new is passed to the fitted logistic regression model so that it can make predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:28:15.659073600Z",
     "start_time": "2023-09-20T02:28:15.419697300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Parch     Fare Embarked     Sex\n0      0   7.8292        Q    male\n1      0   7.0000        S  female\n2      0   9.6875        Q    male\n3      0   8.6625        S    male\n4      1  12.2875        S  female\n5      0   9.2250        S    male\n6      0   7.6292        Q  female\n7      1  29.0000        S    male\n8      0   7.2292        C  female\n9      0  24.1500        S    male",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>Q</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>S</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>Q</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>S</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>9.2250</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>7.6292</td>\n      <td>Q</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>29.0000</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>7.2292</td>\n      <td>C</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>24.1500</td>\n      <td>S</td>\n      <td>male</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = df_new[cols]\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In other words, the Pipeline enabled us to accomplish our second goal, which is to apply the same workflow to training data and new data.\n",
    "\n",
    "As a reminder, we can't evaluate the accuracy of these ten predictions because we don't know the true target values for X_new."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:28:49.407092700Z",
     "start_time": "2023-09-20T02:28:48.927669800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting with the Pipeline:**\n",
    "\n",
    "1. ColumnTransformer applies the same transformations to X_new\n",
    "2. Fitted LogisticRegression model makes predictions on the transformed version of X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just like before, I'm going to show you what happens \"under the hood\" when you make predictions using this Pipeline. Again, you should not actually write the following code, rather it is just for teaching purposes.\n",
    "\n",
    "First, X_new is transformed by the ColumnTransformer into X_new_t, which stands for X_new transformed. Second, the fitted LogisticRegression model makes predictions for the samples in X_new_t. And as you would expect, X_new has the shape 10 by 4, and X_new_t has the shape 10 by 7."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:29:15.756431800Z",
     "start_time": "2023-09-20T02:29:15.546923700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_t = ct.transform(X_new)\n",
    "logreg.predict(X_new_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:29:19.354163600Z",
     "start_time": "2023-09-20T02:29:19.167664100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_new.shape)\n",
    "print(X_new_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ColumnTransformer methods:**\n",
    "\n",
    "1. Run fit_transform on X:\n",
    "  - **fit:** Learn the encoding\n",
    "  - **transform:** Apply the encoding to create 7 columns\n",
    "2. Run transform on X_new:\n",
    "  - **transform:** Apply the encoding to create 7 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "One important point I want to highlight is that the Pipeline's predict method called the ColumnTransformer's transform method, not its fit_transform method. Why would that be?\n",
    "\n",
    "Recall that the fit step is when a transformer learns something, and the transform step is when it uses what it learned to do the transformation. Thus you fit on X to learn an encoding, and you transform on X and X_new to apply that encoding.\n",
    "\n",
    "This is critically important. Our logistic regression model was fit on 7 columns, and so it learned 7 coefficients. To make predictions, you need to pass 7 columns to the predict method, and those 7 columns need to mean the same thing as the 7 columns you used when fitting the model. Thus, the predict method only runs transform so that the exact same encoding will be applied to the training data and the new data.\n",
    "\n",
    "It's okay if you're still a bit fuzzy on the difference between fit and transform, because the Pipeline object will just do the right thing for you when you run fit or predict. However, understanding the difference will ultimately help you to go further with scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Q&A: How do I drop some columns and passthrough others?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently we only have 4 columns in X, namely Parch, Fare, Embarked, and Sex. But imagine that we had many more columns, and we wanted to drop a few columns and passthrough the rest. How would we do that efficiently?\n",
    "\n",
    "We can use the special string \"drop\" to tell the ColumnTransformer which columns to drop, and also tell it to passthrough all remaining columns. So in this example, we're one-hot encoding Embarked and Sex, which creates 5 columns, dropping Fare, and passing through Parch, which adds 1 more column.\n",
    "\n",
    "We could use this same pattern to drop a few columns and passthrough hundreds of columns without having to list the passthrough columns one-by-one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:31:42.877941100Z",
     "start_time": "2023-09-20T02:31:42.653332300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 2.],\n       [1., 0., 0., 1., 0., 0.]])"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('drop', ['Fare']),\n",
    "    remainder='passthrough')\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conversely, we might want to passthrough a few columns and drop the rest. We can use the special string \"passthrough\" to tell the ColumnTransformer which columns to passthrough, and also tell it to drop all remaining columns. So in this example, we're one-hot encoding Embarked and Sex, which creates 5 columns, passing through Parch, which adds 1 more column, and dropping Fare.\n",
    "\n",
    "Again, we can use this pattern to passthrough a few columns and drop hundreds of columns without listing them all.\n",
    "\n",
    "Finally, just a reminder that \"drop\" is the default value for remainder, so you aren't actually required to specify it here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:32:29.599183300Z",
     "start_time": "2023-09-20T02:32:29.359826500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 2.],\n       [1., 0., 0., 1., 0., 0.]])"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('passthrough', ['Parch']),\n",
    "    remainder='drop')\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Q&A: How do I transform the unspecified columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know how to drop or passthrough the unspecified columns in a ColumnTransformer, but let's pretend we wanted to apply a transformation to all of the unspecified columns. This is actually simple to do by passing a transformer to the remainder parameter.\n",
    "\n",
    "For example, we might want to scale all of the unspecified columns. One option is MaxAbsScaler, which divides each feature by its maximum value and thus scales it to the range negative 1 to positive 1. We'll import it from the preprocessing module and then create an instance. Then, we can pass the scaler to the remainder parameter.\n",
    "\n",
    "When we run the fit_transform method, you can see that the first 5 columns were created from Embarked and Sex, and the sixth column is the scaled version of the Parch column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:33:30.252415400Z",
     "start_time": "2023-09-20T02:33:30.011390600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:33:32.050019700Z",
     "start_time": "2023-09-20T02:33:31.845233200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0. , 0. , 1. , 0. , 1. , 0. ],\n       [1. , 0. , 0. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0. ],\n       [0. , 1. , 0. , 0. , 1. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0.5],\n       [0. , 0. , 1. , 1. , 0. , 1. ],\n       [1. , 0. , 0. , 1. , 0. , 0. ]])"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('drop', ['Fare']),\n",
    "    remainder=scaler)\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Q&A: How do I select columns from a NumPy array?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Throughout the course, we've been using a pandas DataFrame as our input. But what if your input data was a NumPy array instead? Let's see how that affects our workflow.\n",
    "\n",
    "We'll start by converting the X and X_new DataFrames into NumPy arrays called X_array and X_new_array. Here's what X_array looks like."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:34:21.807407400Z",
     "start_time": "2023-09-20T02:34:21.560083500Z"
    }
   },
   "outputs": [],
   "source": [
    "X_array = X.to_numpy()\n",
    "X_new_array = X_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:34:24.748710400Z",
     "start_time": "2023-09-20T02:34:24.564407800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 7.25, 'S', 'male'],\n       [0, 71.2833, 'C', 'female'],\n       [0, 7.925, 'S', 'female'],\n       [0, 53.1, 'S', 'female'],\n       [0, 8.05, 'S', 'male'],\n       [0, 8.4583, 'Q', 'male'],\n       [0, 51.8625, 'S', 'male'],\n       [1, 21.075, 'S', 'male'],\n       [2, 11.1333, 'S', 'female'],\n       [0, 30.0708, 'C', 'female']], dtype=object)"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If this was our input data, and we wanted to use a ColumnTransformer, we wouldn't be able to specify the columns by name because columns of a NumPy array don't have names. However, we do have a couple of other options.\n",
    "\n",
    "First, we could specify the columns by integer position. Embarked and Sex are columns 2 and 3, so in this example, we're one-hot encoding Embarked and Sex and passing through the remainder. Note that we're passing X_array, not X, to the fit_transform method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:35:06.829302500Z",
     "start_time": "2023-09-20T02:35:06.664219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.0, 0.0, 1.0, 0.0, 1.0, 0, 7.25],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 71.2833],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 7.925],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 53.1],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 8.05],\n       [0.0, 1.0, 0.0, 0.0, 1.0, 0, 8.4583],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 51.8625],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 1, 21.075],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 2, 11.1333],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 30.0708]], dtype=object)"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, [2, 3]),\n",
    "    remainder='passthrough')\n",
    "ct.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another option is to specify the columns using slices, which is useful for large ranges of columns next to one another. In this case, we're selecting columns 2 through 3 for one-hot encoding, and passing through the remainder. Remember that Python slices are inclusive of the starting value, which is 2 in this case, and exclusive of the ending value, which is 4 in this case.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, slice(2, 4)),\n",
    "    remainder='passthrough')\n",
    "ct.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "One final option is to specify the columns using a boolean mask. Normally you would create the mask using some sort of condition, but in this case I'm just writing out a mask to select columns 2 and 3 for one-hot encoding, and passing through the remainder.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, [False, False, True, True]),\n",
    "    remainder='passthrough')\n",
    "ct.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So those are our three options for selecting columns in a ColumnTransformer when your input source is a NumPy array.\n",
    "\n",
    "Other than that, the rest of our workflow remains the same. We'll just update the Pipeline to use our new ColumnTransformer. Then we can fit the Pipeline with X_array and y, and make predictions for the X_new_array."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for selecting columns from a NumPy array:**\n",
    "\n",
    "- Integer position\n",
    "- Slice\n",
    "- Boolean mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:36:30.600093900Z",
     "start_time": "2023-09-20T02:36:30.494235900Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:36:33.319087100Z",
     "start_time": "2023-09-20T02:36:33.067576400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_array, y)\n",
    "pipe.predict(X_new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Q&A: How do I select columns by data type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:37:23.095572200Z",
     "start_time": "2023-09-20T02:37:22.940100700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far in the course, we've been selecting columns one-by-one. But let's say that we had many more columns, and we simply wanted to one-hot encode all object columns and passthrough all numeric columns without listing all of them out. How would we do that?\n",
    "\n",
    "The easiest way to do this is with the make_column_selector function, which is new in scikit-learn version 0.22.\n",
    "\n",
    "We're going to create two column selectors called select_object and select_number. To do this, we just set the dtype_include parameter to the data type we want to include, and it outputs a callable. Then, we pass the callables to make_column_transformer instead of the column names, and the callables select the columns for us."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:37:29.964410Z",
     "start_time": "2023-09-20T02:37:29.768562700Z"
    }
   },
   "outputs": [],
   "source": [
    "select_object = make_column_selector(dtype_include=object)\n",
    "select_number = make_column_selector(dtype_include='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we run fit_transform, you can see that once again, the object columns have been one-hot encoded and the numeric columns have been passed through.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:37:52.352155Z",
     "start_time": "2023-09-20T02:37:52.087944400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, select_object),\n",
    "    ('passthrough', select_number))\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "One slight variation of this is that you can tell make_column_selector to exclude instead of include a specific data type. In this example, we're using the dtype_exclude parameter to create a column selector that excludes the object data type.\n",
    "\n",
    "This time, we'll tell the ColumnTransformer to one-hot encode all object columns and passthrough all non-object columns, which has the same effect as before."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:38:24.894515700Z",
     "start_time": "2023-09-20T02:38:24.455191900Z"
    }
   },
   "outputs": [],
   "source": [
    "exclude_object = make_column_selector(dtype_exclude=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:38:29.123486700Z",
     "start_time": "2023-09-20T02:38:28.803830600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, select_object),\n",
    "    ('passthrough', exclude_object))\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:38:36.542422500Z",
     "start_time": "2023-09-20T02:38:36.397428400Z"
    }
   },
   "outputs": [],
   "source": [
    "select_datetime = make_column_selector(dtype_include='datetime')\n",
    "select_category = make_column_selector(dtype_include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are also other data type options you can use, such as the datetime data type or the pandas category data type.\n",
    "\n",
    "Finally, it's worth noting that you can also pass a list of multiple data types to make_column_selector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:38:57.941758Z",
     "start_time": "2023-09-20T02:38:57.633073800Z"
    }
   },
   "outputs": [],
   "source": [
    "select_multiple = make_column_selector(dtype_include=[object, 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Q&A: How do I select columns by column name pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's say that we had a lot of columns, and all of the columns that we wanted to select for a particular transformation had the same pattern in their names. For example, maybe all of those columns started with the same word.\n",
    "\n",
    "Once again, we can use the make_column_selector function, which allows us to select columns by regular expression pattern. Here's a silly example in which we select columns that include the capital letters E or S.\n",
    "\n",
    "When we run the fit_transform method, Embarked and Sex have been one-hot encoded, and the remaining columns have been passed through.\n",
    "\n",
    "Again, this is only useful if your column names follow a particular pattern and you know how to write regular expressions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:39:24.389934700Z",
     "start_time": "2023-09-20T02:39:24.161463400Z"
    }
   },
   "outputs": [],
   "source": [
    "select_ES = make_column_selector(pattern='E|S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:39:28.649507100Z",
     "start_time": "2023-09-20T02:39:28.436495500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, select_ES),\n",
    "    remainder='passthrough')\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Q&A: Should I use ColumnTransformer or make_column_transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far in the course, we've been creating ColumnTransformers using the make_column_transformer function. In this lesson, I'll show you how to use the ColumnTransformer class and then compare it to make_column_transformer so that you can decide which one you want to use.\n",
    "\n",
    "To start, we'll import the ColumnTransformer class from the compose module, and then we'll create an instance.\n",
    "\n",
    "When creating an instance, the first difference you might notice is that the tuples have three elements rather than two. The first element of each tuple is a name of your choosing that you are required to assign to the transformer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:41:10.813279300Z",
     "start_time": "2023-09-20T02:41:10.437306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ColumnTransformer(transformers=[('OHE', OneHotEncoder(), ['Embarked', 'Sex']),\n                                ('pass', 'passthrough', ['Parch', 'Fare'])])"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(\n",
    "    [('OHE', ohe, ['Embarked', 'Sex']),\n",
    "     ('pass', 'passthrough', ['Parch', 'Fare'])])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the first tuple is our one-hot encoding of Embarked and Sex, and we're assigning it the name \"OHE\" in all caps. The second tuple is our special passthrough transformer for Parch and Fare, and we're assigning it the name \"pass\". We can see these names when we print out the ColumnTransformer.\n",
    "\n",
    "You might also notice that the tuples are in a list, which is a requirement of the ColumnTransformer class.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuple elements for ColumnTransformer:**\n",
    "\n",
    "1. Transformer name\n",
    "2. Transformer object or \"drop\" or \"passthrough\"\n",
    "3. List of columns to which the transformer should be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's create the same ColumnTransformer using the make_column_transformer function. When using make_column_transformer, we don't define names for the transformers. Instead, each transformer is assigned a default name, which is the lowercase version of the transformer's class name.\n",
    "\n",
    "As you can see when we print it out, the one-hot encoder is assigned the name \"onehotencoder\" (all lowercase), and the passthrough transformer is assigned the name \"passthrough\".\n",
    "\n",
    "All of that being said, which one should you use?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:41:52.346581800Z",
     "start_time": "2023-09-20T02:41:52.232125700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('passthrough', ['Parch', 'Fare']))\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I prefer make_column_transformer, because I find the code both easier to read and easier to write, so that's what I'll use in this course. I usually don't mind the default transformer names, and in fact I like that I don't have to come up with a name for each transformer.\n",
    "\n",
    "However, there are times when defining names for the transformers is useful. Custom names can be clearer if you're performing a grid search of transformer parameters, or if you're using the same type of transformer multiple times in the same ColumnTransformer instance. We'll see examples of this later in the course.\n",
    "\n",
    "One final note is that the ColumnTransformer class enables transformer weights, meaning you can emphasize the output of some transformers more than others. The specific use case of this is not yet clear to me, but if you do decide to use transformer weights, then you can't use the make_column_transformer function and you must use the ColumnTransformer class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; | ColumnTransformer | make_column_transformer\n",
    ":--- | :---: | :---:\n",
    "Allows custom names? | Yes | No\n",
    "Allows transformer weights? | Yes | No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Q&A: Should I use Pipeline or make_pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far in the course, we've been creating Pipelines using the make_pipeline function. In this lesson, I'll show you how to use the Pipeline class and then compare it to make_pipeline so that you can decide which one you want to use.\n",
    "\n",
    "To start, we'll import the Pipeline class from the pipeline module, and then we'll create an instance.\n",
    "\n",
    "When creating an instance, the main difference you might notice is that we're passing in a list of tuples to the Pipeline constructor. Each tuple has two elements, in which the first element is the name you're assigning to the Pipeline step, and the second element is the model or transformer you're including in the Pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:43:30.565985600Z",
     "start_time": "2023-09-20T02:43:30.403629600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('classifier',\n                 LogisticRegression(random_state=1, solver='liblinear'))])"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('preprocessor', ct), ('classifier', logreg)])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuple elements for Pipeline:**\n",
    "\n",
    "1. Step name\n",
    "2. Model or transformer object"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the first tuple is our preprocessing step using ColumnTransformer, and we're assigning it the name \"preprocessor\". The second tuple is our model building step using logistic regression, and we're assigning it the name \"classifier\". We can see these names when we print out the Pipeline.\n",
    "\n",
    "We can also see the step names by accessing the named_steps attribute of the Pipeline and running the keys method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:44:01.165935200Z",
     "start_time": "2023-09-20T02:44:00.965113500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['preprocessor', 'classifier'])"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's create the same Pipeline using the make_pipeline function. When using make_pipeline, we don't define names for the steps. Instead, each step is assigned a default name, which is the lowercase version of the step's class name.\n",
    "\n",
    "As you can see when we print it out, the first step is assigned the name \"columntransformer\" (all lowercase), and the second step is assigned the name \"logisticregression\" (all lowercase). Again, we can also see the step names using the named_steps attribute.\n",
    "\n",
    "All of that being said, which one should you use?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:44:05.843382300Z",
     "start_time": "2023-09-20T02:44:05.654523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:44:13.616426400Z",
     "start_time": "2023-09-20T02:44:13.461003600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['columntransformer', 'logisticregression'])"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; | Pipeline | make_pipeline\n",
    ":--- | :---: | :---:\n",
    "Allows custom names? | Yes | No"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I prefer make_pipeline, because I find the code both easier to read and easier to write, so that's what I'll use in this course. I usually don't mind the default step names, and in fact I like that I don't have to come up with a name for each step.\n",
    "\n",
    "However, custom step names can be useful for clarity, especially if you're performing a grid search of a Pipeline. We'll see many examples of this later in the course."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Q&A: How do I examine the steps of a Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes you might want to examine the steps of a fitted Pipeline so that you can understand what's happening within each step. In this lesson, I'll show you how to do it.\n",
    "\n",
    "We'll start by fitting the Pipeline, which prints out the two steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:45:36.703113900Z",
     "start_time": "2023-09-20T02:45:36.332105600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As I mentioned in the previous lesson, make_pipeline assigned a name to each step, which is the lowercase version of the step's class name. In this case, our step names are \"columntransformer\" and \"logisticregression\".\n",
    "\n",
    "To examine an individual step, you select the named_steps attribute and pass the step name in brackets. Note that if we had assigned custom step names such as \"preprocessor\" and \"classifier\", we would be using those here instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:09.991768600Z",
     "start_time": "2023-09-20T02:46:09.805334500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['columntransformer', 'logisticregression'])"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:12.017528700Z",
     "start_time": "2023-09-20T02:46:11.850663200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:14.616196100Z",
     "start_time": "2023-09-20T02:46:14.479075700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(random_state=1, solver='liblinear')"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['logisticregression']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you've accessed a step, you can examine its attributes or run its methods. For example, we can run the get_feature_names method from the \"columntransformer\" step to learn the names of each feature. As a reminder, the x0 means feature 0 that was passed to the OneHotEncoder, and the x1 means feature 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:33.431611800Z",
     "start_time": "2023-09-20T02:46:33.213196200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhasa\\GitHub\\venvs\\DeepLearning\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "['onehotencoder__x0_C',\n 'onehotencoder__x0_Q',\n 'onehotencoder__x0_S',\n 'onehotencoder__x1_female',\n 'onehotencoder__x1_male',\n 'Parch',\n 'Fare']"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['columntransformer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also see the coefficient values of the 7 features by examining the \"coef_\" attribute of the \"logisticregression\" step. These coefficients are listed in the same order as the features, though the intercept is stored in a separate attribute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By finding the 4 positive coefficients, you can determine that embarking at port C, being female, and having a higher Parch and Fare are all associated with a greater likelihood of survival. Note that these are just associations the model learned from 10 rows of training data. They are not necessarily statistically significant associations, and in fact scikit-learn does not provide p-values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:51.429610600Z",
     "start_time": "2023-09-20T02:46:51.122924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, it's worth noting that there are three other ways that you can examine the steps of a Pipeline:\n",
    "\n",
    "- First, you can use named_steps with periods.\n",
    "- Second, you can exclude the named_steps attribute entirely.\n",
    "- And third, you can reference the step by position rather than by name.\n",
    "\n",
    "Personally, I like the initial bracket notation because I think it's the most readable, even though it's the most typing. However, using named_steps with the periods seems to be the only option that supports autocompleting both the step name and the attribute, which is a nice benefit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:46:56.526023500Z",
     "start_time": "2023-09-20T02:46:56.370046600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.logisticregression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:47:37.357369600Z",
     "start_time": "2023-09-20T02:47:37.217834100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:47:41.618878400Z",
     "start_time": "2023-09-20T02:47:41.431363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Workflow review #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Recap of our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this chapter, we're going to review the workflow that we've built so far to make sure you understand the key concepts before we start adding additional complexity.\n",
    "\n",
    "To start, we're going to walk through all of the code that is necessary to recreate our workflow up to this point. We begin by importing pandas, the OneHotEncoder and LogisticRegression classes, and the make_column_transformer and make_pipeline functions.\n",
    "\n",
    "Next, we create a list of the four columns we're going to select from our data. Then, we read in 10 rows of training data and use it to define our X and y. And we read in 10 rows of new data and use it to define X_new.\n",
    "\n",
    "We create an instance of OneHotEncoder, which is our only transformer at this point. And then we build the ColumnTransformer, which one-hot encodes Embarked and Sex and passes through Parch and Fare.\n",
    "\n",
    "We also create an instance of logistic regression. Finally, we create a two-step Pipeline, fit the Pipeline to X and y, and use the fitted Pipeline to make predictions on X_new.\n",
    "\n",
    "That's really all the code we need to recreate our entire workflow from the last few chapters. You'll notice that there are no calls to fit_transform or transform because all of that functionality is encapsulated by the Pipeline.\n",
    "\n",
    "I did exclude cross-validation from this recap because, as mentioned previously, any model evaluation procedure is highly unreliable with only 10 rows of data. However, we will thoroughly explore the topic of model evaluation later in the course, once we are using the full dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:24.846999300Z",
     "start_time": "2023-09-20T02:55:24.678662800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:29.412014500Z",
     "start_time": "2023-09-20T02:55:29.047196900Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:33.751650400Z",
     "start_time": "2023-09-20T02:55:33.253087600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\n",
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:41.241869700Z",
     "start_time": "2023-09-20T02:55:40.836243300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/MLnewdata', nrows=10)\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:43.772426Z",
     "start_time": "2023-09-20T02:55:43.537592Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:48.480422500Z",
     "start_time": "2023-09-20T02:55:48.170199Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    ('passthrough', ['Parch', 'Fare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:51.168617100Z",
     "start_time": "2023-09-20T02:55:50.959517500Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:55:53.683132Z",
     "start_time": "2023-09-20T02:55:53.447733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Comparing ColumnTransformer and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataschool.io/files/simple_pipeline.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ColumnTransformer vs Pipeline:**\n",
    "\n",
    "- **ColumnTransformer:**\n",
    "  - Selects subsets of columns, transforms them independently, stacks the results side-by-side\n",
    "  - Only includes transformers\n",
    "  - Does not have steps (transformers operate in parallel)\n",
    "- **Pipeline:**\n",
    "  - Series of steps that occur in order\n",
    "  - Output of each step becomes the input to the next step\n",
    "  - Last step is a model or transformer, all other steps are transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to be successful in the rest of the course, it's very important that you clearly understand the differences between a ColumnTransformer and a Pipeline. In this lesson, I'm going to explain those differences. This diagram should help to illustrate the concepts.\n",
    "\n",
    "Let's start with the ColumnTransformer, which received 4 columns of input from the X DataFrame:\n",
    "\n",
    "- It selected 2 of those columns, namely Embarked and Sex, and used the OneHotEncoder to transform them into 5 columns.\n",
    "- It selected the other 2 columns, namely Parch and Fare, and did nothing to them, which of course resulted in 2 columns.\n",
    "- Finally, it stacked the 5 columns output by OneHotEncoder and the 2 columns output by the passthrough transformer side-by-side, resulting in a total of 7 columns.\n",
    "\n",
    "Now let's talk about the Pipeline, which has 2 steps:\n",
    "\n",
    "- Step 1 is a ColumnTransformer that received 4 columns of input and transformed them into 7 columns.\n",
    "- Step 2 is a logistic regression model that received 7 columns of input and used those 7 columns either for fitting or predicting.\n",
    "\n",
    "With those examples in mind, we can step back and summarize the differences between a ColumnTransformer and a Pipeline:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- A ColumnTransformer pulls out subsets of columns and transforms them independently, and then stacks the results side-by-side.\n",
    "- It only ever does data transformations, meaning your ColumnTransformer will never include a model.\n",
    "- And it does not have steps, because each subset of columns is transformed independently. In other words, data does not flow from one transformer to the next.\n",
    "\n",
    "In contrast:\n",
    "\n",
    "- A Pipeline is a series of steps that occur in order, and the output of each step becomes the input to the next step.\n",
    "- Thus if you had a 3-step Pipeline, the output of step 1 becomes the input to step 2, and the output of step 2 becomes the input to step 3.\n",
    "- The last step of a Pipeline can be a model or a transformer, whereas all other steps must be transformers.\n",
    "\n",
    "In summary, a Pipeline contains steps that operate in sequence, whereas a ColumnTransformer contains transformers that operate in parallel. In later chapters, you'll see why this difference is so important and how it guides the structure of our workflow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Creating a Pipeline diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To wrap up this chapter, I want to show you a feature that's new in scikit-learn version 0.23 that can help you to visualize and thus better understand your Pipelines.\n",
    "\n",
    "To start, we'll import the set_config function. Then we run it and set the display parameter to \"diagram\". With that configuration, you'll see a diagram any time you print out a Pipeline or any other estimator.\n",
    "\n",
    "This is basically the same diagram I created. And you can actually click on any element in order to see more details. For example, if you click on the transformer names, you can see the columns they're transforming. And if you click on the class names, you can see any parameters that have been changed from their default values.\n",
    "\n",
    "If you use this configuration but you ever need to see the regular text output, you can just use the print function with your Pipeline.\n",
    "\n",
    "Finally, it's worth noting that displaying diagrams is the default starting in scikit-learn version 1.1. If you'd prefer to always see the text output, you can change the configuration by setting the display parameter to \"text\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:57:44.396124400Z",
     "start_time": "2023-09-20T02:57:44.278731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:57:49.471184300Z",
     "start_time": "2023-09-20T02:57:46.688479800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])",
      "text/html": "<style>#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 {color: black;background-color: white;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 pre{padding: 0;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-toggleable {background-color: white;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-estimator:hover {background-color: #d4ebff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-item {z-index: 1;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-parallel-item:only-child::after {width: 0;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-f7c78ac1-312f-4dbe-b9b7-42d980d1fe80\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;onehotencoder&#x27;,\n                                                  OneHotEncoder(),\n                                                  [&#x27;Embarked&#x27;, &#x27;Sex&#x27;]),\n                                                 (&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n                                                  [&#x27;Parch&#x27;, &#x27;Fare&#x27;])])),\n                (&#x27;logisticregression&#x27;,\n                 LogisticRegression(random_state=1, solver=&#x27;liblinear&#x27;))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"13989772-056a-42f3-9d60-d10282c27fd8\" type=\"checkbox\" ><label for=\"13989772-056a-42f3-9d60-d10282c27fd8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;onehotencoder&#x27;,\n                                                  OneHotEncoder(),\n                                                  [&#x27;Embarked&#x27;, &#x27;Sex&#x27;]),\n                                                 (&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n                                                  [&#x27;Parch&#x27;, &#x27;Fare&#x27;])])),\n                (&#x27;logisticregression&#x27;,\n                 LogisticRegression(random_state=1, solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b9a043bf-59e8-40c2-9e62-2bc1ad266aae\" type=\"checkbox\" ><label for=\"b9a043bf-59e8-40c2-9e62-2bc1ad266aae\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;onehotencoder&#x27;, OneHotEncoder(),\n                                 [&#x27;Embarked&#x27;, &#x27;Sex&#x27;]),\n                                (&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n                                 [&#x27;Parch&#x27;, &#x27;Fare&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3db82ae1-eb34-426f-9737-8d05cb965a70\" type=\"checkbox\" ><label for=\"3db82ae1-eb34-426f-9737-8d05cb965a70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehotencoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Embarked&#x27;, &#x27;Sex&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"aa5f9883-3fbd-418b-afe8-d275878355c4\" type=\"checkbox\" ><label for=\"aa5f9883-3fbd-418b-afe8-d275878355c4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f72765ec-0c32-44f1-8056-91d3e5d4cfdb\" type=\"checkbox\" ><label for=\"f72765ec-0c32-44f1-8056-91d3e5d4cfdb\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Parch&#x27;, &#x27;Fare&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"94434650-ebe9-4061-8327-9806e61b77db\" type=\"checkbox\" ><label for=\"94434650-ebe9-4061-8327-9806e61b77db\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5baabe03-71f7-4d4f-ac95-5952d00c84d3\" type=\"checkbox\" ><label for=\"5baabe03-71f7-4d4f-ac95-5952d00c84d3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:57:55.872378100Z",
     "start_time": "2023-09-20T02:57:55.490707700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('columntransformer',\n",
      "                 ColumnTransformer(transformers=[('onehotencoder',\n",
      "                                                  OneHotEncoder(),\n",
      "                                                  ['Embarked', 'Sex']),\n",
      "                                                 ('passthrough', 'passthrough',\n",
      "                                                  ['Parch', 'Fare'])])),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(random_state=1, solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T02:58:00.345599Z",
     "start_time": "2023-09-20T02:58:00.181301800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_config(display='text')\n",
    "pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
