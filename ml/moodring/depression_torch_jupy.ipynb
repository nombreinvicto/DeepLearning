{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.axes._axes as axes\n",
    "import os\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "# import pytorch specific utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import hiddenlayer as hl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "  Patient-ID  depression_score       date  week_num  \\\n0       19HL                 9  5/19/2020         1   \n1       19HL                12  5/26/2020         2   \n2       19HL                11   6/2/2020         3   \n3       19HL                 7   6/9/2020         4   \n4       19HL                15   7/7/2020         8   \n\n   location_daily_locationvariance  location_daily_loglocationvariance  \\\n0                         0.041307                           -3.009923   \n1                         0.000125                           -4.781617   \n2                         0.000075                           -3.295846   \n3                         0.000081                           -3.292259   \n4                         3.115409                           -3.382519   \n\n   location_daily_totaldistance  location_daily_averagespeed  \\\n0                 109518.156200                    26.916578   \n1                  13617.571040                     4.725519   \n2                   8220.443966                    75.056043   \n3                   9832.078288                    11.578641   \n4                 254789.933100                    67.341228   \n\n   location_daily_varspeed  location_daily_circadianmovement  ...  \\\n0              4451.886200                          0.523614  ...   \n1               248.163360                         -0.563407  ...   \n2              1018.057662                         -0.340036  ...   \n3               576.307436                         -0.236498  ...   \n4             10160.117000                          0.214006  ...   \n\n   call_incoming_daily_count  call_incoming_daily_distinctcontacts  \\\n0                   3.285714                              3.285714   \n1                   3.571429                              3.571429   \n2                   4.571429                              4.571429   \n3                   3.857143                              3.857143   \n4                   4.285714                              4.285714   \n\n   call_incoming_daily_meanduration  call_incoming_daily_sumduration  \\\n0                       1403.285714                      3441.571429   \n1                        894.351190                      4736.000000   \n2                       1946.675170                      7748.285714   \n3                        603.566667                      3692.857143   \n4                       1949.355556                      8088.428571   \n\n   call_incoming_daily_minduration  call_incoming_daily_maxduration  \\\n0                       825.714286                      2641.142857   \n1                         9.857143                      3103.857143   \n2                        95.714286                      5190.000000   \n3                        10.714286                      2197.714286   \n4                       768.571429                      3635.714286   \n\n   call_incoming_daily_modeduration  call_incoming_daily_timefirstcall  \\\n0                       2229.428571                         394.142857   \n1                         11.428571                         634.142857   \n2                       2585.285714                         629.285714   \n3                         70.000000                         300.714286   \n4                       2073.428571                         507.857143   \n\n   call_incoming_daily_timelastcall  depression_level  \n0                        871.285714                 2  \n1                        907.142857                 3  \n2                       1216.571429                 3  \n3                        697.571429                 2  \n4                       1041.857143                 4  \n\n[5 rows x 66 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient-ID</th>\n      <th>depression_score</th>\n      <th>date</th>\n      <th>week_num</th>\n      <th>location_daily_locationvariance</th>\n      <th>location_daily_loglocationvariance</th>\n      <th>location_daily_totaldistance</th>\n      <th>location_daily_averagespeed</th>\n      <th>location_daily_varspeed</th>\n      <th>location_daily_circadianmovement</th>\n      <th>...</th>\n      <th>call_incoming_daily_count</th>\n      <th>call_incoming_daily_distinctcontacts</th>\n      <th>call_incoming_daily_meanduration</th>\n      <th>call_incoming_daily_sumduration</th>\n      <th>call_incoming_daily_minduration</th>\n      <th>call_incoming_daily_maxduration</th>\n      <th>call_incoming_daily_modeduration</th>\n      <th>call_incoming_daily_timefirstcall</th>\n      <th>call_incoming_daily_timelastcall</th>\n      <th>depression_level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19HL</td>\n      <td>9</td>\n      <td>5/19/2020</td>\n      <td>1</td>\n      <td>0.041307</td>\n      <td>-3.009923</td>\n      <td>109518.156200</td>\n      <td>26.916578</td>\n      <td>4451.886200</td>\n      <td>0.523614</td>\n      <td>...</td>\n      <td>3.285714</td>\n      <td>3.285714</td>\n      <td>1403.285714</td>\n      <td>3441.571429</td>\n      <td>825.714286</td>\n      <td>2641.142857</td>\n      <td>2229.428571</td>\n      <td>394.142857</td>\n      <td>871.285714</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19HL</td>\n      <td>12</td>\n      <td>5/26/2020</td>\n      <td>2</td>\n      <td>0.000125</td>\n      <td>-4.781617</td>\n      <td>13617.571040</td>\n      <td>4.725519</td>\n      <td>248.163360</td>\n      <td>-0.563407</td>\n      <td>...</td>\n      <td>3.571429</td>\n      <td>3.571429</td>\n      <td>894.351190</td>\n      <td>4736.000000</td>\n      <td>9.857143</td>\n      <td>3103.857143</td>\n      <td>11.428571</td>\n      <td>634.142857</td>\n      <td>907.142857</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19HL</td>\n      <td>11</td>\n      <td>6/2/2020</td>\n      <td>3</td>\n      <td>0.000075</td>\n      <td>-3.295846</td>\n      <td>8220.443966</td>\n      <td>75.056043</td>\n      <td>1018.057662</td>\n      <td>-0.340036</td>\n      <td>...</td>\n      <td>4.571429</td>\n      <td>4.571429</td>\n      <td>1946.675170</td>\n      <td>7748.285714</td>\n      <td>95.714286</td>\n      <td>5190.000000</td>\n      <td>2585.285714</td>\n      <td>629.285714</td>\n      <td>1216.571429</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19HL</td>\n      <td>7</td>\n      <td>6/9/2020</td>\n      <td>4</td>\n      <td>0.000081</td>\n      <td>-3.292259</td>\n      <td>9832.078288</td>\n      <td>11.578641</td>\n      <td>576.307436</td>\n      <td>-0.236498</td>\n      <td>...</td>\n      <td>3.857143</td>\n      <td>3.857143</td>\n      <td>603.566667</td>\n      <td>3692.857143</td>\n      <td>10.714286</td>\n      <td>2197.714286</td>\n      <td>70.000000</td>\n      <td>300.714286</td>\n      <td>697.571429</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19HL</td>\n      <td>15</td>\n      <td>7/7/2020</td>\n      <td>8</td>\n      <td>3.115409</td>\n      <td>-3.382519</td>\n      <td>254789.933100</td>\n      <td>67.341228</td>\n      <td>10160.117000</td>\n      <td>0.214006</td>\n      <td>...</td>\n      <td>4.285714</td>\n      <td>4.285714</td>\n      <td>1949.355556</td>\n      <td>8088.428571</td>\n      <td>768.571429</td>\n      <td>3635.714286</td>\n      <td>2073.428571</td>\n      <td>507.857143</td>\n      <td>1041.857143</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 66 columns</p>\n</div>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_file = f\"{cwd}//df_depressionlevel_median_imputed.csv\"\n",
    "depression_data = pd.read_csv(data_file)\n",
    "depression_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "x_features = depression_data.drop(axis=1,\n",
    "                                  labels=['Patient-ID',\n",
    "                                          'depression_score',\n",
    "                                          'date', 'week_num',\n",
    "                                          'depression_level'])\n",
    "\n",
    "y_features = depression_data['depression_score']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "   location_daily_locationvariance  location_daily_loglocationvariance  \\\n0                        -3.186726                           -3.009923   \n1                        -8.990827                           -4.781617   \n2                        -9.502030                           -3.295846   \n3                        -9.423534                           -3.292259   \n4                         1.136361                           -3.382519   \n\n   location_daily_totaldistance  location_daily_averagespeed  \\\n0                     11.603846                     3.292742   \n1                      9.519116                     1.552977   \n2                      9.014379                     4.318235   \n3                      9.193406                     2.449162   \n4                     12.448195                     4.209773   \n\n   location_daily_varspeed  location_daily_circadianmovement  \\\n0                 8.401083                          0.523614   \n1                 5.514087                         -0.563407   \n2                 6.925652                         -0.340036   \n3                 6.356641                         -0.236498   \n4                 9.226225                          0.214006   \n\n   location_daily_numberofsignificantplaces  \\\n0                                  2.181224   \n1                                  1.421386   \n2                                  0.619039   \n3                                  1.098612   \n4                                  1.349927   \n\n   location_daily_numberlocationtransitions  location_daily_radiusgyration  \\\n0                                  4.030441                      -4.361094   \n1                                  2.410799                      -3.818417   \n2                                  0.826679                      -4.705670   \n3                                  1.742969                      -4.569582   \n4                                  2.316770                      -3.046489   \n\n   location_daily_timeattop1  ...  ar_daily_sumvehicle  \\\n0                 540.428571  ...             3.478414   \n1                1043.285714  ...             2.877765   \n2                1342.142857  ...             2.231861   \n3                 663.857143  ...             2.337969   \n4                 713.714286  ...             5.078970   \n\n   call_incoming_daily_count  call_incoming_daily_distinctcontacts  \\\n0                   1.189584                              1.189584   \n1                   1.272966                              1.272966   \n2                   1.519826                              1.519826   \n3                   1.349927                              1.349927   \n4                   1.455287                              1.455287   \n\n   call_incoming_daily_meanduration  call_incoming_daily_sumduration  \\\n0                          7.246572                         8.143683   \n1                          6.796099                         8.462948   \n2                          7.573878                         8.955227   \n3                          6.402857                         8.214156   \n4                          7.575254                         8.998190   \n\n   call_incoming_daily_minduration  call_incoming_daily_maxduration  \\\n0                         6.716249                         7.878967   \n1                         2.288196                         8.040401   \n2                         4.561368                         8.554489   \n3                         2.371578                         7.695173   \n4                         6.644534                         8.198561   \n\n   call_incoming_daily_modeduration  call_incoming_daily_timefirstcall  \\\n0                          7.709501                         394.142857   \n1                          2.436116                         634.142857   \n2                          7.857591                         629.285714   \n3                          4.248495                         300.714286   \n4                          7.636959                         507.857143   \n\n   call_incoming_daily_timelastcall  \n0                        871.285714  \n1                        907.142857  \n2                       1216.571429  \n3                        697.571429  \n4                       1041.857143  \n\n[5 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>location_daily_locationvariance</th>\n      <th>location_daily_loglocationvariance</th>\n      <th>location_daily_totaldistance</th>\n      <th>location_daily_averagespeed</th>\n      <th>location_daily_varspeed</th>\n      <th>location_daily_circadianmovement</th>\n      <th>location_daily_numberofsignificantplaces</th>\n      <th>location_daily_numberlocationtransitions</th>\n      <th>location_daily_radiusgyration</th>\n      <th>location_daily_timeattop1</th>\n      <th>...</th>\n      <th>ar_daily_sumvehicle</th>\n      <th>call_incoming_daily_count</th>\n      <th>call_incoming_daily_distinctcontacts</th>\n      <th>call_incoming_daily_meanduration</th>\n      <th>call_incoming_daily_sumduration</th>\n      <th>call_incoming_daily_minduration</th>\n      <th>call_incoming_daily_maxduration</th>\n      <th>call_incoming_daily_modeduration</th>\n      <th>call_incoming_daily_timefirstcall</th>\n      <th>call_incoming_daily_timelastcall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-3.186726</td>\n      <td>-3.009923</td>\n      <td>11.603846</td>\n      <td>3.292742</td>\n      <td>8.401083</td>\n      <td>0.523614</td>\n      <td>2.181224</td>\n      <td>4.030441</td>\n      <td>-4.361094</td>\n      <td>540.428571</td>\n      <td>...</td>\n      <td>3.478414</td>\n      <td>1.189584</td>\n      <td>1.189584</td>\n      <td>7.246572</td>\n      <td>8.143683</td>\n      <td>6.716249</td>\n      <td>7.878967</td>\n      <td>7.709501</td>\n      <td>394.142857</td>\n      <td>871.285714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-8.990827</td>\n      <td>-4.781617</td>\n      <td>9.519116</td>\n      <td>1.552977</td>\n      <td>5.514087</td>\n      <td>-0.563407</td>\n      <td>1.421386</td>\n      <td>2.410799</td>\n      <td>-3.818417</td>\n      <td>1043.285714</td>\n      <td>...</td>\n      <td>2.877765</td>\n      <td>1.272966</td>\n      <td>1.272966</td>\n      <td>6.796099</td>\n      <td>8.462948</td>\n      <td>2.288196</td>\n      <td>8.040401</td>\n      <td>2.436116</td>\n      <td>634.142857</td>\n      <td>907.142857</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-9.502030</td>\n      <td>-3.295846</td>\n      <td>9.014379</td>\n      <td>4.318235</td>\n      <td>6.925652</td>\n      <td>-0.340036</td>\n      <td>0.619039</td>\n      <td>0.826679</td>\n      <td>-4.705670</td>\n      <td>1342.142857</td>\n      <td>...</td>\n      <td>2.231861</td>\n      <td>1.519826</td>\n      <td>1.519826</td>\n      <td>7.573878</td>\n      <td>8.955227</td>\n      <td>4.561368</td>\n      <td>8.554489</td>\n      <td>7.857591</td>\n      <td>629.285714</td>\n      <td>1216.571429</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-9.423534</td>\n      <td>-3.292259</td>\n      <td>9.193406</td>\n      <td>2.449162</td>\n      <td>6.356641</td>\n      <td>-0.236498</td>\n      <td>1.098612</td>\n      <td>1.742969</td>\n      <td>-4.569582</td>\n      <td>663.857143</td>\n      <td>...</td>\n      <td>2.337969</td>\n      <td>1.349927</td>\n      <td>1.349927</td>\n      <td>6.402857</td>\n      <td>8.214156</td>\n      <td>2.371578</td>\n      <td>7.695173</td>\n      <td>4.248495</td>\n      <td>300.714286</td>\n      <td>697.571429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.136361</td>\n      <td>-3.382519</td>\n      <td>12.448195</td>\n      <td>4.209773</td>\n      <td>9.226225</td>\n      <td>0.214006</td>\n      <td>1.349927</td>\n      <td>2.316770</td>\n      <td>-3.046489</td>\n      <td>713.714286</td>\n      <td>...</td>\n      <td>5.078970</td>\n      <td>1.455287</td>\n      <td>1.455287</td>\n      <td>7.575254</td>\n      <td>8.998190</td>\n      <td>6.644534</td>\n      <td>8.198561</td>\n      <td>7.636959</td>\n      <td>507.857143</td>\n      <td>1041.857143</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_features_logscaled = x_features.copy()\n",
    "\n",
    "for col in x_features.columns:\n",
    "    skew = x_features[col].skew()\n",
    "    if np.abs(skew) > 1:\n",
    "        # then scale the feature\n",
    "        x_features_logscaled[col] = x_features[col].apply(lambda x: np.log(x))\n",
    "\n",
    "x_features_logscaled.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "(192,)"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_features_transformed = scaler.fit_transform(x_features_logscaled)\n",
    "# %%\n",
    "y_features = np.array(y_features)\n",
    "y_features.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "X = torch.from_numpy(x_features_transformed.astype(np.float32))\n",
    "Y = torch.from_numpy(y_features.astype(np.float32)).view(y_features.shape[0], 1)\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "eps = torch.tensor(1e-7, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "\n",
    "    return (1 - ss_res / (ss_tot + eps))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "all_layers = OrderedDict()\n",
    "\n",
    "\n",
    "def create_model(first_layer_units=1024):\n",
    "    # create the constant part of the model\n",
    "    all_layers['l0'] = nn.Linear(in_features=X.shape[1],\n",
    "                                 out_features=first_layer_units)\n",
    "    all_layers['r0'] = nn.ReLU()\n",
    "    #all_layers['b0'] = nn.BatchNorm1d(num_features=first_layer_units)\n",
    "\n",
    "    # populate the neuron list\n",
    "    units_list = []\n",
    "    units_list.append(first_layer_units)\n",
    "    while first_layer_units % 2 == 0:\n",
    "        first_layer_units /= 2\n",
    "        units_list.append(int(first_layer_units))\n",
    "\n",
    "    print(f\"this is units_list: {units_list}\")\n",
    "    # create the variable part of the model\n",
    "    for i, units in enumerate(units_list):\n",
    "        if i < len(units_list) - 2:\n",
    "            all_layers[f\"l{i + 1}\"] = nn.Linear(in_features=units_list[i],\n",
    "                                                out_features=units_list[i + 1])\n",
    "            all_layers[f\"r{i + 1}\"] = nn.ReLU()\n",
    "            # all_layers[f\"b{i + 1}\"] = nn.BatchNorm1d(\n",
    "            #     num_features=units_list[i + 1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # return the created model\n",
    "    print(i)\n",
    "    all_layers[f\"l{i + 1}\"] = nn.Linear(in_features=units_list[i],\n",
    "                                        out_features=1)\n",
    "\n",
    "    # return the model\n",
    "    return nn.Sequential(all_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is units_list: [1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1]\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sequential(\n  (l0): Linear(in_features=61, out_features=1024, bias=True)\n  (r0): ReLU()\n  (l1): Linear(in_features=1024, out_features=512, bias=True)\n  (r1): ReLU()\n  (l2): Linear(in_features=512, out_features=256, bias=True)\n  (r2): ReLU()\n  (l3): Linear(in_features=256, out_features=128, bias=True)\n  (r3): ReLU()\n  (l4): Linear(in_features=128, out_features=64, bias=True)\n  (r4): ReLU()\n  (l5): Linear(in_features=64, out_features=32, bias=True)\n  (r5): ReLU()\n  (l6): Linear(in_features=32, out_features=16, bias=True)\n  (r6): ReLU()\n  (l7): Linear(in_features=16, out_features=8, bias=True)\n  (r7): ReLU()\n  (l8): Linear(in_features=8, out_features=4, bias=True)\n  (r8): ReLU()\n  (l9): Linear(in_features=4, out_features=2, bias=True)\n  (r9): ReLU()\n  (l10): Linear(in_features=2, out_features=1, bias=True)\n)"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(first_layer_units=1024)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]          63,488\n",
      "              ReLU-2                 [-1, 1024]               0\n",
      "            Linear-3                  [-1, 512]         524,800\n",
      "              ReLU-4                  [-1, 512]               0\n",
      "            Linear-5                  [-1, 256]         131,328\n",
      "              ReLU-6                  [-1, 256]               0\n",
      "            Linear-7                  [-1, 128]          32,896\n",
      "              ReLU-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 64]           8,256\n",
      "             ReLU-10                   [-1, 64]               0\n",
      "           Linear-11                   [-1, 32]           2,080\n",
      "             ReLU-12                   [-1, 32]               0\n",
      "           Linear-13                   [-1, 16]             528\n",
      "             ReLU-14                   [-1, 16]               0\n",
      "           Linear-15                    [-1, 8]             136\n",
      "             ReLU-16                    [-1, 8]               0\n",
      "           Linear-17                    [-1, 4]              36\n",
      "             ReLU-18                    [-1, 4]               0\n",
      "           Linear-19                    [-1, 2]              10\n",
      "             ReLU-20                    [-1, 2]               0\n",
      "           Linear-21                    [-1, 1]               3\n",
      "================================================================\n",
      "Total params: 763,561\n",
      "Trainable params: 763,561\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 2.91\n",
      "Estimated Total Size (MB): 2.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(61, ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:0--TrainLoss:117.52787780761719--TrainR2:-6.721092700958252--TestLoss:127.81476593017578--TestR2:-8.580438613891602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1--TrainLoss:117.44315338134766--TrainR2:-6.715527057647705--TestLoss:127.7081069946289--TestR2:-8.572443962097168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2--TrainLoss:117.34107208251953--TrainR2:-6.708820343017578--TestLoss:127.56244659423828--TestR2:-8.56152629852295\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3--TrainLoss:117.20267486572266--TrainR2:-6.699728488922119--TestLoss:127.39791107177734--TestR2:-8.549193382263184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:4--TrainLoss:117.04439544677734--TrainR2:-6.689330101013184--TestLoss:127.1757583618164--TestR2:-8.532541275024414\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:5--TrainLoss:116.83293914794922--TrainR2:-6.675437927246094--TestLoss:126.92415618896484--TestR2:-8.51368236541748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:6--TrainLoss:116.60167694091797--TrainR2:-6.660245418548584--TestLoss:126.73902130126953--TestR2:-8.499805450439453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:7--TrainLoss:116.42302703857422--TrainR2:-6.648508548736572--TestLoss:126.59165954589844--TestR2:-8.488759994506836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:8--TrainLoss:116.29293060302734--TrainR2:-6.6399617195129395--TestLoss:126.56572723388672--TestR2:-8.48681640625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:9--TrainLoss:116.26368713378906--TrainR2:-6.638040542602539--TestLoss:126.54447174072266--TestR2:-8.485222816467285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:10--TrainLoss:116.25564575195312--TrainR2:-6.63751220703125--TestLoss:126.53787231445312--TestR2:-8.484728813171387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:11--TrainLoss:116.2647476196289--TrainR2:-6.638110160827637--TestLoss:126.52913665771484--TestR2:-8.484073638916016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:12--TrainLoss:116.27021789550781--TrainR2:-6.638469696044922--TestLoss:126.49732971191406--TestR2:-8.481689453125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:13--TrainLoss:116.230712890625--TrainR2:-6.635874271392822--TestLoss:126.4594497680664--TestR2:-8.478850364685059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:14--TrainLoss:116.17400360107422--TrainR2:-6.6321492195129395--TestLoss:126.43817901611328--TestR2:-8.477255821228027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:15--TrainLoss:116.1431655883789--TrainR2:-6.630123138427734--TestLoss:126.41693878173828--TestR2:-8.475664138793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:16--TrainLoss:116.12306213378906--TrainR2:-6.6288018226623535--TestLoss:126.39568328857422--TestR2:-8.47407054901123\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:17--TrainLoss:116.10298156738281--TrainR2:-6.627482891082764--TestLoss:126.37445831298828--TestR2:-8.472479820251465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:18--TrainLoss:116.08293914794922--TrainR2:-6.626166343688965--TestLoss:126.35320281982422--TestR2:-8.47088623046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:19--TrainLoss:116.0628433227539--TrainR2:-6.6248459815979--TestLoss:126.33197021484375--TestR2:-8.469294548034668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:20--TrainLoss:116.04281616210938--TrainR2:-6.623530387878418--TestLoss:126.31073760986328--TestR2:-8.467702865600586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:21--TrainLoss:116.02272033691406--TrainR2:-6.622210502624512--TestLoss:126.28949737548828--TestR2:-8.466111183166504\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:22--TrainLoss:116.00269317626953--TrainR2:-6.620894432067871--TestLoss:126.26829528808594--TestR2:-8.464521408081055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:23--TrainLoss:115.98262786865234--TrainR2:-6.6195759773254395--TestLoss:126.24705505371094--TestR2:-8.462930679321289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:24--TrainLoss:115.96260833740234--TrainR2:-6.618261337280273--TestLoss:126.22582244873047--TestR2:-8.461338996887207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:25--TrainLoss:115.94254302978516--TrainR2:-6.616942882537842--TestLoss:126.2046127319336--TestR2:-8.459749221801758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:26--TrainLoss:115.92250061035156--TrainR2:-6.615626335144043--TestLoss:126.18339538574219--TestR2:-8.458158493041992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:27--TrainLoss:115.9024429321289--TrainR2:-6.614308834075928--TestLoss:126.16218566894531--TestR2:-8.456568717956543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:28--TrainLoss:115.88241577148438--TrainR2:-6.612992763519287--TestLoss:126.14096069335938--TestR2:-8.454977989196777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:29--TrainLoss:115.86237335205078--TrainR2:-6.611676216125488--TestLoss:126.1197280883789--TestR2:-8.453386306762695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:30--TrainLoss:115.84234619140625--TrainR2:-6.610360145568848--TestLoss:126.0985336303711--TestR2:-8.451797485351562\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:31--TrainLoss:115.82232666015625--TrainR2:-6.609045505523682--TestLoss:126.07734680175781--TestR2:-8.450209617614746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:32--TrainLoss:115.80224609375--TrainR2:-6.607726097106934--TestLoss:126.0561294555664--TestR2:-8.44861888885498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:33--TrainLoss:115.78227233886719--TrainR2:-6.606413841247559--TestLoss:126.03492736816406--TestR2:-8.447030067443848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:34--TrainLoss:115.76220703125--TrainR2:-6.605095386505127--TestLoss:126.01372528076172--TestR2:-8.445440292358398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:35--TrainLoss:115.74220275878906--TrainR2:-6.603781223297119--TestLoss:125.99252319335938--TestR2:-8.443851470947266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:36--TrainLoss:115.72216796875--TrainR2:-6.6024651527404785--TestLoss:125.97134399414062--TestR2:-8.44226360321045\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:37--TrainLoss:115.7021713256836--TrainR2:-6.601150989532471--TestLoss:125.95015716552734--TestR2:-8.440675735473633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:38--TrainLoss:115.68212890625--TrainR2:-6.599834442138672--TestLoss:125.92896270751953--TestR2:-8.4390869140625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:39--TrainLoss:115.66216278076172--TrainR2:-6.598522663116455--TestLoss:125.90777587890625--TestR2:-8.437499046325684\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:40--TrainLoss:115.64208221435547--TrainR2:-6.597203731536865--TestLoss:125.88660430908203--TestR2:-8.435912132263184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:41--TrainLoss:115.62213134765625--TrainR2:-6.595892906188965--TestLoss:125.86542510986328--TestR2:-8.434324264526367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:42--TrainLoss:115.60209655761719--TrainR2:-6.594577312469482--TestLoss:125.84423828125--TestR2:-8.43273639678955\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:43--TrainLoss:115.58212280273438--TrainR2:-6.593264579772949--TestLoss:125.82308197021484--TestR2:-8.431150436401367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:44--TrainLoss:115.56212615966797--TrainR2:-6.591951370239258--TestLoss:125.80191802978516--TestR2:-8.429564476013184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:45--TrainLoss:115.54212951660156--TrainR2:-6.59063720703125--TestLoss:125.78074645996094--TestR2:-8.427977561950684\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:46--TrainLoss:115.52213287353516--TrainR2:-6.589323997497559--TestLoss:125.75956726074219--TestR2:-8.426389694213867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:47--TrainLoss:115.50214385986328--TrainR2:-6.588010311126709--TestLoss:125.73841857910156--TestR2:-8.4248046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:48--TrainLoss:115.48213195800781--TrainR2:-6.586695671081543--TestLoss:125.7172622680664--TestR2:-8.423218727111816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:49--TrainLoss:115.46220397949219--TrainR2:-6.585386276245117--TestLoss:125.69609832763672--TestR2:-8.421632766723633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:50--TrainLoss:115.44216918945312--TrainR2:-6.584070682525635--TestLoss:125.67495727539062--TestR2:-8.420047760009766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:51--TrainLoss:115.42223358154297--TrainR2:-6.582760810852051--TestLoss:125.6537857055664--TestR2:-8.418460845947266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:52--TrainLoss:115.40220642089844--TrainR2:-6.58144474029541--TestLoss:125.63265228271484--TestR2:-8.416876792907715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:53--TrainLoss:115.38228607177734--TrainR2:-6.580136299133301--TestLoss:125.61148834228516--TestR2:-8.415290832519531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:54--TrainLoss:115.36225891113281--TrainR2:-6.578820705413818--TestLoss:125.5903549194336--TestR2:-8.41370677947998\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:55--TrainLoss:115.34236145019531--TrainR2:-6.577513217926025--TestLoss:125.56919860839844--TestR2:-8.412120819091797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:56--TrainLoss:115.32235717773438--TrainR2:-6.576199054718018--TestLoss:125.54808044433594--TestR2:-8.410537719726562\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:57--TrainLoss:115.3023910522461--TrainR2:-6.574887275695801--TestLoss:125.52693939208984--TestR2:-8.408953666687012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:58--TrainLoss:115.28242492675781--TrainR2:-6.573575973510742--TestLoss:125.50579833984375--TestR2:-8.407368659973145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:59--TrainLoss:115.26245880126953--TrainR2:-6.572264194488525--TestLoss:125.48467254638672--TestR2:-8.40578556060791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:60--TrainLoss:115.24252319335938--TrainR2:-6.570954322814941--TestLoss:125.46355438232422--TestR2:-8.404202461242676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:61--TrainLoss:115.2225570678711--TrainR2:-6.569642543792725--TestLoss:125.44244384765625--TestR2:-8.402620315551758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:62--TrainLoss:115.20263671875--TrainR2:-6.568334102630615--TestLoss:125.42132568359375--TestR2:-8.401037216186523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:63--TrainLoss:115.18270111083984--TrainR2:-6.567024230957031--TestLoss:125.40019226074219--TestR2:-8.399453163146973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:64--TrainLoss:115.16275024414062--TrainR2:-6.565713405609131--TestLoss:125.37906646728516--TestR2:-8.397869110107422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:65--TrainLoss:115.14279174804688--TrainR2:-6.5644025802612305--TestLoss:125.35794830322266--TestR2:-8.396286010742188\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:66--TrainLoss:115.12287902832031--TrainR2:-6.563094615936279--TestLoss:125.33686065673828--TestR2:-8.394705772399902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:67--TrainLoss:115.1029052734375--TrainR2:-6.561781883239746--TestLoss:125.31575775146484--TestR2:-8.393123626708984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:68--TrainLoss:115.08301544189453--TrainR2:-6.5604753494262695--TestLoss:125.29464721679688--TestR2:-8.391541481018066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:69--TrainLoss:115.06304931640625--TrainR2:-6.559163570404053--TestLoss:125.27355194091797--TestR2:-8.389960289001465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:70--TrainLoss:115.04315948486328--TrainR2:-6.557857036590576--TestLoss:125.25247955322266--TestR2:-8.388381004333496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:71--TrainLoss:115.02320861816406--TrainR2:-6.556546211242676--TestLoss:125.23136138916016--TestR2:-8.386797904968262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:72--TrainLoss:115.00330352783203--TrainR2:-6.555238723754883--TestLoss:125.21028900146484--TestR2:-8.385218620300293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:73--TrainLoss:114.98336791992188--TrainR2:-6.553928852081299--TestLoss:125.18917846679688--TestR2:-8.383636474609375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:74--TrainLoss:114.9634780883789--TrainR2:-6.552622318267822--TestLoss:125.1680908203125--TestR2:-8.38205623626709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:75--TrainLoss:114.94355010986328--TrainR2:-6.551313400268555--TestLoss:125.14703369140625--TestR2:-8.380476951599121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:76--TrainLoss:114.92366027832031--TrainR2:-6.550006866455078--TestLoss:125.12593841552734--TestR2:-8.37889575958252\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:77--TrainLoss:114.9037094116211--TrainR2:-6.548696041107178--TestLoss:125.10486602783203--TestR2:-8.37731647491455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:78--TrainLoss:114.88383483886719--TrainR2:-6.547389984130859--TestLoss:125.08379364013672--TestR2:-8.375737190246582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:79--TrainLoss:114.86396026611328--TrainR2:-6.546084403991699--TestLoss:125.06272888183594--TestR2:-8.374157905578613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:80--TrainLoss:114.84403991699219--TrainR2:-6.54477596282959--TestLoss:125.04164123535156--TestR2:-8.372577667236328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:81--TrainLoss:114.82415771484375--TrainR2:-6.543469429016113--TestLoss:125.02058410644531--TestR2:-8.370999336242676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:82--TrainLoss:114.80423736572266--TrainR2:-6.542160987854004--TestLoss:124.99954986572266--TestR2:-8.36942195892334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:83--TrainLoss:114.78437805175781--TrainR2:-6.54085636138916--TestLoss:124.97847747802734--TestR2:-8.367842674255371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:84--TrainLoss:114.76446533203125--TrainR2:-6.539547920227051--TestLoss:124.95740509033203--TestR2:-8.366263389587402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:85--TrainLoss:114.74461364746094--TrainR2:-6.538243770599365--TestLoss:124.93637084960938--TestR2:-8.364686965942383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:86--TrainLoss:114.72470092773438--TrainR2:-6.536935329437256--TestLoss:124.91531372070312--TestR2:-8.36310863494873\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:87--TrainLoss:114.70484924316406--TrainR2:-6.5356316566467285--TestLoss:124.89427185058594--TestR2:-8.361531257629395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:88--TrainLoss:114.68497467041016--TrainR2:-6.53432559967041--TestLoss:124.87322235107422--TestR2:-8.359953880310059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:89--TrainLoss:114.66510009765625--TrainR2:-6.53302001953125--TestLoss:124.85218811035156--TestR2:-8.358376502990723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:90--TrainLoss:114.64524841308594--TrainR2:-6.5317158699035645--TestLoss:124.8311538696289--TestR2:-8.356800079345703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:91--TrainLoss:114.62535858154297--TrainR2:-6.530409336090088--TestLoss:124.81011199951172--TestR2:-8.355222702026367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:92--TrainLoss:114.6054916381836--TrainR2:-6.529104232788086--TestLoss:124.78907775878906--TestR2:-8.353646278381348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:93--TrainLoss:114.58563232421875--TrainR2:-6.527799606323242--TestLoss:124.76805114746094--TestR2:-8.352070808410645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:94--TrainLoss:114.56578063964844--TrainR2:-6.526495456695557--TestLoss:124.74701690673828--TestR2:-8.350494384765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:95--TrainLoss:114.54592895507812--TrainR2:-6.525191307067871--TestLoss:124.72599792480469--TestR2:-8.348917961120605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:96--TrainLoss:114.52609252929688--TrainR2:-6.523888111114502--TestLoss:124.7049789428711--TestR2:-8.347342491149902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:97--TrainLoss:114.50623321533203--TrainR2:-6.5225830078125--TestLoss:124.6839599609375--TestR2:-8.3457670211792\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:98--TrainLoss:114.48638916015625--TrainR2:-6.521279811859131--TestLoss:124.66294860839844--TestR2:-8.344192504882812\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:99--TrainLoss:114.46659088134766--TrainR2:-6.519979000091553--TestLoss:124.64192962646484--TestR2:-8.34261703491211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:100--TrainLoss:114.44669342041016--TrainR2:-6.51867151260376--TestLoss:124.62093353271484--TestR2:-8.341042518615723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:101--TrainLoss:114.4268798828125--TrainR2:-6.517370223999023--TestLoss:124.59989929199219--TestR2:-8.339466094970703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:102--TrainLoss:114.4071044921875--TrainR2:-6.51607084274292--TestLoss:124.57890319824219--TestR2:-8.337892532348633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:103--TrainLoss:114.38717651367188--TrainR2:-6.514761924743652--TestLoss:124.55789184570312--TestR2:-8.336318016052246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:104--TrainLoss:114.36740112304688--TrainR2:-6.513463020324707--TestLoss:124.53691101074219--TestR2:-8.334744453430176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:105--TrainLoss:114.34752655029297--TrainR2:-6.512156963348389--TestLoss:124.51593780517578--TestR2:-8.333172798156738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:106--TrainLoss:114.32775115966797--TrainR2:-6.510858058929443--TestLoss:124.49492645263672--TestR2:-8.331598281860352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:107--TrainLoss:114.30791473388672--TrainR2:-6.509554862976074--TestLoss:124.47396087646484--TestR2:-8.330026626586914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:108--TrainLoss:114.28807830810547--TrainR2:-6.508251667022705--TestLoss:124.45296478271484--TestR2:-8.328453063964844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:109--TrainLoss:114.26831817626953--TrainR2:-6.506953239440918--TestLoss:124.43196868896484--TestR2:-8.326878547668457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:110--TrainLoss:114.24846649169922--TrainR2:-6.505649089813232--TestLoss:124.41098022460938--TestR2:-8.325305938720703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:111--TrainLoss:114.22868347167969--TrainR2:-6.504349231719971--TestLoss:124.39002227783203--TestR2:-8.323735237121582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:112--TrainLoss:114.20885467529297--TrainR2:-6.503046989440918--TestLoss:124.3690414428711--TestR2:-8.322162628173828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:113--TrainLoss:114.18907165527344--TrainR2:-6.501747131347656--TestLoss:124.34806823730469--TestR2:-8.320590019226074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:114--TrainLoss:114.16929626464844--TrainR2:-6.500447750091553--TestLoss:124.3271255493164--TestR2:-8.31902027130127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:115--TrainLoss:114.14944458007812--TrainR2:-6.499144077301025--TestLoss:124.30615234375--TestR2:-8.317448616027832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:116--TrainLoss:114.12968444824219--TrainR2:-6.497845649719238--TestLoss:124.28519439697266--TestR2:-8.315876960754395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:117--TrainLoss:114.10987091064453--TrainR2:-6.496543884277344--TestLoss:124.26419830322266--TestR2:-8.314303398132324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:118--TrainLoss:114.0901107788086--TrainR2:-6.495245933532715--TestLoss:124.2432632446289--TestR2:-8.312734603881836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:119--TrainLoss:114.07035064697266--TrainR2:-6.493947505950928--TestLoss:124.22230529785156--TestR2:-8.311163902282715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:120--TrainLoss:114.050537109375--TrainR2:-6.492646217346191--TestLoss:124.20137023925781--TestR2:-8.30959415435791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:121--TrainLoss:114.03077697753906--TrainR2:-6.491347789764404--TestLoss:124.18041229248047--TestR2:-8.308023452758789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:122--TrainLoss:114.01097869873047--TrainR2:-6.490046977996826--TestLoss:124.15947723388672--TestR2:-8.3064546585083\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:123--TrainLoss:113.99120330810547--TrainR2:-6.488748073577881--TestLoss:124.13853454589844--TestR2:-8.304884910583496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:124--TrainLoss:113.9714584350586--TrainR2:-6.48745059967041--TestLoss:124.11759948730469--TestR2:-8.303315162658691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:125--TrainLoss:113.95169830322266--TrainR2:-6.486152648925781--TestLoss:124.0966567993164--TestR2:-8.301745414733887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:126--TrainLoss:113.93192291259766--TrainR2:-6.484853267669678--TestLoss:124.07574462890625--TestR2:-8.300178527832031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:127--TrainLoss:113.91212463378906--TrainR2:-6.4835524559021--TestLoss:124.05477142333984--TestR2:-8.298605918884277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:128--TrainLoss:113.89238739013672--TrainR2:-6.482255935668945--TestLoss:124.03390502929688--TestR2:-8.297041893005371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:129--TrainLoss:113.87265014648438--TrainR2:-6.480959415435791--TestLoss:124.01293182373047--TestR2:-8.295470237731934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:130--TrainLoss:113.85286712646484--TrainR2:-6.479659557342529--TestLoss:123.99201202392578--TestR2:-8.293901443481445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:131--TrainLoss:113.83314514160156--TrainR2:-6.478363990783691--TestLoss:123.97110748291016--TestR2:-8.29233455657959\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:132--TrainLoss:113.81343078613281--TrainR2:-6.477068901062012--TestLoss:123.9501953125--TestR2:-8.290767669677734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:133--TrainLoss:113.79364776611328--TrainR2:-6.47576904296875--TestLoss:123.92928314208984--TestR2:-8.289199829101562\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:134--TrainLoss:113.77389526367188--TrainR2:-6.4744720458984375--TestLoss:123.90836334228516--TestR2:-8.28763198852539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:135--TrainLoss:113.754150390625--TrainR2:-6.473174571990967--TestLoss:123.88746643066406--TestR2:-8.286066055297852\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:136--TrainLoss:113.73445129394531--TrainR2:-6.4718804359436035--TestLoss:123.86654663085938--TestR2:-8.284497261047363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:137--TrainLoss:113.71469116210938--TrainR2:-6.470582008361816--TestLoss:123.84566497802734--TestR2:-8.28293228149414\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:138--TrainLoss:113.6949691772461--TrainR2:-6.4692864418029785--TestLoss:123.82476806640625--TestR2:-8.281366348266602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:139--TrainLoss:113.67524719238281--TrainR2:-6.467990875244141--TestLoss:123.80387115478516--TestR2:-8.279799461364746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:140--TrainLoss:113.65550994873047--TrainR2:-6.466694355010986--TestLoss:123.78297424316406--TestR2:-8.278233528137207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:141--TrainLoss:113.63577270507812--TrainR2:-6.465397834777832--TestLoss:123.76206970214844--TestR2:-8.276666641235352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:142--TrainLoss:113.6160888671875--TrainR2:-6.464104652404785--TestLoss:123.7412109375--TestR2:-8.275102615356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:143--TrainLoss:113.59634399414062--TrainR2:-6.4628071784973145--TestLoss:123.7203369140625--TestR2:-8.273538589477539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:144--TrainLoss:113.57666778564453--TrainR2:-6.461514949798584--TestLoss:123.69944763183594--TestR2:-8.27197265625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:145--TrainLoss:113.55696105957031--TrainR2:-6.460219860076904--TestLoss:123.67857360839844--TestR2:-8.270407676696777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:146--TrainLoss:113.53723907470703--TrainR2:-6.458924293518066--TestLoss:123.6576919555664--TestR2:-8.268842697143555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:147--TrainLoss:113.51752471923828--TrainR2:-6.457629203796387--TestLoss:123.6368179321289--TestR2:-8.267277717590332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:148--TrainLoss:113.49783325195312--TrainR2:-6.456335544586182--TestLoss:123.61595916748047--TestR2:-8.265714645385742\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:149--TrainLoss:113.47811889648438--TrainR2:-6.455040454864502--TestLoss:123.59507751464844--TestR2:-8.26414966583252\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:150--TrainLoss:113.45845031738281--TrainR2:-6.4537482261657715--TestLoss:123.5742416381836--TestR2:-8.262587547302246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:151--TrainLoss:113.43876647949219--TrainR2:-6.452455043792725--TestLoss:123.55337524414062--TestR2:-8.26102352142334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:152--TrainLoss:113.4190673828125--TrainR2:-6.451160907745361--TestLoss:123.53250122070312--TestR2:-8.259458541870117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:153--TrainLoss:113.39936828613281--TrainR2:-6.449866771697998--TestLoss:123.5116958618164--TestR2:-8.257899284362793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:154--TrainLoss:113.37970733642578--TrainR2:-6.448575496673584--TestLoss:123.4908218383789--TestR2:-8.256335258483887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:155--TrainLoss:113.3599853515625--TrainR2:-6.447279930114746--TestLoss:123.47000122070312--TestR2:-8.25477409362793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:156--TrainLoss:113.34037017822266--TrainR2:-6.445990562438965--TestLoss:123.4491195678711--TestR2:-8.253209114074707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:157--TrainLoss:113.32069396972656--TrainR2:-6.444698333740234--TestLoss:123.42830657958984--TestR2:-8.251648902893066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:158--TrainLoss:113.30095672607422--TrainR2:-6.44340181350708--TestLoss:123.40745544433594--TestR2:-8.250085830688477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:159--TrainLoss:113.28136444091797--TrainR2:-6.442114353179932--TestLoss:123.3866195678711--TestR2:-8.248523712158203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:160--TrainLoss:113.2616958618164--TrainR2:-6.440822124481201--TestLoss:123.36579895019531--TestR2:-8.246963500976562\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:161--TrainLoss:113.24197387695312--TrainR2:-6.439526557922363--TestLoss:123.34495544433594--TestR2:-8.245401382446289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:162--TrainLoss:113.22232055664062--TrainR2:-6.438235759735107--TestLoss:123.32414245605469--TestR2:-8.243841171264648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:163--TrainLoss:113.20271301269531--TrainR2:-6.436947822570801--TestLoss:123.30330657958984--TestR2:-8.242280006408691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:164--TrainLoss:113.1830062866211--TrainR2:-6.435652732849121--TestLoss:123.28247833251953--TestR2:-8.240717887878418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:165--TrainLoss:113.16336059570312--TrainR2:-6.434361934661865--TestLoss:123.26168060302734--TestR2:-8.23915958404541\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:166--TrainLoss:113.14373016357422--TrainR2:-6.433072090148926--TestLoss:123.24085998535156--TestR2:-8.237598419189453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:167--TrainLoss:113.12405395507812--TrainR2:-6.431779861450195--TestLoss:123.22003936767578--TestR2:-8.236038208007812\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:168--TrainLoss:113.10444641113281--TrainR2:-6.430491924285889--TestLoss:123.19923400878906--TestR2:-8.234477996826172\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:169--TrainLoss:113.08478546142578--TrainR2:-6.429200172424316--TestLoss:123.17843627929688--TestR2:-8.232919692993164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:170--TrainLoss:113.06515502929688--TrainR2:-6.427910327911377--TestLoss:123.15763854980469--TestR2:-8.23136043548584\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:171--TrainLoss:113.0455093383789--TrainR2:-6.426620006561279--TestLoss:123.1368179321289--TestR2:-8.2298002243042\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:172--TrainLoss:113.02591705322266--TrainR2:-6.425332546234131--TestLoss:123.11603546142578--TestR2:-8.228242874145508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:173--TrainLoss:113.00625610351562--TrainR2:-6.424040794372559--TestLoss:123.09525299072266--TestR2:-8.2266845703125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:174--TrainLoss:112.98664855957031--TrainR2:-6.422752857208252--TestLoss:123.07445526123047--TestR2:-8.225126266479492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:175--TrainLoss:112.967041015625--TrainR2:-6.421464443206787--TestLoss:123.05367279052734--TestR2:-8.223567962646484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:176--TrainLoss:112.94739532470703--TrainR2:-6.4201741218566895--TestLoss:123.03287506103516--TestR2:-8.222009658813477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:177--TrainLoss:112.92777252197266--TrainR2:-6.418885231018066--TestLoss:123.01209259033203--TestR2:-8.220451354980469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:178--TrainLoss:112.9081802368164--TrainR2:-6.417597770690918--TestLoss:122.99132537841797--TestR2:-8.218894958496094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:179--TrainLoss:112.88855743408203--TrainR2:-6.416308879852295--TestLoss:122.97055053710938--TestR2:-8.217337608337402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:180--TrainLoss:112.86893463134766--TrainR2:-6.415019512176514--TestLoss:122.94976806640625--TestR2:-8.215780258178711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:181--TrainLoss:112.84935760498047--TrainR2:-6.413733005523682--TestLoss:122.92900085449219--TestR2:-8.21422290802002\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:182--TrainLoss:112.82971954345703--TrainR2:-6.412443161010742--TestLoss:122.90824127197266--TestR2:-8.212667465209961\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:183--TrainLoss:112.81015014648438--TrainR2:-6.411157608032227--TestLoss:122.88748168945312--TestR2:-8.211111068725586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:184--TrainLoss:112.79055786132812--TrainR2:-6.409870624542236--TestLoss:122.86671447753906--TestR2:-8.209553718566895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:185--TrainLoss:112.77095794677734--TrainR2:-6.408583164215088--TestLoss:122.84595489501953--TestR2:-8.207998275756836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:186--TrainLoss:112.75138092041016--TrainR2:-6.407296657562256--TestLoss:122.8251953125--TestR2:-8.206441879272461\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:187--TrainLoss:112.73178100585938--TrainR2:-6.406009197235107--TestLoss:122.80445098876953--TestR2:-8.204887390136719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:188--TrainLoss:112.71216583251953--TrainR2:-6.404720306396484--TestLoss:122.78369140625--TestR2:-8.203330993652344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:189--TrainLoss:112.69261932373047--TrainR2:-6.403436183929443--TestLoss:122.76295471191406--TestR2:-8.201777458190918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:190--TrainLoss:112.67302703857422--TrainR2:-6.402149200439453--TestLoss:122.74220275878906--TestR2:-8.200221061706543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:191--TrainLoss:112.65345764160156--TrainR2:-6.4008636474609375--TestLoss:122.72145080566406--TestR2:-8.1986665725708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:192--TrainLoss:112.63385772705078--TrainR2:-6.399576187133789--TestLoss:122.70072174072266--TestR2:-8.197112083435059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:193--TrainLoss:112.61431884765625--TrainR2:-6.398292064666748--TestLoss:122.67998504638672--TestR2:-8.195558547973633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:194--TrainLoss:112.5947494506836--TrainR2:-6.397006511688232--TestLoss:122.65925598144531--TestR2:-8.19400405883789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:195--TrainLoss:112.57518768310547--TrainR2:-6.395721912384033--TestLoss:122.63853454589844--TestR2:-8.192450523376465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:196--TrainLoss:112.55560302734375--TrainR2:-6.394435405731201--TestLoss:122.61781311035156--TestR2:-8.190897941589355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:197--TrainLoss:112.53608703613281--TrainR2:-6.393152713775635--TestLoss:122.59708404541016--TestR2:-8.189343452453613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:198--TrainLoss:112.51653289794922--TrainR2:-6.3918681144714355--TestLoss:122.57637023925781--TestR2:-8.18779182434082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:199--TrainLoss:112.4969253540039--TrainR2:-6.390580177307129--TestLoss:122.5556411743164--TestR2:-8.186237335205078\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:200--TrainLoss:112.4773941040039--TrainR2:-6.389297008514404--TestLoss:122.53492736816406--TestR2:-8.184685707092285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:201--TrainLoss:112.4578628540039--TrainR2:-6.38801383972168--TestLoss:122.51422119140625--TestR2:-8.183133125305176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:202--TrainLoss:112.43830108642578--TrainR2:-6.386728763580322--TestLoss:122.49351501464844--TestR2:-8.181580543518066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:203--TrainLoss:112.41876220703125--TrainR2:-6.3854451179504395--TestLoss:122.47279357910156--TestR2:-8.180027961730957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:204--TrainLoss:112.39923095703125--TrainR2:-6.384161949157715--TestLoss:122.45209503173828--TestR2:-8.178476333618164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:205--TrainLoss:112.37969970703125--TrainR2:-6.38287878036499--TestLoss:122.43140411376953--TestR2:-8.176925659179688\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:206--TrainLoss:112.36017608642578--TrainR2:-6.381596565246582--TestLoss:122.41070556640625--TestR2:-8.175374031066895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:207--TrainLoss:112.34063720703125--TrainR2:-6.380312442779541--TestLoss:122.3900146484375--TestR2:-8.173822402954102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:208--TrainLoss:112.32109069824219--TrainR2:-6.3790283203125--TestLoss:122.36931610107422--TestR2:-8.172271728515625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:209--TrainLoss:112.30159759521484--TrainR2:-6.377748012542725--TestLoss:122.3486099243164--TestR2:-8.170719146728516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:210--TrainLoss:112.28204345703125--TrainR2:-6.376462936401367--TestLoss:122.32793426513672--TestR2:-8.169170379638672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:211--TrainLoss:112.26251983642578--TrainR2:-6.375180721282959--TestLoss:122.3072509765625--TestR2:-8.167619705200195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:212--TrainLoss:112.24302673339844--TrainR2:-6.373900413513184--TestLoss:122.28656768798828--TestR2:-8.166069030761719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:213--TrainLoss:112.22352600097656--TrainR2:-6.372618675231934--TestLoss:122.26591491699219--TestR2:-8.164521217346191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:214--TrainLoss:112.20396423339844--TrainR2:-6.371334075927734--TestLoss:122.24523162841797--TestR2:-8.162970542907715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:215--TrainLoss:112.18450164794922--TrainR2:-6.370055198669434--TestLoss:122.22454833984375--TestR2:-8.161419868469238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:216--TrainLoss:112.16497039794922--TrainR2:-6.368772029876709--TestLoss:122.20388793945312--TestR2:-8.159872055053711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:217--TrainLoss:112.14550018310547--TrainR2:-6.367493152618408--TestLoss:122.18321990966797--TestR2:-8.15832233428955\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:218--TrainLoss:112.12598419189453--TrainR2:-6.3662109375--TestLoss:122.16259765625--TestR2:-8.156776428222656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:219--TrainLoss:112.1064682006836--TrainR2:-6.364928245544434--TestLoss:122.14192962646484--TestR2:-8.155227661132812\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:220--TrainLoss:112.0870361328125--TrainR2:-6.363652229309082--TestLoss:122.12126922607422--TestR2:-8.153678894042969\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:221--TrainLoss:112.06748962402344--TrainR2:-6.362368106842041--TestLoss:122.1006088256836--TestR2:-8.152131080627441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:222--TrainLoss:112.04801177978516--TrainR2:-6.361088275909424--TestLoss:122.0799560546875--TestR2:-8.150582313537598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:223--TrainLoss:112.02851867675781--TrainR2:-6.35980749130249--TestLoss:122.05931091308594--TestR2:-8.14903450012207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:224--TrainLoss:112.00907897949219--TrainR2:-6.358530521392822--TestLoss:122.03865814208984--TestR2:-8.14748764038086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:225--TrainLoss:111.98959350585938--TrainR2:-6.357250690460205--TestLoss:122.0180435180664--TestR2:-8.145941734313965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:226--TrainLoss:111.97013092041016--TrainR2:-6.355971813201904--TestLoss:121.99740600585938--TestR2:-8.144394874572754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:227--TrainLoss:111.95062255859375--TrainR2:-6.3546905517578125--TestLoss:121.97673797607422--TestR2:-8.142845153808594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:228--TrainLoss:111.93116760253906--TrainR2:-6.35341215133667--TestLoss:121.95611572265625--TestR2:-8.141300201416016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:229--TrainLoss:111.91168975830078--TrainR2:-6.352132320404053--TestLoss:121.93548583984375--TestR2:-8.139753341674805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:230--TrainLoss:111.8922348022461--TrainR2:-6.350854396820068--TestLoss:121.91486358642578--TestR2:-8.13820743560791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:231--TrainLoss:111.87274169921875--TrainR2:-6.349573612213135--TestLoss:121.89424133300781--TestR2:-8.136662483215332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:232--TrainLoss:111.85330963134766--TrainR2:-6.348297595977783--TestLoss:121.8736343383789--TestR2:-8.135117530822754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:233--TrainLoss:111.8338851928711--TrainR2:-6.347021102905273--TestLoss:121.8530044555664--TestR2:-8.133570671081543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:234--TrainLoss:111.81439971923828--TrainR2:-6.345741271972656--TestLoss:121.8323974609375--TestR2:-8.132026672363281\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:235--TrainLoss:111.79498291015625--TrainR2:-6.344465732574463--TestLoss:121.81178283691406--TestR2:-8.130481719970703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:236--TrainLoss:111.77550506591797--TrainR2:-6.343185901641846--TestLoss:121.79116821289062--TestR2:-8.128935813903809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:237--TrainLoss:111.75607299804688--TrainR2:-6.341909408569336--TestLoss:121.77056121826172--TestR2:-8.12739086151123\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:238--TrainLoss:111.73663330078125--TrainR2:-6.34063196182251--TestLoss:121.74996185302734--TestR2:-8.125847816467285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:239--TrainLoss:111.71717834472656--TrainR2:-6.339354038238525--TestLoss:121.7293701171875--TestR2:-8.124303817749023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:240--TrainLoss:111.69771575927734--TrainR2:-6.338075160980225--TestLoss:121.70875549316406--TestR2:-8.122758865356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:241--TrainLoss:111.6783218383789--TrainR2:-6.336801052093506--TestLoss:121.68816375732422--TestR2:-8.1212158203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:242--TrainLoss:111.65890502929688--TrainR2:-6.3355255126953125--TestLoss:121.66756439208984--TestR2:-8.119671821594238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:243--TrainLoss:111.63947296142578--TrainR2:-6.334249496459961--TestLoss:121.64698791503906--TestR2:-8.118128776550293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:244--TrainLoss:111.6200180053711--TrainR2:-6.332971096038818--TestLoss:121.62641143798828--TestR2:-8.116586685180664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:245--TrainLoss:111.60059356689453--TrainR2:-6.331694602966309--TestLoss:121.60580444335938--TestR2:-8.115042686462402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:246--TrainLoss:111.5811996459961--TrainR2:-6.330420970916748--TestLoss:121.58522033691406--TestR2:-8.113499641418457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:247--TrainLoss:111.56178283691406--TrainR2:-6.329145431518555--TestLoss:121.56465148925781--TestR2:-8.111957550048828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:248--TrainLoss:111.5423812866211--TrainR2:-6.3278703689575195--TestLoss:121.54408264160156--TestR2:-8.1104154586792\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:249--TrainLoss:111.52296447753906--TrainR2:-6.326594829559326--TestLoss:121.52351379394531--TestR2:-8.10887336730957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:250--TrainLoss:111.50354766845703--TrainR2:-6.325319290161133--TestLoss:121.5029296875--TestR2:-8.107331275939941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:251--TrainLoss:111.48414611816406--TrainR2:-6.324044704437256--TestLoss:121.48236846923828--TestR2:-8.105790138244629\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:252--TrainLoss:111.46476745605469--TrainR2:-6.3227715492248535--TestLoss:121.46179962158203--TestR2:-8.104248046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:253--TrainLoss:111.4453353881836--TrainR2:-6.321495056152344--TestLoss:121.44125366210938--TestR2:-8.102707862854004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:254--TrainLoss:111.42597198486328--TrainR2:-6.320222854614258--TestLoss:121.42070770263672--TestR2:-8.101168632507324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:255--TrainLoss:111.40653991699219--TrainR2:-6.318946361541748--TestLoss:121.40015411376953--TestR2:-8.099627494812012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:256--TrainLoss:111.38717651367188--TrainR2:-6.317674160003662--TestLoss:121.37958526611328--TestR2:-8.098085403442383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:257--TrainLoss:111.36775970458984--TrainR2:-6.316398620605469--TestLoss:121.35904693603516--TestR2:-8.096546173095703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:258--TrainLoss:111.34841918945312--TrainR2:-6.315128326416016--TestLoss:121.3385009765625--TestR2:-8.095005989074707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:259--TrainLoss:111.32901763916016--TrainR2:-6.313853740692139--TestLoss:121.31794738769531--TestR2:-8.093465805053711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:260--TrainLoss:111.30965423583984--TrainR2:-6.3125810623168945--TestLoss:121.29741668701172--TestR2:-8.091926574707031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:261--TrainLoss:111.29029846191406--TrainR2:-6.311309814453125--TestLoss:121.27689361572266--TestR2:-8.090388298034668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:262--TrainLoss:111.2708740234375--TrainR2:-6.310033321380615--TestLoss:121.25636291503906--TestR2:-8.088849067687988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:263--TrainLoss:111.25150299072266--TrainR2:-6.308761119842529--TestLoss:121.23582458496094--TestR2:-8.087309837341309\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:264--TrainLoss:111.2321548461914--TrainR2:-6.30748987197876--TestLoss:121.21529388427734--TestR2:-8.085771560668945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:265--TrainLoss:111.21279907226562--TrainR2:-6.30621862411499--TestLoss:121.19477081298828--TestR2:-8.084233283996582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:266--TrainLoss:111.19343566894531--TrainR2:-6.304946422576904--TestLoss:121.17425537109375--TestR2:-8.082695007324219\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:267--TrainLoss:111.17405700683594--TrainR2:-6.303673267364502--TestLoss:121.15372467041016--TestR2:-8.081155776977539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:268--TrainLoss:111.15473175048828--TrainR2:-6.302403450012207--TestLoss:121.13322448730469--TestR2:-8.079619407653809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:269--TrainLoss:111.1353759765625--TrainR2:-6.3011322021484375--TestLoss:121.1126937866211--TestR2:-8.078080177307129\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:270--TrainLoss:111.11599731445312--TrainR2:-6.299859046936035--TestLoss:121.0921859741211--TestR2:-8.076543807983398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:271--TrainLoss:111.09667205810547--TrainR2:-6.29858922958374--TestLoss:121.0716781616211--TestR2:-8.075006484985352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:272--TrainLoss:111.07734680175781--TrainR2:-6.2973198890686035--TestLoss:121.0511703491211--TestR2:-8.073469161987305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:273--TrainLoss:111.0579833984375--TrainR2:-6.296047687530518--TestLoss:121.03070068359375--TestR2:-8.071934700012207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:274--TrainLoss:111.03864288330078--TrainR2:-6.294776916503906--TestLoss:121.01019287109375--TestR2:-8.07039737701416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:275--TrainLoss:111.01931762695312--TrainR2:-6.293507099151611--TestLoss:120.98968505859375--TestR2:-8.068860054016113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:276--TrainLoss:110.9999771118164--TrainR2:-6.292236804962158--TestLoss:120.96918487548828--TestR2:-8.067323684692383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:277--TrainLoss:110.98067474365234--TrainR2:-6.290968418121338--TestLoss:120.94867706298828--TestR2:-8.065787315368652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:278--TrainLoss:110.96134948730469--TrainR2:-6.289699077606201--TestLoss:120.92820739746094--TestR2:-8.064252853393555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:279--TrainLoss:110.94200897216797--TrainR2:-6.28842830657959--TestLoss:120.90773010253906--TestR2:-8.06271743774414\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:280--TrainLoss:110.92266845703125--TrainR2:-6.287158012390137--TestLoss:120.88724517822266--TestR2:-8.061182022094727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:281--TrainLoss:110.90338897705078--TrainR2:-6.285891532897949--TestLoss:120.86675262451172--TestR2:-8.059645652770996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:282--TrainLoss:110.88402557373047--TrainR2:-6.284619331359863--TestLoss:120.8462905883789--TestR2:-8.058112144470215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:283--TrainLoss:110.86473846435547--TrainR2:-6.283351898193359--TestLoss:120.82582092285156--TestR2:-8.056577682495117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:284--TrainLoss:110.8454360961914--TrainR2:-6.282083988189697--TestLoss:120.80533599853516--TestR2:-8.055042266845703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:285--TrainLoss:110.82611846923828--TrainR2:-6.280815124511719--TestLoss:120.78489685058594--TestR2:-8.053509712219238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:286--TrainLoss:110.80679321289062--TrainR2:-6.279545307159424--TestLoss:120.76441955566406--TestR2:-8.051976203918457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:287--TrainLoss:110.78749084472656--TrainR2:-6.278277397155762--TestLoss:120.74396514892578--TestR2:-8.050442695617676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:288--TrainLoss:110.76818084716797--TrainR2:-6.277008533477783--TestLoss:120.72348022460938--TestR2:-8.048907279968262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:289--TrainLoss:110.7489013671875--TrainR2:-6.275742053985596--TestLoss:120.70304870605469--TestR2:-8.047375679016113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:290--TrainLoss:110.7296371459961--TrainR2:-6.274476528167725--TestLoss:120.68257904052734--TestR2:-8.045841217041016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:291--TrainLoss:110.7103271484375--TrainR2:-6.273207664489746--TestLoss:120.66214752197266--TestR2:-8.044309616088867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:292--TrainLoss:110.69104766845703--TrainR2:-6.271941661834717--TestLoss:120.6417007446289--TestR2:-8.042777061462402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:293--TrainLoss:110.67172241210938--TrainR2:-6.270671844482422--TestLoss:120.62125396728516--TestR2:-8.041244506835938\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:294--TrainLoss:110.65247344970703--TrainR2:-6.269407272338867--TestLoss:120.60079956054688--TestR2:-8.039710998535156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:295--TrainLoss:110.63320922851562--TrainR2:-6.268141746520996--TestLoss:120.58039093017578--TestR2:-8.03818130493164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:296--TrainLoss:110.61392974853516--TrainR2:-6.266875267028809--TestLoss:120.55994415283203--TestR2:-8.036648750305176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:297--TrainLoss:110.59465789794922--TrainR2:-6.265608787536621--TestLoss:120.53951263427734--TestR2:-8.035117149353027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:298--TrainLoss:110.57539367675781--TrainR2:-6.26434326171875--TestLoss:120.51908111572266--TestR2:-8.033585548400879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:299--TrainLoss:110.55610656738281--TrainR2:-6.263076305389404--TestLoss:120.4986572265625--TestR2:-8.032054901123047\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:300--TrainLoss:110.53682708740234--TrainR2:-6.261809825897217--TestLoss:120.47822570800781--TestR2:-8.030524253845215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:301--TrainLoss:110.517578125--TrainR2:-6.260545253753662--TestLoss:120.45781707763672--TestR2:-8.0289945602417\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:302--TrainLoss:110.49827575683594--TrainR2:-6.25927734375--TestLoss:120.43738555908203--TestR2:-8.02746295928955\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:303--TrainLoss:110.47908020019531--TrainR2:-6.258016109466553--TestLoss:120.4169921875--TestR2:-8.025934219360352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:304--TrainLoss:110.4598388671875--TrainR2:-6.256752014160156--TestLoss:120.39656066894531--TestR2:-8.024402618408203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:305--TrainLoss:110.44062805175781--TrainR2:-6.255489826202393--TestLoss:120.37616729736328--TestR2:-8.022873878479004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:306--TrainLoss:110.42130279541016--TrainR2:-6.254220008850098--TestLoss:120.35575866699219--TestR2:-8.021344184875488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:307--TrainLoss:110.4020767211914--TrainR2:-6.252957344055176--TestLoss:120.33536529541016--TestR2:-8.019815444946289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:308--TrainLoss:110.38282775878906--TrainR2:-6.251692771911621--TestLoss:120.31495666503906--TestR2:-8.018285751342773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:309--TrainLoss:110.36358642578125--TrainR2:-6.250428676605225--TestLoss:120.29454803466797--TestR2:-8.016756057739258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:310--TrainLoss:110.34435272216797--TrainR2:-6.249165058135986--TestLoss:120.27416229248047--TestR2:-8.015228271484375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:311--TrainLoss:110.32513427734375--TrainR2:-6.247902870178223--TestLoss:120.25375366210938--TestR2:-8.01369857788086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:312--TrainLoss:110.30590057373047--TrainR2:-6.246638774871826--TestLoss:120.2333755493164--TestR2:-8.012170791625977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:313--TrainLoss:110.28665924072266--TrainR2:-6.245375156402588--TestLoss:120.21296691894531--TestR2:-8.010641098022461\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:314--TrainLoss:110.26744079589844--TrainR2:-6.244112014770508--TestLoss:120.19263458251953--TestR2:-8.009117126464844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:315--TrainLoss:110.24825286865234--TrainR2:-6.242851734161377--TestLoss:120.17221069335938--TestR2:-8.007586479187012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:316--TrainLoss:110.22900390625--TrainR2:-6.241586685180664--TestLoss:120.15184020996094--TestR2:-8.006059646606445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:317--TrainLoss:110.20976257324219--TrainR2:-6.240323066711426--TestLoss:120.13146209716797--TestR2:-8.004531860351562\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:318--TrainLoss:110.19053649902344--TrainR2:-6.239059925079346--TestLoss:120.11109924316406--TestR2:-8.003005981445312\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:319--TrainLoss:110.1713638305664--TrainR2:-6.237800598144531--TestLoss:120.09073638916016--TestR2:-8.001479148864746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:320--TrainLoss:110.15213012695312--TrainR2:-6.236536502838135--TestLoss:120.07035064697266--TestR2:-7.999951362609863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:321--TrainLoss:110.13294219970703--TrainR2:-6.235276222229004--TestLoss:120.05001831054688--TestR2:-7.998427391052246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:322--TrainLoss:110.11375427246094--TrainR2:-6.234015464782715--TestLoss:120.02964782714844--TestR2:-7.99690055847168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:323--TrainLoss:110.09454345703125--TrainR2:-6.232753276824951--TestLoss:120.00929260253906--TestR2:-7.99537467956543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:324--TrainLoss:110.0753402709961--TrainR2:-6.231492042541504--TestLoss:119.98892974853516--TestR2:-7.99384880065918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:325--TrainLoss:110.0561294555664--TrainR2:-6.23022985458374--TestLoss:119.96858978271484--TestR2:-7.99232292175293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:326--TrainLoss:110.03697204589844--TrainR2:-6.228971004486084--TestLoss:119.9482192993164--TestR2:-7.990796089172363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:327--TrainLoss:110.01778411865234--TrainR2:-6.227710723876953--TestLoss:119.92789459228516--TestR2:-7.9892730712890625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:328--TrainLoss:109.99858093261719--TrainR2:-6.226449489593506--TestLoss:119.90755462646484--TestR2:-7.987748146057129\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:329--TrainLoss:109.97940826416016--TrainR2:-6.225189685821533--TestLoss:119.88720703125--TestR2:-7.986223220825195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:330--TrainLoss:109.96023559570312--TrainR2:-6.223930358886719--TestLoss:119.86687469482422--TestR2:-7.984699249267578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:331--TrainLoss:109.94104766845703--TrainR2:-6.22266960144043--TestLoss:119.84652709960938--TestR2:-7.9831743240356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:332--TrainLoss:109.92191314697266--TrainR2:-6.221412658691406--TestLoss:119.82621002197266--TestR2:-7.981651306152344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:333--TrainLoss:109.90270233154297--TrainR2:-6.220150470733643--TestLoss:119.80587768554688--TestR2:-7.980127334594727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:334--TrainLoss:109.88352966308594--TrainR2:-6.21889066696167--TestLoss:119.78553009033203--TestR2:-7.978602409362793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:335--TrainLoss:109.8643798828125--TrainR2:-6.21763277053833--TestLoss:119.76525115966797--TestR2:-7.977082252502441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:336--TrainLoss:109.8451919555664--TrainR2:-6.216372489929199--TestLoss:119.74488830566406--TestR2:-7.975556373596191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:337--TrainLoss:109.82605743408203--TrainR2:-6.215115070343018--TestLoss:119.7245864868164--TestR2:-7.974034309387207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:338--TrainLoss:109.806884765625--TrainR2:-6.213855743408203--TestLoss:119.70427703857422--TestR2:-7.972512245178223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:339--TrainLoss:109.7877197265625--TrainR2:-6.212596893310547--TestLoss:119.6839828491211--TestR2:-7.970990180969238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:340--TrainLoss:109.76856231689453--TrainR2:-6.211338043212891--TestLoss:119.66365814208984--TestR2:-7.9694671630859375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:341--TrainLoss:109.7494125366211--TrainR2:-6.210080146789551--TestLoss:119.64334106445312--TestR2:-7.967944145202637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:342--TrainLoss:109.73027801513672--TrainR2:-6.208822727203369--TestLoss:119.623046875--TestR2:-7.966423034667969\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:343--TrainLoss:109.71114349365234--TrainR2:-6.207565784454346--TestLoss:119.60272979736328--TestR2:-7.964900016784668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:344--TrainLoss:109.69200897216797--TrainR2:-6.206308364868164--TestLoss:119.58245849609375--TestR2:-7.963380813598633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:345--TrainLoss:109.67285919189453--TrainR2:-6.205050468444824--TestLoss:119.5621337890625--TestR2:-7.961857795715332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:346--TrainLoss:109.65373229980469--TrainR2:-6.203794479370117--TestLoss:119.54185485839844--TestR2:-7.9603376388549805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:347--TrainLoss:109.63459777832031--TrainR2:-6.2025370597839355--TestLoss:119.52156829833984--TestR2:-7.958817481994629\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:348--TrainLoss:109.61548614501953--TrainR2:-6.201281547546387--TestLoss:119.50128936767578--TestR2:-7.957296371459961\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:349--TrainLoss:109.59632873535156--TrainR2:-6.200023174285889--TestLoss:119.48101043701172--TestR2:-7.955776214599609\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:350--TrainLoss:109.57720947265625--TrainR2:-6.198766708374023--TestLoss:119.46073913574219--TestR2:-7.954257011413574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:351--TrainLoss:109.55810546875--TrainR2:-6.197512149810791--TestLoss:119.4404525756836--TestR2:-7.952736854553223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:352--TrainLoss:109.53900146484375--TrainR2:-6.196256637573242--TestLoss:119.42018127441406--TestR2:-7.9512176513671875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:353--TrainLoss:109.5198745727539--TrainR2:-6.195000171661377--TestLoss:119.39990234375--TestR2:-7.949697494506836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:354--TrainLoss:109.5007553100586--TrainR2:-6.19374418258667--TestLoss:119.37964630126953--TestR2:-7.948179244995117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:355--TrainLoss:109.48162841796875--TrainR2:-6.192487716674805--TestLoss:119.3593521118164--TestR2:-7.946657180786133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:356--TrainLoss:109.46251678466797--TrainR2:-6.191232204437256--TestLoss:119.33910369873047--TestR2:-7.9451398849487305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:357--TrainLoss:109.44342041015625--TrainR2:-6.189977645874023--TestLoss:119.31886291503906--TestR2:-7.943622589111328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:358--TrainLoss:109.4243392944336--TrainR2:-6.188724040985107--TestLoss:119.29859161376953--TestR2:-7.942103385925293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:359--TrainLoss:109.40524291992188--TrainR2:-6.187469482421875--TestLoss:119.2783203125--TestR2:-7.940584182739258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:360--TrainLoss:109.38614654541016--TrainR2:-6.186214923858643--TestLoss:119.25808715820312--TestR2:-7.939067840576172\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:361--TrainLoss:109.36705780029297--TrainR2:-6.184960842132568--TestLoss:119.23785400390625--TestR2:-7.9375505447387695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:362--TrainLoss:109.34790802001953--TrainR2:-6.1837029457092285--TestLoss:119.21761322021484--TestR2:-7.936033248901367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:363--TrainLoss:109.32882690429688--TrainR2:-6.1824493408203125--TestLoss:119.19734191894531--TestR2:-7.934514045715332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:364--TrainLoss:109.30975341796875--TrainR2:-6.181196212768555--TestLoss:119.1771240234375--TestR2:-7.9329986572265625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:365--TrainLoss:109.29069519042969--TrainR2:-6.179944038391113--TestLoss:119.1568603515625--TestR2:-7.931480407714844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:366--TrainLoss:109.27162170410156--TrainR2:-6.178691387176514--TestLoss:119.13665771484375--TestR2:-7.929965019226074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:367--TrainLoss:109.25252532958984--TrainR2:-6.177436828613281--TestLoss:119.11642456054688--TestR2:-7.928448677062988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:368--TrainLoss:109.23346710205078--TrainR2:-6.17618465423584--TestLoss:119.0961685180664--TestR2:-7.9269304275512695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:369--TrainLoss:109.21440887451172--TrainR2:-6.174932479858398--TestLoss:119.07595825195312--TestR2:-7.925415992736816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:370--TrainLoss:109.1953125--TrainR2:-6.173677921295166--TestLoss:119.05574035644531--TestR2:-7.923900604248047\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:371--TrainLoss:109.17625427246094--TrainR2:-6.172425746917725--TestLoss:119.03550720214844--TestR2:-7.9223833084106445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:372--TrainLoss:109.15719604492188--TrainR2:-6.171173572540283--TestLoss:119.01529693603516--TestR2:-7.920868873596191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:373--TrainLoss:109.13812255859375--TrainR2:-6.169920921325684--TestLoss:118.9950942993164--TestR2:-7.919354438781738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:374--TrainLoss:109.11906433105469--TrainR2:-6.168668746948242--TestLoss:118.97486114501953--TestR2:-7.917838096618652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:375--TrainLoss:109.10002899169922--TrainR2:-6.167418479919434--TestLoss:118.95466613769531--TestR2:-7.916324615478516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:376--TrainLoss:109.08097076416016--TrainR2:-6.166166305541992--TestLoss:118.9344711303711--TestR2:-7.9148101806640625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:377--TrainLoss:109.06193542480469--TrainR2:-6.164915561676025--TestLoss:118.91425323486328--TestR2:-7.913294792175293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:378--TrainLoss:109.04289245605469--TrainR2:-6.1636643409729--TestLoss:118.89404296875--TestR2:-7.91178035736084\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:379--TrainLoss:109.02384948730469--TrainR2:-6.162413120269775--TestLoss:118.87387084960938--TestR2:-7.910268783569336\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:380--TrainLoss:109.00482177734375--TrainR2:-6.161163806915283--TestLoss:118.85367584228516--TestR2:-7.908754348754883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:381--TrainLoss:108.98580169677734--TrainR2:-6.159914016723633--TestLoss:118.8334732055664--TestR2:-7.90723991394043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:382--TrainLoss:108.96674346923828--TrainR2:-6.158661842346191--TestLoss:118.81327819824219--TestR2:-7.905726432800293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:383--TrainLoss:108.94772338867188--TrainR2:-6.157412528991699--TestLoss:118.7930908203125--TestR2:-7.904213905334473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:384--TrainLoss:108.9286880493164--TrainR2:-6.156161785125732--TestLoss:118.7729263305664--TestR2:-7.902701377868652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:385--TrainLoss:108.90966796875--TrainR2:-6.15491247177124--TestLoss:118.75271606445312--TestR2:-7.901186943054199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:386--TrainLoss:108.89064025878906--TrainR2:-6.153662204742432--TestLoss:118.7325439453125--TestR2:-7.899675369262695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:387--TrainLoss:108.87162017822266--TrainR2:-6.152412414550781--TestLoss:118.7123794555664--TestR2:-7.898163795471191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:388--TrainLoss:108.85260009765625--TrainR2:-6.151163101196289--TestLoss:118.69222259521484--TestR2:-7.8966522216796875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:389--TrainLoss:108.83358764648438--TrainR2:-6.149914264678955--TestLoss:118.67205047607422--TestR2:-7.895140647888184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:390--TrainLoss:108.81458282470703--TrainR2:-6.148665428161621--TestLoss:118.65187072753906--TestR2:-7.893628120422363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:391--TrainLoss:108.79557037353516--TrainR2:-6.147416591644287--TestLoss:118.63172149658203--TestR2:-7.892117500305176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:392--TrainLoss:108.77658081054688--TrainR2:-6.1461687088012695--TestLoss:118.61155700683594--TestR2:-7.890605926513672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:393--TrainLoss:108.75756072998047--TrainR2:-6.144919395446777--TestLoss:118.5914077758789--TestR2:-7.889096260070801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:394--TrainLoss:108.7385482788086--TrainR2:-6.143670558929443--TestLoss:118.57125091552734--TestR2:-7.887585639953613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:395--TrainLoss:108.71955108642578--TrainR2:-6.142422676086426--TestLoss:118.55107116699219--TestR2:-7.886072158813477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:396--TrainLoss:108.7005615234375--TrainR2:-6.141174793243408--TestLoss:118.53092193603516--TestR2:-7.8845624923706055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:397--TrainLoss:108.68156433105469--TrainR2:-6.139926910400391--TestLoss:118.51079559326172--TestR2:-7.883053779602051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:398--TrainLoss:108.66256713867188--TrainR2:-6.138679027557373--TestLoss:118.49066162109375--TestR2:-7.88154411315918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:399--TrainLoss:108.64360046386719--TrainR2:-6.13743257522583--TestLoss:118.47051239013672--TestR2:-7.880034446716309\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:400--TrainLoss:108.62461853027344--TrainR2:-6.136185646057129--TestLoss:118.45035552978516--TestR2:-7.878523826599121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:401--TrainLoss:108.60563659667969--TrainR2:-6.134938716888428--TestLoss:118.43021392822266--TestR2:-7.877013206481934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:402--TrainLoss:108.58665466308594--TrainR2:-6.133691310882568--TestLoss:118.41009521484375--TestR2:-7.875505447387695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:403--TrainLoss:108.56768035888672--TrainR2:-6.132445335388184--TestLoss:118.38997650146484--TestR2:-7.873997688293457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:404--TrainLoss:108.54869842529297--TrainR2:-6.131197929382324--TestLoss:118.36984252929688--TestR2:-7.872488021850586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:405--TrainLoss:108.52971649169922--TrainR2:-6.129951000213623--TestLoss:118.34972381591797--TestR2:-7.870980262756348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:406--TrainLoss:108.5107650756836--TrainR2:-6.1287055015563965--TestLoss:118.32958984375--TestR2:-7.869471549987793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:407--TrainLoss:108.49179077148438--TrainR2:-6.127459526062012--TestLoss:118.30949401855469--TestR2:-7.867964744567871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:408--TrainLoss:108.47283172607422--TrainR2:-6.126214027404785--TestLoss:118.28939056396484--TestR2:-7.866457939147949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:409--TrainLoss:108.4538803100586--TrainR2:-6.124968528747559--TestLoss:118.26925659179688--TestR2:-7.8649492263793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:410--TrainLoss:108.43490600585938--TrainR2:-6.123722553253174--TestLoss:118.24916076660156--TestR2:-7.863442420959473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:411--TrainLoss:108.41596221923828--TrainR2:-6.122478008270264--TestLoss:118.22905731201172--TestR2:-7.861935615539551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:412--TrainLoss:108.39701080322266--TrainR2:-6.121232509613037--TestLoss:118.2089614868164--TestR2:-7.860429763793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:413--TrainLoss:108.3780517578125--TrainR2:-6.1199870109558105--TestLoss:118.1888427734375--TestR2:-7.858921051025391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:414--TrainLoss:108.35909271240234--TrainR2:-6.118741989135742--TestLoss:118.16875457763672--TestR2:-7.857416152954102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:415--TrainLoss:108.34016418457031--TrainR2:-6.11749792098999--TestLoss:118.14865112304688--TestR2:-7.855908393859863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:416--TrainLoss:108.32122039794922--TrainR2:-6.11625337600708--TestLoss:118.12855529785156--TestR2:-7.854402542114258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:417--TrainLoss:108.30229949951172--TrainR2:-6.115010738372803--TestLoss:118.10845184326172--TestR2:-7.852895736694336\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:418--TrainLoss:108.28335571289062--TrainR2:-6.113766193389893--TestLoss:118.08836364746094--TestR2:-7.8513898849487305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:419--TrainLoss:108.2644271850586--TrainR2:-6.112522602081299--TestLoss:118.06828308105469--TestR2:-7.849884986877441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:420--TrainLoss:108.2454833984375--TrainR2:-6.111278057098389--TestLoss:118.04822540283203--TestR2:-7.848381996154785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:421--TrainLoss:108.22657775878906--TrainR2:-6.1100358963012695--TestLoss:118.02814483642578--TestR2:-7.84687614440918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:422--TrainLoss:108.20765686035156--TrainR2:-6.108792781829834--TestLoss:118.00806427001953--TestR2:-7.845371246337891\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:423--TrainLoss:108.18872833251953--TrainR2:-6.10754919052124--TestLoss:117.98798370361328--TestR2:-7.843865394592285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:424--TrainLoss:108.16980743408203--TrainR2:-6.106306552886963--TestLoss:117.96791076660156--TestR2:-7.8423614501953125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:425--TrainLoss:108.15088653564453--TrainR2:-6.105063438415527--TestLoss:117.94783782958984--TestR2:-7.84085750579834\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:426--TrainLoss:108.13197326660156--TrainR2:-6.10382080078125--TestLoss:117.92779541015625--TestR2:-7.839354515075684\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:427--TrainLoss:108.11307525634766--TrainR2:-6.102579593658447--TestLoss:117.90773010253906--TestR2:-7.837850570678711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:428--TrainLoss:108.09416198730469--TrainR2:-6.101336479187012--TestLoss:117.8876724243164--TestR2:-7.836346626281738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:429--TrainLoss:108.07527923583984--TrainR2:-6.100096225738525--TestLoss:117.86759185791016--TestR2:-7.834841728210449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:430--TrainLoss:108.05638885498047--TrainR2:-6.098855018615723--TestLoss:117.84757232666016--TestR2:-7.833340644836426\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:431--TrainLoss:108.03748321533203--TrainR2:-6.0976128578186035--TestLoss:117.82752227783203--TestR2:-7.831838607788086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:432--TrainLoss:108.01858520507812--TrainR2:-6.096371650695801--TestLoss:117.80746459960938--TestR2:-7.83033561706543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:433--TrainLoss:107.99967956542969--TrainR2:-6.09512996673584--TestLoss:117.78743743896484--TestR2:-7.82883358001709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:434--TrainLoss:107.9808120727539--TrainR2:-6.093890190124512--TestLoss:117.76738739013672--TestR2:-7.82733154296875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:435--TrainLoss:107.96190643310547--TrainR2:-6.092648506164551--TestLoss:117.74734497070312--TestR2:-7.825828552246094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:436--TrainLoss:107.9430160522461--TrainR2:-6.091407299041748--TestLoss:117.72728729248047--TestR2:-7.8243255615234375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:437--TrainLoss:107.92411804199219--TrainR2:-6.090166091918945--TestLoss:117.70726013183594--TestR2:-7.822823524475098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:438--TrainLoss:107.9052505493164--TrainR2:-6.088926315307617--TestLoss:117.68722534179688--TestR2:-7.821322441101074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:439--TrainLoss:107.88638305664062--TrainR2:-6.087687015533447--TestLoss:117.66720581054688--TestR2:-7.819821357727051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:440--TrainLoss:107.86750793457031--TrainR2:-6.086446762084961--TestLoss:117.64720153808594--TestR2:-7.81832218170166\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:441--TrainLoss:107.8486557006836--TrainR2:-6.085207939147949--TestLoss:117.62716674804688--TestR2:-7.816821098327637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:442--TrainLoss:107.82977294921875--TrainR2:-6.083967685699463--TestLoss:117.60714721679688--TestR2:-7.815320014953613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:443--TrainLoss:107.81090545654297--TrainR2:-6.082727909088135--TestLoss:117.58714294433594--TestR2:-7.813820838928223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:444--TrainLoss:107.79205322265625--TrainR2:-6.081489562988281--TestLoss:117.56712341308594--TestR2:-7.812319755554199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:445--TrainLoss:107.773193359375--TrainR2:-6.0802507400512695--TestLoss:117.54710388183594--TestR2:-7.810819625854492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:446--TrainLoss:107.75434112548828--TrainR2:-6.079011917114258--TestLoss:117.52709197998047--TestR2:-7.809319496154785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:447--TrainLoss:107.73548889160156--TrainR2:-6.077773094177246--TestLoss:117.50709533691406--TestR2:-7.807821273803711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:448--TrainLoss:107.71662139892578--TrainR2:-6.076533794403076--TestLoss:117.48710632324219--TestR2:-7.80632209777832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:449--TrainLoss:107.6977767944336--TrainR2:-6.075295925140381--TestLoss:117.46707153320312--TestR2:-7.804821014404297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:450--TrainLoss:107.67894744873047--TrainR2:-6.074059009552002--TestLoss:117.44710540771484--TestR2:-7.803323745727539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:451--TrainLoss:107.66011810302734--TrainR2:-6.072822093963623--TestLoss:117.42709350585938--TestR2:-7.801824569702148\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:452--TrainLoss:107.64126586914062--TrainR2:-6.071583271026611--TestLoss:117.40711212158203--TestR2:-7.800326347351074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:453--TrainLoss:107.62242126464844--TrainR2:-6.070345401763916--TestLoss:117.38712310791016--TestR2:-7.798828125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:454--TrainLoss:107.60356903076172--TrainR2:-6.069106578826904--TestLoss:117.36713409423828--TestR2:-7.797329902648926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:455--TrainLoss:107.58470916748047--TrainR2:-6.067868232727051--TestLoss:117.34715270996094--TestR2:-7.795832633972168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:456--TrainLoss:107.5658950805664--TrainR2:-6.06663179397583--TestLoss:117.3271713256836--TestR2:-7.794334411621094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:457--TrainLoss:107.5471420288086--TrainR2:-6.065400123596191--TestLoss:117.30720520019531--TestR2:-7.792838096618652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:458--TrainLoss:107.5282974243164--TrainR2:-6.064162254333496--TestLoss:117.28722381591797--TestR2:-7.791339874267578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:459--TrainLoss:107.50946044921875--TrainR2:-6.062924385070801--TestLoss:117.26724243164062--TestR2:-7.78984260559082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:460--TrainLoss:107.49060821533203--TrainR2:-6.061685562133789--TestLoss:117.24729919433594--TestR2:-7.788347244262695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:461--TrainLoss:107.47176361083984--TrainR2:-6.060447692871094--TestLoss:117.22732543945312--TestR2:-7.7868499755859375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:462--TrainLoss:107.4530258178711--TrainR2:-6.059216499328613--TestLoss:117.20735931396484--TestR2:-7.785353660583496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:463--TrainLoss:107.43424224853516--TrainR2:-6.057982921600342--TestLoss:117.1874008178711--TestR2:-7.783857345581055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:464--TrainLoss:107.41541290283203--TrainR2:-6.056746006011963--TestLoss:117.16744232177734--TestR2:-7.78236198425293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:465--TrainLoss:107.3965835571289--TrainR2:-6.055509090423584--TestLoss:117.14747619628906--TestR2:-7.780864715576172\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:466--TrainLoss:107.37775421142578--TrainR2:-6.054272174835205--TestLoss:117.1275405883789--TestR2:-7.77937126159668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:467--TrainLoss:107.35896301269531--TrainR2:-6.053037643432617--TestLoss:117.10759735107422--TestR2:-7.777875900268555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:468--TrainLoss:107.34022521972656--TrainR2:-6.051806449890137--TestLoss:117.08763885498047--TestR2:-7.77638053894043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:469--TrainLoss:107.3214111328125--TrainR2:-6.050570487976074--TestLoss:117.06769561767578--TestR2:-7.774885177612305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:470--TrainLoss:107.30260467529297--TrainR2:-6.04933500289917--TestLoss:117.0477523803711--TestR2:-7.77338981628418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:471--TrainLoss:107.2837905883789--TrainR2:-6.048099040985107--TestLoss:117.02783203125--TestR2:-7.771897315979004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:472--TrainLoss:107.26502227783203--TrainR2:-6.046865940093994--TestLoss:117.00788879394531--TestR2:-7.770401954650879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:473--TrainLoss:107.24626922607422--TrainR2:-6.0456342697143555--TestLoss:116.98795318603516--TestR2:-7.768908500671387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:474--TrainLoss:107.22747039794922--TrainR2:-6.044398784637451--TestLoss:116.96800994873047--TestR2:-7.767413139343262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:475--TrainLoss:107.20867919921875--TrainR2:-6.043164253234863--TestLoss:116.94808197021484--TestR2:-7.7659196853637695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:476--TrainLoss:107.18991088867188--TrainR2:-6.041931629180908--TestLoss:116.92816925048828--TestR2:-7.764427185058594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:477--TrainLoss:107.17115783691406--TrainR2:-6.040699481964111--TestLoss:116.90824127197266--TestR2:-7.762932777404785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:478--TrainLoss:107.15239715576172--TrainR2:-6.039466857910156--TestLoss:116.88834381103516--TestR2:-7.761442184448242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:479--TrainLoss:107.13360595703125--TrainR2:-6.038232326507568--TestLoss:116.86842346191406--TestR2:-7.75994873046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:480--TrainLoss:107.11483764648438--TrainR2:-6.036999225616455--TestLoss:116.84849548339844--TestR2:-7.758455276489258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:481--TrainLoss:107.09608459472656--TrainR2:-6.035767555236816--TestLoss:116.82859802246094--TestR2:-7.756963729858398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:482--TrainLoss:107.07734680175781--TrainR2:-6.034536361694336--TestLoss:116.80867767333984--TestR2:-7.755470275878906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:483--TrainLoss:107.05860137939453--TrainR2:-6.033304691314697--TestLoss:116.78880310058594--TestR2:-7.75398063659668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:484--TrainLoss:107.03981018066406--TrainR2:-6.032070159912109--TestLoss:116.76888275146484--TestR2:-7.7524871826171875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:485--TrainLoss:107.02107238769531--TrainR2:-6.030838966369629--TestLoss:116.74897003173828--TestR2:-7.750994682312012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:486--TrainLoss:107.00232696533203--TrainR2:-6.029607772827148--TestLoss:116.72908020019531--TestR2:-7.749504089355469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:487--TrainLoss:106.98359680175781--TrainR2:-6.028377532958984--TestLoss:116.70919799804688--TestR2:-7.748013496398926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:488--TrainLoss:106.96483612060547--TrainR2:-6.027144908905029--TestLoss:116.68930053710938--TestR2:-7.746522903442383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:489--TrainLoss:106.94609832763672--TrainR2:-6.025913715362549--TestLoss:116.6694107055664--TestR2:-7.745031356811523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:490--TrainLoss:106.9273681640625--TrainR2:-6.024683475494385--TestLoss:116.64952850341797--TestR2:-7.7435407638549805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:491--TrainLoss:106.90863800048828--TrainR2:-6.0234527587890625--TestLoss:116.62964630126953--TestR2:-7.742051124572754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:492--TrainLoss:106.88993072509766--TrainR2:-6.022223949432373--TestLoss:116.60978698730469--TestR2:-7.740562438964844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:493--TrainLoss:106.87117767333984--TrainR2:-6.020991802215576--TestLoss:116.58990478515625--TestR2:-7.739071846008301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:494--TrainLoss:106.85246276855469--TrainR2:-6.0197625160217285--TestLoss:116.57003784179688--TestR2:-7.737583160400391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:495--TrainLoss:106.833740234375--TrainR2:-6.018532752990723--TestLoss:116.55015563964844--TestR2:-7.736092567443848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:496--TrainLoss:106.81502532958984--TrainR2:-6.017302989959717--TestLoss:116.5302963256836--TestR2:-7.7346038818359375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:497--TrainLoss:106.79627990722656--TrainR2:-6.016071319580078--TestLoss:116.51041412353516--TestR2:-7.733114242553711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:498--TrainLoss:106.77758026123047--TrainR2:-6.014842987060547--TestLoss:116.49054718017578--TestR2:-7.731624603271484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:499--TrainLoss:106.75889587402344--TrainR2:-6.013615608215332--TestLoss:116.47068786621094--TestR2:-7.730135917663574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:500--TrainLoss:106.74015808105469--TrainR2:-6.012384414672852--TestLoss:116.45084381103516--TestR2:-7.728649139404297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:501--TrainLoss:106.72146606445312--TrainR2:-6.0111565589904785--TestLoss:116.43099212646484--TestR2:-7.727160453796387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:502--TrainLoss:106.70276641845703--TrainR2:-6.009928226470947--TestLoss:116.41114807128906--TestR2:-7.725672721862793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:503--TrainLoss:106.6840591430664--TrainR2:-6.0086989402771--TestLoss:116.39130401611328--TestR2:-7.724185943603516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:504--TrainLoss:106.66534423828125--TrainR2:-6.007469654083252--TestLoss:116.37144470214844--TestR2:-7.7226972579956055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:505--TrainLoss:106.64665985107422--TrainR2:-6.006242275238037--TestLoss:116.3515853881836--TestR2:-7.721208572387695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:506--TrainLoss:106.62799072265625--TrainR2:-6.0050153732299805--TestLoss:116.33173370361328--TestR2:-7.719720840454102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:507--TrainLoss:106.6092758178711--TrainR2:-6.003785610198975--TestLoss:116.31193542480469--TestR2:-7.718236923217773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:508--TrainLoss:106.59060668945312--TrainR2:-6.002559185028076--TestLoss:116.2920913696289--TestR2:-7.71674919128418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:509--TrainLoss:106.57192993164062--TrainR2:-6.001332759857178--TestLoss:116.27226257324219--TestR2:-7.715262413024902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:510--TrainLoss:106.55322265625--TrainR2:-6.000103950500488--TestLoss:116.2524185180664--TestR2:-7.713775634765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:511--TrainLoss:106.53455352783203--TrainR2:-5.998877048492432--TestLoss:116.23258209228516--TestR2:-7.712288856506348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:512--TrainLoss:106.51587677001953--TrainR2:-5.997650146484375--TestLoss:116.2127914428711--TestR2:-7.7108049392700195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:513--TrainLoss:106.49720764160156--TrainR2:-5.996423721313477--TestLoss:116.19294738769531--TestR2:-7.709318161010742\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:514--TrainLoss:106.4785385131836--TrainR2:-5.99519681930542--TestLoss:116.17314147949219--TestR2:-7.707833290100098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:515--TrainLoss:106.4598617553711--TrainR2:-5.9939703941345215--TestLoss:116.15333557128906--TestR2:-7.706348419189453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:516--TrainLoss:106.44121551513672--TrainR2:-5.9927449226379395--TestLoss:116.13351440429688--TestR2:-7.704862594604492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:517--TrainLoss:106.42256164550781--TrainR2:-5.991519451141357--TestLoss:116.11369323730469--TestR2:-7.703377723693848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:518--TrainLoss:106.40386962890625--TrainR2:-5.990291595458984--TestLoss:116.09391021728516--TestR2:-7.701894760131836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:519--TrainLoss:106.3852310180664--TrainR2:-5.989067077636719--TestLoss:116.07408142089844--TestR2:-7.700407981872559\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:520--TrainLoss:106.36658477783203--TrainR2:-5.987842082977295--TestLoss:116.05427551269531--TestR2:-7.698923110961914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:521--TrainLoss:106.34790802001953--TrainR2:-5.9866156578063965--TestLoss:116.03450775146484--TestR2:-7.697442054748535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:522--TrainLoss:106.32927703857422--TrainR2:-5.985391139984131--TestLoss:116.01468658447266--TestR2:-7.695956230163574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:523--TrainLoss:106.3106460571289--TrainR2:-5.984167098999023--TestLoss:115.99488830566406--TestR2:-7.694472312927246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:524--TrainLoss:106.29198455810547--TrainR2:-5.982941627502441--TestLoss:115.9751205444336--TestR2:-7.692990303039551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:525--TrainLoss:106.2733383178711--TrainR2:-5.981716632843018--TestLoss:115.95531463623047--TestR2:-7.691506385803223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:526--TrainLoss:106.25470733642578--TrainR2:-5.98049259185791--TestLoss:115.935546875--TestR2:-7.690024375915527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:527--TrainLoss:106.23609161376953--TrainR2:-5.979269504547119--TestLoss:115.91575622558594--TestR2:-7.688540458679199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:528--TrainLoss:106.21743774414062--TrainR2:-5.978043556213379--TestLoss:115.89598083496094--TestR2:-7.687058448791504\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:529--TrainLoss:106.19879913330078--TrainR2:-5.9768195152282715--TestLoss:115.87619018554688--TestR2:-7.685575485229492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:530--TrainLoss:106.18019104003906--TrainR2:-5.975596904754639--TestLoss:115.85643005371094--TestR2:-7.684094429016113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:531--TrainLoss:106.16154479980469--TrainR2:-5.974372386932373--TestLoss:115.83667755126953--TestR2:-7.682613372802734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:532--TrainLoss:106.1429443359375--TrainR2:-5.97314977645874--TestLoss:115.81690979003906--TestR2:-7.681131362915039\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:533--TrainLoss:106.12432861328125--TrainR2:-5.971927165985107--TestLoss:115.79711151123047--TestR2:-7.679647445678711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:534--TrainLoss:106.10572052001953--TrainR2:-5.970704555511475--TestLoss:115.77735900878906--TestR2:-7.678167343139648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:535--TrainLoss:106.08709716796875--TrainR2:-5.969480991363525--TestLoss:115.75761413574219--TestR2:-7.676687240600586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:536--TrainLoss:106.06846618652344--TrainR2:-5.968257427215576--TestLoss:115.73785400390625--TestR2:-7.675206184387207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:537--TrainLoss:106.04986572265625--TrainR2:-5.967035293579102--TestLoss:115.71807098388672--TestR2:-7.673723220825195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:538--TrainLoss:106.03128814697266--TrainR2:-5.965814590454102--TestLoss:115.6983413696289--TestR2:-7.672244071960449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:539--TrainLoss:106.01264190673828--TrainR2:-5.964589595794678--TestLoss:115.6785888671875--TestR2:-7.670763969421387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:540--TrainLoss:105.99407196044922--TrainR2:-5.963369369506836--TestLoss:115.65885162353516--TestR2:-7.669284820556641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:541--TrainLoss:105.9754638671875--TrainR2:-5.962147235870361--TestLoss:115.63910675048828--TestR2:-7.667804718017578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:542--TrainLoss:105.9568862915039--TrainR2:-5.960926532745361--TestLoss:115.61936950683594--TestR2:-7.666324615478516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:543--TrainLoss:105.93830108642578--TrainR2:-5.959705352783203--TestLoss:115.5996322631836--TestR2:-7.6648454666137695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:544--TrainLoss:105.91968536376953--TrainR2:-5.95848274230957--TestLoss:115.57988739013672--TestR2:-7.663365364074707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:545--TrainLoss:105.90111541748047--TrainR2:-5.9572625160217285--TestLoss:115.56017303466797--TestR2:-7.661888122558594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:546--TrainLoss:105.88252258300781--TrainR2:-5.95604133605957--TestLoss:115.5404281616211--TestR2:-7.660408020019531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:547--TrainLoss:105.86395263671875--TrainR2:-5.9548211097717285--TestLoss:115.52069854736328--TestR2:-7.658928871154785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:548--TrainLoss:105.84539031982422--TrainR2:-5.953601837158203--TestLoss:115.50099182128906--TestR2:-7.657451629638672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:549--TrainLoss:105.82677459716797--TrainR2:-5.95237922668457--TestLoss:115.48125457763672--TestR2:-7.655972480773926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:550--TrainLoss:105.80824279785156--TrainR2:-5.9511613845825195--TestLoss:115.46154022216797--TestR2:-7.654494285583496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:551--TrainLoss:105.7896499633789--TrainR2:-5.949940204620361--TestLoss:115.44183349609375--TestR2:-7.653017044067383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:552--TrainLoss:105.77108764648438--TrainR2:-5.948720455169678--TestLoss:115.42212677001953--TestR2:-7.651540756225586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:553--TrainLoss:105.75253295898438--TrainR2:-5.9475016593933105--TestLoss:115.40239715576172--TestR2:-7.65006160736084\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:554--TrainLoss:105.73396301269531--TrainR2:-5.946281909942627--TestLoss:115.3826904296875--TestR2:-7.648584365844727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:555--TrainLoss:105.71542358398438--TrainR2:-5.945063591003418--TestLoss:115.36299133300781--TestR2:-7.64710807800293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:556--TrainLoss:105.69683074951172--TrainR2:-5.94384241104126--TestLoss:115.34329986572266--TestR2:-7.645631790161133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:557--TrainLoss:105.67829895019531--TrainR2:-5.942625045776367--TestLoss:115.32357025146484--TestR2:-7.644152641296387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:558--TrainLoss:105.65973663330078--TrainR2:-5.941405296325684--TestLoss:115.30387115478516--TestR2:-7.64267635345459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:559--TrainLoss:105.64122009277344--TrainR2:-5.940188884735107--TestLoss:115.28419494628906--TestR2:-7.641201972961426\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:560--TrainLoss:105.6226577758789--TrainR2:-5.938969135284424--TestLoss:115.26451110839844--TestR2:-7.639726638793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:561--TrainLoss:105.60411834716797--TrainR2:-5.937751770019531--TestLoss:115.24482727050781--TestR2:-7.638251304626465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:562--TrainLoss:105.58557891845703--TrainR2:-5.936533451080322--TestLoss:115.22513580322266--TestR2:-7.636775016784668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:563--TrainLoss:105.56703186035156--TrainR2:-5.935315132141113--TestLoss:115.2054443359375--TestR2:-7.635298728942871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:564--TrainLoss:105.54849243164062--TrainR2:-5.9340972900390625--TestLoss:115.18576049804688--TestR2:-7.633823394775391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:565--TrainLoss:105.52998352050781--TrainR2:-5.9328813552856445--TestLoss:115.16609191894531--TestR2:-7.632349014282227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:566--TrainLoss:105.5114517211914--TrainR2:-5.931663513183594--TestLoss:115.14640808105469--TestR2:-7.630873680114746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:567--TrainLoss:105.4928970336914--TrainR2:-5.930444717407227--TestLoss:115.12675476074219--TestR2:-7.629400253295898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:568--TrainLoss:105.47439575195312--TrainR2:-5.929229259490967--TestLoss:115.10709381103516--TestR2:-7.627926826477051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:569--TrainLoss:105.45585632324219--TrainR2:-5.928011417388916--TestLoss:115.08740234375--TestR2:-7.626450538635254\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:570--TrainLoss:105.43736267089844--TrainR2:-5.9267964363098145--TestLoss:115.0677490234375--TestR2:-7.624978065490723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:571--TrainLoss:105.4188461303711--TrainR2:-5.92557954788208--TestLoss:115.04810333251953--TestR2:-7.623505592346191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:572--TrainLoss:105.40032958984375--TrainR2:-5.924363613128662--TestLoss:115.0284423828125--TestR2:-7.622032165527344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:573--TrainLoss:105.3818130493164--TrainR2:-5.923147201538086--TestLoss:115.0087890625--TestR2:-7.620558738708496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:574--TrainLoss:105.36331939697266--TrainR2:-5.921931743621826--TestLoss:114.98912048339844--TestR2:-7.619084358215332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:575--TrainLoss:105.34481048583984--TrainR2:-5.920715808868408--TestLoss:114.96946716308594--TestR2:-7.617610931396484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:576--TrainLoss:105.3263168334961--TrainR2:-5.919501304626465--TestLoss:114.94982147216797--TestR2:-7.616138458251953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:577--TrainLoss:105.30777740478516--TrainR2:-5.918282985687256--TestLoss:114.93017578125--TestR2:-7.614665985107422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:578--TrainLoss:105.28931427001953--TrainR2:-5.917069911956787--TestLoss:114.9105453491211--TestR2:-7.613194465637207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:579--TrainLoss:105.27079772949219--TrainR2:-5.915853500366211--TestLoss:114.89089965820312--TestR2:-7.611721992492676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:580--TrainLoss:105.25233459472656--TrainR2:-5.9146409034729--TestLoss:114.87125396728516--TestR2:-7.6102495193481445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:581--TrainLoss:105.23381042480469--TrainR2:-5.913424015045166--TestLoss:114.85163879394531--TestR2:-7.608778953552246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:582--TrainLoss:105.21537017822266--TrainR2:-5.912211894989014--TestLoss:114.83199310302734--TestR2:-7.607306480407715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:583--TrainLoss:105.19682312011719--TrainR2:-5.910994052886963--TestLoss:114.8123779296875--TestR2:-7.605835914611816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:584--TrainLoss:105.17840576171875--TrainR2:-5.909783840179443--TestLoss:114.79273986816406--TestR2:-7.604364395141602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:585--TrainLoss:105.15987396240234--TrainR2:-5.908566474914551--TestLoss:114.77313995361328--TestR2:-7.6028947830200195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:586--TrainLoss:105.14144134521484--TrainR2:-5.907355308532715--TestLoss:114.75350189208984--TestR2:-7.601423263549805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:587--TrainLoss:105.12290954589844--TrainR2:-5.906137943267822--TestLoss:114.73390197753906--TestR2:-7.599953651428223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:588--TrainLoss:105.1045150756836--TrainR2:-5.904929161071777--TestLoss:114.71427917480469--TestR2:-7.598483085632324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:589--TrainLoss:105.08597564697266--TrainR2:-5.903711318969727--TestLoss:114.69468688964844--TestR2:-7.597014427185059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:590--TrainLoss:105.06757354736328--TrainR2:-5.902502536773682--TestLoss:114.67506408691406--TestR2:-7.59554386138916\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:591--TrainLoss:105.04904174804688--TrainR2:-5.901285171508789--TestLoss:114.65547180175781--TestR2:-7.5940752029418945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:592--TrainLoss:105.0306396484375--TrainR2:-5.900075912475586--TestLoss:114.6358642578125--TestR2:-7.5926055908203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:593--TrainLoss:105.01212310791016--TrainR2:-5.89885950088501--TestLoss:114.61626434326172--TestR2:-7.5911359786987305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:594--TrainLoss:104.99372863769531--TrainR2:-5.897651195526123--TestLoss:114.59666442871094--TestR2:-7.589667320251465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:595--TrainLoss:104.97535705566406--TrainR2:-5.896444320678711--TestLoss:114.5770492553711--TestR2:-7.588196754455566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:596--TrainLoss:104.95681762695312--TrainR2:-5.895226001739502--TestLoss:114.55745697021484--TestR2:-7.586728096008301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:597--TrainLoss:104.93843841552734--TrainR2:-5.89401912689209--TestLoss:114.53788757324219--TestR2:-7.585261344909668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:598--TrainLoss:104.9198989868164--TrainR2:-5.892801284790039--TestLoss:114.51830291748047--TestR2:-7.583793640136719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:599--TrainLoss:104.90154266357422--TrainR2:-5.891594886779785--TestLoss:114.49873352050781--TestR2:-7.582326889038086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:600--TrainLoss:104.88302612304688--TrainR2:-5.890378952026367--TestLoss:114.47914123535156--TestR2:-7.58085823059082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:601--TrainLoss:104.86465454101562--TrainR2:-5.889171600341797--TestLoss:114.45954895019531--TestR2:-7.579389572143555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:602--TrainLoss:104.84615325927734--TrainR2:-5.887956619262695--TestLoss:114.43997955322266--TestR2:-7.577922821044922\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:603--TrainLoss:104.82775115966797--TrainR2:-5.886747360229492--TestLoss:114.42042541503906--TestR2:-7.5764570236206055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:604--TrainLoss:104.80935668945312--TrainR2:-5.8855390548706055--TestLoss:114.40082550048828--TestR2:-7.57498836517334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:605--TrainLoss:104.79090881347656--TrainR2:-5.884326934814453--TestLoss:114.38127136230469--TestR2:-7.573522567749023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:606--TrainLoss:104.77249908447266--TrainR2:-5.88311767578125--TestLoss:114.3617172241211--TestR2:-7.572056770324707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:607--TrainLoss:104.7540512084961--TrainR2:-5.881905555725098--TestLoss:114.34214782714844--TestR2:-7.570590019226074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:608--TrainLoss:104.73567199707031--TrainR2:-5.880698204040527--TestLoss:114.32259368896484--TestR2:-7.569124221801758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:609--TrainLoss:104.71722412109375--TrainR2:-5.879486083984375--TestLoss:114.30302429199219--TestR2:-7.567657470703125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:610--TrainLoss:104.69884490966797--TrainR2:-5.8782782554626465--TestLoss:114.28345489501953--TestR2:-7.566190719604492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:611--TrainLoss:104.6804428100586--TrainR2:-5.877069473266602--TestLoss:114.26392364501953--TestR2:-7.564726829528809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:612--TrainLoss:104.6620101928711--TrainR2:-5.875858783721924--TestLoss:114.24436950683594--TestR2:-7.563261032104492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:613--TrainLoss:104.64361572265625--TrainR2:-5.874650478363037--TestLoss:114.22483825683594--TestR2:-7.561797142028809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:614--TrainLoss:104.6252212524414--TrainR2:-5.87344217300415--TestLoss:114.20527648925781--TestR2:-7.560330390930176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:615--TrainLoss:104.6068115234375--TrainR2:-5.872232437133789--TestLoss:114.18574523925781--TestR2:-7.558866500854492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:616--TrainLoss:104.58841705322266--TrainR2:-5.871023654937744--TestLoss:114.16620635986328--TestR2:-7.557401657104492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:617--TrainLoss:104.57003021240234--TrainR2:-5.869816303253174--TestLoss:114.14667510986328--TestR2:-7.555937767028809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:618--TrainLoss:104.55165100097656--TrainR2:-5.868608474731445--TestLoss:114.12712860107422--TestR2:-7.554472923278809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:619--TrainLoss:104.53324890136719--TrainR2:-5.8673996925354--TestLoss:114.10762023925781--TestR2:-7.553010940551758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:620--TrainLoss:104.51486206054688--TrainR2:-5.866191387176514--TestLoss:114.08808898925781--TestR2:-7.551547050476074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:621--TrainLoss:104.49652099609375--TrainR2:-5.864986896514893--TestLoss:114.06857299804688--TestR2:-7.550084114074707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:622--TrainLoss:104.4781265258789--TrainR2:-5.863778591156006--TestLoss:114.04902648925781--TestR2:-7.548619270324707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:623--TrainLoss:104.45974731445312--TrainR2:-5.862570762634277--TestLoss:114.02952575683594--TestR2:-7.547157287597656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:624--TrainLoss:104.44136810302734--TrainR2:-5.861363410949707--TestLoss:114.01000213623047--TestR2:-7.545693397521973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:625--TrainLoss:104.42298126220703--TrainR2:-5.8601555824279785--TestLoss:113.99048614501953--TestR2:-7.5442304611206055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:626--TrainLoss:104.4046401977539--TrainR2:-5.858950614929199--TestLoss:113.97097778320312--TestR2:-7.542768478393555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:627--TrainLoss:104.38627624511719--TrainR2:-5.857744216918945--TestLoss:113.95148468017578--TestR2:-7.54130744934082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:628--TrainLoss:104.3678970336914--TrainR2:-5.856536865234375--TestLoss:113.93196868896484--TestR2:-7.539844512939453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:629--TrainLoss:104.34955596923828--TrainR2:-5.855331897735596--TestLoss:113.9124755859375--TestR2:-7.538383483886719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:630--TrainLoss:104.3311767578125--TrainR2:-5.854124546051025--TestLoss:113.8929672241211--TestR2:-7.536921501159668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:631--TrainLoss:104.31283569335938--TrainR2:-5.852919101715088--TestLoss:113.87348175048828--TestR2:-7.53546142578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:632--TrainLoss:104.29447174072266--TrainR2:-5.851713180541992--TestLoss:113.8539810180664--TestR2:-7.533998489379883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:633--TrainLoss:104.27611541748047--TrainR2:-5.8505072593688965--TestLoss:113.8344955444336--TestR2:-7.532538414001465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:634--TrainLoss:104.25779724121094--TrainR2:-5.849303722381592--TestLoss:113.81500244140625--TestR2:-7.5310773849487305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:635--TrainLoss:104.23942565917969--TrainR2:-5.8480963706970215--TestLoss:113.7955093383789--TestR2:-7.529616355895996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:636--TrainLoss:104.22108459472656--TrainR2:-5.8468918800354--TestLoss:113.77603149414062--TestR2:-7.528156280517578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:637--TrainLoss:104.20278930664062--TrainR2:-5.84568977355957--TestLoss:113.75656127929688--TestR2:-7.526697158813477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:638--TrainLoss:104.18440246582031--TrainR2:-5.844481945037842--TestLoss:113.73706817626953--TestR2:-7.525236129760742\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:639--TrainLoss:104.16609191894531--TrainR2:-5.843278884887695--TestLoss:113.71762084960938--TestR2:-7.523777961730957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:640--TrainLoss:104.14778900146484--TrainR2:-5.842076301574707--TestLoss:113.69812774658203--TestR2:-7.522316932678223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:641--TrainLoss:104.1294174194336--TrainR2:-5.840869426727295--TestLoss:113.67864990234375--TestR2:-7.520856857299805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:642--TrainLoss:104.11111450195312--TrainR2:-5.839667320251465--TestLoss:113.65919494628906--TestR2:-7.5193986892700195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:643--TrainLoss:104.09278106689453--TrainR2:-5.838462829589844--TestLoss:113.63973236083984--TestR2:-7.517940521240234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:644--TrainLoss:104.07444763183594--TrainR2:-5.837258338928223--TestLoss:113.62027740478516--TestR2:-7.516482353210449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:645--TrainLoss:104.05615997314453--TrainR2:-5.836057186126709--TestLoss:113.60082244873047--TestR2:-7.515023231506348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:646--TrainLoss:104.03783416748047--TrainR2:-5.834852695465088--TestLoss:113.58132934570312--TestR2:-7.513562202453613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:647--TrainLoss:104.0195083618164--TrainR2:-5.833649158477783--TestLoss:113.5619125366211--TestR2:-7.512106895446777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:648--TrainLoss:104.00121307373047--TrainR2:-5.832447052001953--TestLoss:113.5424575805664--TestR2:-7.510648727416992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:649--TrainLoss:103.98290252685547--TrainR2:-5.831243991851807--TestLoss:113.52301025390625--TestR2:-7.509191513061523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:650--TrainLoss:103.96458435058594--TrainR2:-5.830040454864502--TestLoss:113.50355529785156--TestR2:-7.507732391357422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:651--TrainLoss:103.9463119506836--TrainR2:-5.828840255737305--TestLoss:113.48411560058594--TestR2:-7.506275177001953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:652--TrainLoss:103.92799377441406--TrainR2:-5.827637195587158--TestLoss:113.46469116210938--TestR2:-7.504819869995117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:653--TrainLoss:103.90967559814453--TrainR2:-5.8264336585998535--TestLoss:113.44523620605469--TestR2:-7.503361701965332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:654--TrainLoss:103.89142608642578--TrainR2:-5.825234413146973--TestLoss:113.4258041381836--TestR2:-7.50190544128418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:655--TrainLoss:103.87310791015625--TrainR2:-5.824031352996826--TestLoss:113.4063491821289--TestR2:-7.500446319580078\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:656--TrainLoss:103.8548583984375--TrainR2:-5.822832107543945--TestLoss:113.38694763183594--TestR2:-7.498991966247559\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:657--TrainLoss:103.83654022216797--TrainR2:-5.821629047393799--TestLoss:113.36751556396484--TestR2:-7.497535705566406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:658--TrainLoss:103.81826782226562--TrainR2:-5.820428371429443--TestLoss:113.34810638427734--TestR2:-7.496081352233887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:659--TrainLoss:103.80000305175781--TrainR2:-5.819228649139404--TestLoss:113.32866668701172--TestR2:-7.494623184204102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:660--TrainLoss:103.78170013427734--TrainR2:-5.818026065826416--TestLoss:113.30924224853516--TestR2:-7.493167877197266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:661--TrainLoss:103.763427734375--TrainR2:-5.816825866699219--TestLoss:113.28984069824219--TestR2:-7.491713523864746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:662--TrainLoss:103.74517059326172--TrainR2:-5.81562614440918--TestLoss:113.27042388916016--TestR2:-7.49025821685791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:663--TrainLoss:103.72691345214844--TrainR2:-5.814426898956299--TestLoss:113.25101470947266--TestR2:-7.488802909851074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:664--TrainLoss:103.7085952758789--TrainR2:-5.813223361968994--TestLoss:113.23160552978516--TestR2:-7.487348556518555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:665--TrainLoss:103.69036102294922--TrainR2:-5.812025547027588--TestLoss:113.21220397949219--TestR2:-7.485894203186035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:666--TrainLoss:103.67211151123047--TrainR2:-5.810826301574707--TestLoss:113.19281005859375--TestR2:-7.484440803527832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:667--TrainLoss:103.6538314819336--TrainR2:-5.809625625610352--TestLoss:113.17341613769531--TestR2:-7.4829864501953125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:668--TrainLoss:103.63558959960938--TrainR2:-5.808427333831787--TestLoss:113.15402221679688--TestR2:-7.481533050537109\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:669--TrainLoss:103.61734008789062--TrainR2:-5.8072285652160645--TestLoss:113.13461303710938--TestR2:-7.48007869720459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:670--TrainLoss:103.59911346435547--TrainR2:-5.806030750274658--TestLoss:113.11524963378906--TestR2:-7.4786272048950195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:671--TrainLoss:103.58082580566406--TrainR2:-5.8048295974731445--TestLoss:113.0958251953125--TestR2:-7.477170944213867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:672--TrainLoss:103.56257629394531--TrainR2:-5.803630352020264--TestLoss:113.07644653320312--TestR2:-7.4757184982299805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:673--TrainLoss:103.54434204101562--TrainR2:-5.802432537078857--TestLoss:113.05706787109375--TestR2:-7.474266052246094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:674--TrainLoss:103.52610778808594--TrainR2:-5.801234722137451--TestLoss:113.0376968383789--TestR2:-7.472813606262207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:675--TrainLoss:103.50786590576172--TrainR2:-5.8000359535217285--TestLoss:113.01831817626953--TestR2:-7.47136116027832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:676--TrainLoss:103.48963165283203--TrainR2:-5.798838138580322--TestLoss:112.99893951416016--TestR2:-7.469908714294434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:677--TrainLoss:103.47140502929688--TrainR2:-5.797640800476074--TestLoss:112.97956848144531--TestR2:-7.468457221984863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:678--TrainLoss:103.45315551757812--TrainR2:-5.796442031860352--TestLoss:112.96018981933594--TestR2:-7.46700382232666\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:679--TrainLoss:103.43492126464844--TrainR2:-5.795244216918945--TestLoss:112.94084167480469--TestR2:-7.465554237365723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:680--TrainLoss:103.41670989990234--TrainR2:-5.794047832489014--TestLoss:112.92146301269531--TestR2:-7.464101791381836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:681--TrainLoss:103.39851379394531--TrainR2:-5.792852401733398--TestLoss:112.90210723876953--TestR2:-7.462650299072266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:682--TrainLoss:103.38029479980469--TrainR2:-5.791655540466309--TestLoss:112.88273620605469--TestR2:-7.461198806762695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:683--TrainLoss:103.362060546875--TrainR2:-5.790457725524902--TestLoss:112.86339569091797--TestR2:-7.459749221801758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:684--TrainLoss:103.34383392333984--TrainR2:-5.789259910583496--TestLoss:112.84403991699219--TestR2:-7.4582977294921875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:685--TrainLoss:103.32563018798828--TrainR2:-5.788064002990723--TestLoss:112.8246841430664--TestR2:-7.456847190856934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:686--TrainLoss:103.30744171142578--TrainR2:-5.786869049072266--TestLoss:112.80533599853516--TestR2:-7.455397605895996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:687--TrainLoss:103.28923797607422--TrainR2:-5.78567361831665--TestLoss:112.7860107421875--TestR2:-7.453948020935059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:688--TrainLoss:103.27101135253906--TrainR2:-5.784475803375244--TestLoss:112.76665496826172--TestR2:-7.452497482299805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:689--TrainLoss:103.25281524658203--TrainR2:-5.783280372619629--TestLoss:112.74730682373047--TestR2:-7.451047897338867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:690--TrainLoss:103.23461151123047--TrainR2:-5.7820844650268555--TestLoss:112.72798919677734--TestR2:-7.449599266052246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:691--TrainLoss:103.21644592285156--TrainR2:-5.780891418457031--TestLoss:112.70863342285156--TestR2:-7.448148727416992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:692--TrainLoss:103.19828033447266--TrainR2:-5.779697895050049--TestLoss:112.68931579589844--TestR2:-7.4467010498046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:693--TrainLoss:103.18000793457031--TrainR2:-5.778497219085693--TestLoss:112.66998291015625--TestR2:-7.44525146484375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:694--TrainLoss:103.16182708740234--TrainR2:-5.777303218841553--TestLoss:112.65064239501953--TestR2:-7.4438018798828125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:695--TrainLoss:103.14366912841797--TrainR2:-5.7761101722717285--TestLoss:112.63133239746094--TestR2:-7.442355155944824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:696--TrainLoss:103.1255111694336--TrainR2:-5.774917125701904--TestLoss:112.61201477050781--TestR2:-7.440906524658203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:697--TrainLoss:103.10736083984375--TrainR2:-5.7737250328063965--TestLoss:112.59269714355469--TestR2:-7.439458847045898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:698--TrainLoss:103.08911895751953--TrainR2:-5.772526741027832--TestLoss:112.5733642578125--TestR2:-7.438010215759277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:699--TrainLoss:103.07093811035156--TrainR2:-5.771331787109375--TestLoss:112.5540771484375--TestR2:-7.436563491821289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:700--TrainLoss:103.05278778076172--TrainR2:-5.770139217376709--TestLoss:112.53475189208984--TestR2:-7.435115814208984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:701--TrainLoss:103.03462982177734--TrainR2:-5.768946647644043--TestLoss:112.51543426513672--TestR2:-7.43366813659668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:702--TrainLoss:103.0164566040039--TrainR2:-5.767752647399902--TestLoss:112.49614715576172--TestR2:-7.432221412658691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:703--TrainLoss:102.99829864501953--TrainR2:-5.766560077667236--TestLoss:112.47682189941406--TestR2:-7.430773735046387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:704--TrainLoss:102.9801254272461--TrainR2:-5.765366077423096--TestLoss:112.4575424194336--TestR2:-7.429327964782715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:705--TrainLoss:102.96197509765625--TrainR2:-5.76417350769043--TestLoss:112.43824005126953--TestR2:-7.427881240844727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:706--TrainLoss:102.9438247680664--TrainR2:-5.762981414794922--TestLoss:112.4189682006836--TestR2:-7.4264373779296875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:707--TrainLoss:102.9256591796875--TrainR2:-5.7617878913879395--TestLoss:112.39965057373047--TestR2:-7.424988746643066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:708--TrainLoss:102.90751647949219--TrainR2:-5.760595798492432--TestLoss:112.38035583496094--TestR2:-7.4235429763793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:709--TrainLoss:102.8893814086914--TrainR2:-5.759404182434082--TestLoss:112.36109161376953--TestR2:-7.422098159790039\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:710--TrainLoss:102.87122344970703--TrainR2:-5.758211612701416--TestLoss:112.341796875--TestR2:-7.420652389526367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:711--TrainLoss:102.85308837890625--TrainR2:-5.757019996643066--TestLoss:112.32251739501953--TestR2:-7.419207572937012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:712--TrainLoss:102.8349380493164--TrainR2:-5.755827903747559--TestLoss:112.30322265625--TestR2:-7.417760848999023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:713--TrainLoss:102.81681823730469--TrainR2:-5.754637241363525--TestLoss:112.28396606445312--TestR2:-7.416317939758301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:714--TrainLoss:102.79866027832031--TrainR2:-5.753444671630859--TestLoss:112.26470184326172--TestR2:-7.414873123168945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:715--TrainLoss:102.7805404663086--TrainR2:-5.752254009246826--TestLoss:112.24542999267578--TestR2:-7.413429260253906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:716--TrainLoss:102.76244354248047--TrainR2:-5.751065254211426--TestLoss:112.22615814208984--TestR2:-7.411984443664551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:717--TrainLoss:102.74425506591797--TrainR2:-5.749870300292969--TestLoss:112.20689392089844--TestR2:-7.410540580749512\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:718--TrainLoss:102.72615051269531--TrainR2:-5.748680591583252--TestLoss:112.1876220703125--TestR2:-7.409096717834473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:719--TrainLoss:102.7080307006836--TrainR2:-5.747490406036377--TestLoss:112.16838073730469--TestR2:-7.40765380859375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:720--TrainLoss:102.68991088867188--TrainR2:-5.746300220489502--TestLoss:112.14912414550781--TestR2:-7.406210899353027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:721--TrainLoss:102.67181396484375--TrainR2:-5.745110988616943--TestLoss:112.12984466552734--TestR2:-7.4047651290893555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:722--TrainLoss:102.65370178222656--TrainR2:-5.743921279907227--TestLoss:112.11060333251953--TestR2:-7.403323173522949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:723--TrainLoss:102.63558959960938--TrainR2:-5.74273157119751--TestLoss:112.09136199951172--TestR2:-7.401880264282227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:724--TrainLoss:102.61747741699219--TrainR2:-5.741541385650635--TestLoss:112.07212829589844--TestR2:-7.400439262390137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:725--TrainLoss:102.59934997558594--TrainR2:-5.740350246429443--TestLoss:112.05287170410156--TestR2:-7.398995399475098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:726--TrainLoss:102.58124542236328--TrainR2:-5.739161014556885--TestLoss:112.03363037109375--TestR2:-7.397553443908691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:727--TrainLoss:102.56315612792969--TrainR2:-5.737972736358643--TestLoss:112.01438903808594--TestR2:-7.396111488342285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:728--TrainLoss:102.54508209228516--TrainR2:-5.736785411834717--TestLoss:111.99515533447266--TestR2:-7.394669532775879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:729--TrainLoss:102.52696990966797--TrainR2:-5.735595226287842--TestLoss:111.97592163085938--TestR2:-7.393228530883789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:730--TrainLoss:102.5088882446289--TrainR2:-5.734407424926758--TestLoss:111.95669555664062--TestR2:-7.391786575317383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:731--TrainLoss:102.49079895019531--TrainR2:-5.733219146728516--TestLoss:111.93746185302734--TestR2:-7.390345573425293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:732--TrainLoss:102.47271728515625--TrainR2:-5.732030868530273--TestLoss:111.91824340820312--TestR2:-7.388904571533203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:733--TrainLoss:102.45462799072266--TrainR2:-5.730842590332031--TestLoss:111.89904022216797--TestR2:-7.387465476989746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:734--TrainLoss:102.4365463256836--TrainR2:-5.7296552658081055--TestLoss:111.87982177734375--TestR2:-7.386024475097656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:735--TrainLoss:102.41844940185547--TrainR2:-5.728466033935547--TestLoss:111.86058807373047--TestR2:-7.384583473205566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:736--TrainLoss:102.40035247802734--TrainR2:-5.7272772789001465--TestLoss:111.84136962890625--TestR2:-7.383142471313477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:737--TrainLoss:102.38228607177734--TrainR2:-5.726090431213379--TestLoss:111.82215118408203--TestR2:-7.381702423095703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:738--TrainLoss:102.36421966552734--TrainR2:-5.724903583526611--TestLoss:111.80294799804688--TestR2:-7.38026237487793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:739--TrainLoss:102.34615325927734--TrainR2:-5.723716735839844--TestLoss:111.78372955322266--TestR2:-7.378822326660156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:740--TrainLoss:102.32808685302734--TrainR2:-5.722529888153076--TestLoss:111.76453399658203--TestR2:-7.377383232116699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:741--TrainLoss:102.31005096435547--TrainR2:-5.721344947814941--TestLoss:111.74535369873047--TestR2:-7.375946044921875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:742--TrainLoss:102.2919692993164--TrainR2:-5.720157146453857--TestLoss:111.72615051269531--TestR2:-7.374505996704102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:743--TrainLoss:102.27392578125--TrainR2:-5.718971252441406--TestLoss:111.70694732666016--TestR2:-7.3730669021606445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:744--TrainLoss:102.25585174560547--TrainR2:-5.717784404754639--TestLoss:111.68775939941406--TestR2:-7.371628761291504\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:745--TrainLoss:102.23780822753906--TrainR2:-5.716598987579346--TestLoss:111.66857147216797--TestR2:-7.370190620422363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:746--TrainLoss:102.21975708007812--TrainR2:-5.7154130935668945--TestLoss:111.6493911743164--TestR2:-7.368752479553223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:747--TrainLoss:102.20172119140625--TrainR2:-5.71422815322876--TestLoss:111.63021087646484--TestR2:-7.367314338684082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:748--TrainLoss:102.18366241455078--TrainR2:-5.71304178237915--TestLoss:111.61103057861328--TestR2:-7.365877151489258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:749--TrainLoss:102.16563415527344--TrainR2:-5.711857318878174--TestLoss:111.59185028076172--TestR2:-7.364439010620117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:750--TrainLoss:102.1475601196289--TrainR2:-5.710669994354248--TestLoss:111.57266998291016--TestR2:-7.363001823425293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:751--TrainLoss:102.12953186035156--TrainR2:-5.7094855308532715--TestLoss:111.55349731445312--TestR2:-7.361564636230469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:752--TrainLoss:102.11149597167969--TrainR2:-5.708300590515137--TestLoss:111.53431701660156--TestR2:-7.3601274490356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:753--TrainLoss:102.09341430664062--TrainR2:-5.7071123123168945--TestLoss:111.51515197753906--TestR2:-7.35869026184082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:754--TrainLoss:102.07540130615234--TrainR2:-5.705929279327393--TestLoss:111.49597930908203--TestR2:-7.3572540283203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:755--TrainLoss:102.057373046875--TrainR2:-5.704745292663574--TestLoss:111.47681427001953--TestR2:-7.355816841125488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:756--TrainLoss:102.03936004638672--TrainR2:-5.703561782836914--TestLoss:111.45767211914062--TestR2:-7.354381561279297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:757--TrainLoss:102.02135467529297--TrainR2:-5.702378749847412--TestLoss:111.4384994506836--TestR2:-7.352945327758789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:758--TrainLoss:102.00334167480469--TrainR2:-5.70119571685791--TestLoss:111.41935729980469--TestR2:-7.351510047912598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:759--TrainLoss:101.9853286743164--TrainR2:-5.700011730194092--TestLoss:111.40019989013672--TestR2:-7.350074768066406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:760--TrainLoss:101.96731567382812--TrainR2:-5.698828220367432--TestLoss:111.38105010986328--TestR2:-7.348638534545898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:761--TrainLoss:101.94931030273438--TrainR2:-5.697645664215088--TestLoss:111.36189270019531--TestR2:-7.347203254699707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:762--TrainLoss:101.93130493164062--TrainR2:-5.696462631225586--TestLoss:111.3427505493164--TestR2:-7.345767974853516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:763--TrainLoss:101.91329193115234--TrainR2:-5.695279121398926--TestLoss:111.32361602783203--TestR2:-7.344333648681641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:764--TrainLoss:101.89529418945312--TrainR2:-5.69409704208374--TestLoss:111.30445861816406--TestR2:-7.342898368835449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:765--TrainLoss:101.87726593017578--TrainR2:-5.692912578582764--TestLoss:111.28531646728516--TestR2:-7.341463088989258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:766--TrainLoss:101.85928344726562--TrainR2:-5.6917314529418945--TestLoss:111.26619720458984--TestR2:-7.340030670166016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:767--TrainLoss:101.84131622314453--TrainR2:-5.690551280975342--TestLoss:111.2470474243164--TestR2:-7.338594436645508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:768--TrainLoss:101.82331085205078--TrainR2:-5.68936824798584--TestLoss:111.2279281616211--TestR2:-7.337161064147949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:769--TrainLoss:101.8052978515625--TrainR2:-5.68818473815918--TestLoss:111.20879364013672--TestR2:-7.335727691650391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:770--TrainLoss:101.78732299804688--TrainR2:-5.6870036125183105--TestLoss:111.18968200683594--TestR2:-7.334294319152832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:771--TrainLoss:101.76932525634766--TrainR2:-5.685821533203125--TestLoss:111.17057037353516--TestR2:-7.332862854003906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:772--TrainLoss:101.75135040283203--TrainR2:-5.684640884399414--TestLoss:111.15144348144531--TestR2:-7.331428527832031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:773--TrainLoss:101.73336791992188--TrainR2:-5.683459281921387--TestLoss:111.13233947753906--TestR2:-7.329996109008789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:774--TrainLoss:101.71539306640625--TrainR2:-5.682278156280518--TestLoss:111.11322021484375--TestR2:-7.328563690185547\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:775--TrainLoss:101.69743347167969--TrainR2:-5.681098461151123--TestLoss:111.0941162109375--TestR2:-7.327131271362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:776--TrainLoss:101.679443359375--TrainR2:-5.679916858673096--TestLoss:111.07500457763672--TestR2:-7.325699806213379\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:777--TrainLoss:101.66149139404297--TrainR2:-5.678737163543701--TestLoss:111.05590057373047--TestR2:-7.324267387390137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:778--TrainLoss:101.6435317993164--TrainR2:-5.677556991577148--TestLoss:111.03678131103516--TestR2:-7.322834014892578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:779--TrainLoss:101.62555694580078--TrainR2:-5.6763763427734375--TestLoss:111.01771545410156--TestR2:-7.321405410766602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:780--TrainLoss:101.60758972167969--TrainR2:-5.675196170806885--TestLoss:110.99859619140625--TestR2:-7.319972038269043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:781--TrainLoss:101.5896224975586--TrainR2:-5.674015522003174--TestLoss:110.9794921875--TestR2:-7.318539619445801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:782--TrainLoss:101.57166290283203--TrainR2:-5.672835826873779--TestLoss:110.96039581298828--TestR2:-7.317109107971191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:783--TrainLoss:101.55372619628906--TrainR2:-5.671657085418701--TestLoss:110.94132995605469--TestR2:-7.315679550170898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:784--TrainLoss:101.53577423095703--TrainR2:-5.670478343963623--TestLoss:110.92222595214844--TestR2:-7.314247131347656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:785--TrainLoss:101.51781463623047--TrainR2:-5.66929817199707--TestLoss:110.90316009521484--TestR2:-7.31281852722168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:786--TrainLoss:101.49986267089844--TrainR2:-5.668118476867676--TestLoss:110.88406372070312--TestR2:-7.311387062072754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:787--TrainLoss:101.48197937011719--TrainR2:-5.6669440269470215--TestLoss:110.86500549316406--TestR2:-7.309958457946777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:788--TrainLoss:101.46400451660156--TrainR2:-5.665762901306152--TestLoss:110.84591674804688--TestR2:-7.308527946472168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:789--TrainLoss:101.446044921875--TrainR2:-5.664583206176758--TestLoss:110.82685089111328--TestR2:-7.307098388671875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:790--TrainLoss:101.42810821533203--TrainR2:-5.66340446472168--TestLoss:110.80779266357422--TestR2:-7.305669784545898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:791--TrainLoss:101.4101791381836--TrainR2:-5.662226676940918--TestLoss:110.78871154785156--TestR2:-7.3042402267456055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:792--TrainLoss:101.39224243164062--TrainR2:-5.661048889160156--TestLoss:110.7696304321289--TestR2:-7.302809715270996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:793--TrainLoss:101.37429809570312--TrainR2:-5.65986967086792--TestLoss:110.75057220458984--TestR2:-7.3013811111450195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:794--TrainLoss:101.35639953613281--TrainR2:-5.658693790435791--TestLoss:110.73150634765625--TestR2:-7.299952507019043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:795--TrainLoss:101.33848571777344--TrainR2:-5.657516956329346--TestLoss:110.71247863769531--TestR2:-7.298525810241699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:796--TrainLoss:101.320556640625--TrainR2:-5.656339168548584--TestLoss:110.69339752197266--TestR2:-7.29709529876709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:797--TrainLoss:101.30261993408203--TrainR2:-5.655160903930664--TestLoss:110.67436981201172--TestR2:-7.295668601989746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:798--TrainLoss:101.28470611572266--TrainR2:-5.6539835929870605--TestLoss:110.65528869628906--TestR2:-7.294239044189453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:799--TrainLoss:101.26677703857422--TrainR2:-5.652806282043457--TestLoss:110.63626861572266--TestR2:-7.292813301086426\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:800--TrainLoss:101.2488784790039--TrainR2:-5.65162992477417--TestLoss:110.61722564697266--TestR2:-7.291385650634766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:801--TrainLoss:101.23096466064453--TrainR2:-5.650453567504883--TestLoss:110.5981674194336--TestR2:-7.2899580001831055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:802--TrainLoss:101.21316528320312--TrainR2:-5.6492838859558105--TestLoss:110.5791244506836--TestR2:-7.288530349731445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:803--TrainLoss:101.19522857666016--TrainR2:-5.648105621337891--TestLoss:110.56011199951172--TestR2:-7.287104606628418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:804--TrainLoss:101.17731475830078--TrainR2:-5.646928787231445--TestLoss:110.54106903076172--TestR2:-7.285676956176758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:805--TrainLoss:101.1594009399414--TrainR2:-5.645751953125--TestLoss:110.52203369140625--TestR2:-7.2842512130737305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:806--TrainLoss:101.1415023803711--TrainR2:-5.644576072692871--TestLoss:110.5030288696289--TestR2:-7.2828264236450195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:807--TrainLoss:101.12360382080078--TrainR2:-5.643400192260742--TestLoss:110.48397064208984--TestR2:-7.281397819519043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:808--TrainLoss:101.10570526123047--TrainR2:-5.642224311828613--TestLoss:110.46495819091797--TestR2:-7.279973030090332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:809--TrainLoss:101.08782958984375--TrainR2:-5.641050338745117--TestLoss:110.4459228515625--TestR2:-7.278546333312988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:810--TrainLoss:101.06993865966797--TrainR2:-5.6398749351501465--TestLoss:110.42691802978516--TestR2:-7.277121543884277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:811--TrainLoss:101.05204772949219--TrainR2:-5.638699531555176--TestLoss:110.40789031982422--TestR2:-7.275694847106934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:812--TrainLoss:101.03418731689453--TrainR2:-5.637526035308838--TestLoss:110.38890075683594--TestR2:-7.2742719650268555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:813--TrainLoss:101.01631164550781--TrainR2:-5.636351108551025--TestLoss:110.36988830566406--TestR2:-7.2728471755981445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:814--TrainLoss:100.99844360351562--TrainR2:-5.6351776123046875--TestLoss:110.35087585449219--TestR2:-7.271421432495117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:815--TrainLoss:100.98057556152344--TrainR2:-5.63400411605835--TestLoss:110.33185577392578--TestR2:-7.26999568939209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:816--TrainLoss:100.96267700195312--TrainR2:-5.632828235626221--TestLoss:110.31287384033203--TestR2:-7.268572807312012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:817--TrainLoss:100.94482421875--TrainR2:-5.631654739379883--TestLoss:110.29386901855469--TestR2:-7.267148971557617\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:818--TrainLoss:100.92694854736328--TrainR2:-5.630480766296387--TestLoss:110.27488708496094--TestR2:-7.265726089477539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:819--TrainLoss:100.9090805053711--TrainR2:-5.629306793212891--TestLoss:110.25587463378906--TestR2:-7.264300346374512\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:820--TrainLoss:100.89125061035156--TrainR2:-5.628135681152344--TestLoss:110.23689270019531--TestR2:-7.262877464294434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:821--TrainLoss:100.87339782714844--TrainR2:-5.626962661743164--TestLoss:110.2178955078125--TestR2:-7.2614545822143555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:822--TrainLoss:100.85552215576172--TrainR2:-5.62578821182251--TestLoss:110.19892883300781--TestR2:-7.260032653808594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:823--TrainLoss:100.8376693725586--TrainR2:-5.62461519241333--TestLoss:110.17993927001953--TestR2:-7.258608818054199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:824--TrainLoss:100.81985473632812--TrainR2:-5.6234450340271--TestLoss:110.16095733642578--TestR2:-7.257185935974121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:825--TrainLoss:100.80198669433594--TrainR2:-5.622271537780762--TestLoss:110.1419677734375--TestR2:-7.255762100219727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:826--TrainLoss:100.78414916992188--TrainR2:-5.621099472045898--TestLoss:110.12300872802734--TestR2:-7.254342079162598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:827--TrainLoss:100.7663345336914--TrainR2:-5.61992883682251--TestLoss:110.10401916503906--TestR2:-7.252918243408203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:828--TrainLoss:100.74849700927734--TrainR2:-5.618757247924805--TestLoss:110.08504486083984--TestR2:-7.251496315002441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:829--TrainLoss:100.73065185546875--TrainR2:-5.617585182189941--TestLoss:110.06607818603516--TestR2:-7.25007438659668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:830--TrainLoss:100.7127914428711--TrainR2:-5.6164116859436035--TestLoss:110.04712677001953--TestR2:-7.248653411865234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:831--TrainLoss:100.6949691772461--TrainR2:-5.615240573883057--TestLoss:110.02816009521484--TestR2:-7.247232437133789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:832--TrainLoss:100.67713165283203--TrainR2:-5.614068984985352--TestLoss:110.00919342041016--TestR2:-7.245810508728027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:833--TrainLoss:100.65933990478516--TrainR2:-5.6128997802734375--TestLoss:109.990234375--TestR2:-7.244389533996582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:834--TrainLoss:100.64148712158203--TrainR2:-5.611727237701416--TestLoss:109.97127532958984--TestR2:-7.242968559265137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:835--TrainLoss:100.62367248535156--TrainR2:-5.610556602478027--TestLoss:109.95233917236328--TestR2:-7.241548538208008\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:836--TrainLoss:100.60588073730469--TrainR2:-5.60938835144043--TestLoss:109.93339538574219--TestR2:-7.240128517150879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:837--TrainLoss:100.58806610107422--TrainR2:-5.608217239379883--TestLoss:109.9144287109375--TestR2:-7.238707542419434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:838--TrainLoss:100.57022857666016--TrainR2:-5.607045650482178--TestLoss:109.8954849243164--TestR2:-7.237287521362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:839--TrainLoss:100.55243682861328--TrainR2:-5.605876922607422--TestLoss:109.87654113769531--TestR2:-7.235867500305176\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:840--TrainLoss:100.5346450805664--TrainR2:-5.604708194732666--TestLoss:109.85761260986328--TestR2:-7.234448432922363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:841--TrainLoss:100.51683044433594--TrainR2:-5.603537559509277--TestLoss:109.83866882324219--TestR2:-7.233028411865234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:842--TrainLoss:100.49899291992188--TrainR2:-5.602365970611572--TestLoss:109.81974792480469--TestR2:-7.231610298156738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:843--TrainLoss:100.48117065429688--TrainR2:-5.601194858551025--TestLoss:109.8008041381836--TestR2:-7.230191230773926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:844--TrainLoss:100.46345520019531--TrainR2:-5.600030899047852--TestLoss:109.78189086914062--TestR2:-7.22877311706543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:845--TrainLoss:100.44564819335938--TrainR2:-5.5988616943359375--TestLoss:109.76294708251953--TestR2:-7.227353096008301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:846--TrainLoss:100.4278335571289--TrainR2:-5.597691059112549--TestLoss:109.74405670166016--TestR2:-7.2259368896484375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:847--TrainLoss:100.4100570678711--TrainR2:-5.596522808074951--TestLoss:109.72511291503906--TestR2:-7.224516868591309\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:848--TrainLoss:100.39228057861328--TrainR2:-5.595355033874512--TestLoss:109.70619201660156--TestR2:-7.2230987548828125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:849--TrainLoss:100.37451171875--TrainR2:-5.594188213348389--TestLoss:109.68727111816406--TestR2:-7.221680641174316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:850--TrainLoss:100.35670471191406--TrainR2:-5.593018054962158--TestLoss:109.66838073730469--TestR2:-7.220264434814453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:851--TrainLoss:100.33892059326172--TrainR2:-5.5918498039245605--TestLoss:109.64945220947266--TestR2:-7.218846321105957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:852--TrainLoss:100.32112884521484--TrainR2:-5.590681076049805--TestLoss:109.63056182861328--TestR2:-7.217430114746094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:853--TrainLoss:100.30335998535156--TrainR2:-5.589513778686523--TestLoss:109.61164093017578--TestR2:-7.216012001037598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:854--TrainLoss:100.28561401367188--TrainR2:-5.588347911834717--TestLoss:109.5927505493164--TestR2:-7.214595794677734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:855--TrainLoss:100.26786041259766--TrainR2:-5.587181091308594--TestLoss:109.5738525390625--TestR2:-7.213179588317871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:856--TrainLoss:100.25006866455078--TrainR2:-5.586012840270996--TestLoss:109.55494689941406--TestR2:-7.211762428283691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:857--TrainLoss:100.2322998046875--TrainR2:-5.584845066070557--TestLoss:109.53605651855469--TestR2:-7.210346221923828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:858--TrainLoss:100.21455383300781--TrainR2:-5.58367919921875--TestLoss:109.51715087890625--TestR2:-7.208929061889648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:859--TrainLoss:100.19678497314453--TrainR2:-5.582512378692627--TestLoss:109.49827575683594--TestR2:-7.207513809204102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:860--TrainLoss:100.17903137207031--TrainR2:-5.581345558166504--TestLoss:109.47937774658203--TestR2:-7.206097602844238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:861--TrainLoss:100.16127014160156--TrainR2:-5.580178737640381--TestLoss:109.46050262451172--TestR2:-7.204683303833008\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:862--TrainLoss:100.1435546875--TrainR2:-5.579015254974365--TestLoss:109.44163513183594--TestR2:-7.203269004821777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:863--TrainLoss:100.12578582763672--TrainR2:-5.577847480773926--TestLoss:109.42273712158203--TestR2:-7.201852798461914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:864--TrainLoss:100.10802459716797--TrainR2:-5.576681137084961--TestLoss:109.40386199951172--TestR2:-7.200437545776367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:865--TrainLoss:100.09027099609375--TrainR2:-5.575514316558838--TestLoss:109.38500213623047--TestR2:-7.199024200439453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:866--TrainLoss:100.07257080078125--TrainR2:-5.574351787567139--TestLoss:109.36612701416016--TestR2:-7.197608947753906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:867--TrainLoss:100.05482482910156--TrainR2:-5.573185920715332--TestLoss:109.34725189208984--TestR2:-7.196194648742676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:868--TrainLoss:100.0370864868164--TrainR2:-5.572020530700684--TestLoss:109.32837677001953--TestR2:-7.194779396057129\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:869--TrainLoss:100.01934814453125--TrainR2:-5.570855140686035--TestLoss:109.30953216552734--TestR2:-7.193367004394531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:870--TrainLoss:100.00165557861328--TrainR2:-5.569692611694336--TestLoss:109.29066467285156--TestR2:-7.191952705383301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:871--TrainLoss:99.98388671875--TrainR2:-5.568525791168213--TestLoss:109.27179718017578--TestR2:-7.19053840637207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:872--TrainLoss:99.9661865234375--TrainR2:-5.567362308502197--TestLoss:109.25294494628906--TestR2:-7.189125061035156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:873--TrainLoss:99.94844055175781--TrainR2:-5.566196918487549--TestLoss:109.23410034179688--TestR2:-7.187712669372559\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:874--TrainLoss:99.93072509765625--TrainR2:-5.565033435821533--TestLoss:109.21524810791016--TestR2:-7.1862993240356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:875--TrainLoss:99.91302490234375--TrainR2:-5.563870429992676--TestLoss:109.1964111328125--TestR2:-7.184887886047363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:876--TrainLoss:99.89529418945312--TrainR2:-5.562705039978027--TestLoss:109.17755889892578--TestR2:-7.183474540710449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:877--TrainLoss:99.87757110595703--TrainR2:-5.5615410804748535--TestLoss:109.1587142944336--TestR2:-7.182062149047852\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:878--TrainLoss:99.85992431640625--TrainR2:-5.560381889343262--TestLoss:109.13986206054688--TestR2:-7.1806488037109375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:879--TrainLoss:99.8421630859375--TrainR2:-5.5592145919799805--TestLoss:109.12104034423828--TestR2:-7.179238319396973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:880--TrainLoss:99.8244400024414--TrainR2:-5.558050632476807--TestLoss:109.10220336914062--TestR2:-7.177825927734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:881--TrainLoss:99.80677795410156--TrainR2:-5.556890487670898--TestLoss:109.08338165283203--TestR2:-7.17641544342041\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:882--TrainLoss:99.78907775878906--TrainR2:-5.555727005004883--TestLoss:109.06453704833984--TestR2:-7.1750030517578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:883--TrainLoss:99.7713623046875--TrainR2:-5.554563522338867--TestLoss:109.04572296142578--TestR2:-7.173592567443848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:884--TrainLoss:99.75366973876953--TrainR2:-5.553401470184326--TestLoss:109.0268783569336--TestR2:-7.17218017578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:885--TrainLoss:99.73599243164062--TrainR2:-5.552239894866943--TestLoss:109.00806427001953--TestR2:-7.170769691467285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:886--TrainLoss:99.71830749511719--TrainR2:-5.5510783195495605--TestLoss:108.98927307128906--TestR2:-7.169361114501953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:887--TrainLoss:99.70060729980469--TrainR2:-5.549915313720703--TestLoss:108.97044372558594--TestR2:-7.167949676513672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:888--TrainLoss:99.68293762207031--TrainR2:-5.5487542152404785--TestLoss:108.95162200927734--TestR2:-7.166539192199707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:889--TrainLoss:99.66524505615234--TrainR2:-5.5475921630859375--TestLoss:108.93280792236328--TestR2:-7.165128707885742\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:890--TrainLoss:99.6475601196289--TrainR2:-5.546430587768555--TestLoss:108.91400909423828--TestR2:-7.16372013092041\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:891--TrainLoss:99.62991333007812--TrainR2:-5.545270919799805--TestLoss:108.89521026611328--TestR2:-7.162310600280762\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:892--TrainLoss:99.61223602294922--TrainR2:-5.544109344482422--TestLoss:108.87638854980469--TestR2:-7.160900115966797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:893--TrainLoss:99.59455108642578--TrainR2:-5.542947769165039--TestLoss:108.85759735107422--TestR2:-7.159491539001465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:894--TrainLoss:99.57689666748047--TrainR2:-5.541788101196289--TestLoss:108.83881378173828--TestR2:-7.158083915710449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:895--TrainLoss:99.55919647216797--TrainR2:-5.54062557220459--TestLoss:108.81999969482422--TestR2:-7.156673431396484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:896--TrainLoss:99.54154205322266--TrainR2:-5.539465427398682--TestLoss:108.80119323730469--TestR2:-7.155263900756836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:897--TrainLoss:99.5239028930664--TrainR2:-5.538306713104248--TestLoss:108.78242492675781--TestR2:-7.153857231140137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:898--TrainLoss:99.50624084472656--TrainR2:-5.537146091461182--TestLoss:108.76361846923828--TestR2:-7.152447700500488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:899--TrainLoss:99.48855590820312--TrainR2:-5.535984516143799--TestLoss:108.74485778808594--TestR2:-7.151041030883789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:900--TrainLoss:99.470947265625--TrainR2:-5.53482723236084--TestLoss:108.72605895996094--TestR2:-7.149632453918457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:901--TrainLoss:99.45330047607422--TrainR2:-5.533668041229248--TestLoss:108.70728302001953--TestR2:-7.148224830627441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:902--TrainLoss:99.43563079833984--TrainR2:-5.532507419586182--TestLoss:108.68851470947266--TestR2:-7.146818161010742\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:903--TrainLoss:99.4179916381836--TrainR2:-5.53134822845459--TestLoss:108.66972351074219--TestR2:-7.14540958404541\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:904--TrainLoss:99.40035247802734--TrainR2:-5.530189514160156--TestLoss:108.65095520019531--TestR2:-7.144002914428711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:905--TrainLoss:99.3826904296875--TrainR2:-5.529029369354248--TestLoss:108.6322021484375--TestR2:-7.142597198486328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:906--TrainLoss:99.36505889892578--TrainR2:-5.527871131896973--TestLoss:108.61343383789062--TestR2:-7.141190528869629\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:907--TrainLoss:99.3474349975586--TrainR2:-5.5267133712768555--TestLoss:108.59465026855469--TestR2:-7.139782905578613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:908--TrainLoss:99.32978820800781--TrainR2:-5.525554180145264--TestLoss:108.57588195800781--TestR2:-7.138376235961914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:909--TrainLoss:99.31218719482422--TrainR2:-5.524397850036621--TestLoss:108.55714416503906--TestR2:-7.136971473693848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:910--TrainLoss:99.29452514648438--TrainR2:-5.523237228393555--TestLoss:108.53839874267578--TestR2:-7.135565757751465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:911--TrainLoss:99.27693176269531--TrainR2:-5.5220818519592285--TestLoss:108.5196304321289--TestR2:-7.134159088134766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:912--TrainLoss:99.25931549072266--TrainR2:-5.520924091339111--TestLoss:108.50089263916016--TestR2:-7.132754325866699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:913--TrainLoss:99.24165344238281--TrainR2:-5.519763946533203--TestLoss:108.48212432861328--TestR2:-7.13134765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:914--TrainLoss:99.22406768798828--TrainR2:-5.518608570098877--TestLoss:108.46337890625--TestR2:-7.129942893981934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:915--TrainLoss:99.20643615722656--TrainR2:-5.517450332641602--TestLoss:108.44462585449219--TestR2:-7.128537178039551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:916--TrainLoss:99.18883514404297--TrainR2:-5.516294002532959--TestLoss:108.4258804321289--TestR2:-7.127132415771484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:917--TrainLoss:99.17123413085938--TrainR2:-5.515137195587158--TestLoss:108.40716552734375--TestR2:-7.125729560852051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:918--TrainLoss:99.15362548828125--TrainR2:-5.513980865478516--TestLoss:108.38841247558594--TestR2:-7.124323844909668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:919--TrainLoss:99.13602447509766--TrainR2:-5.512824535369873--TestLoss:108.36968994140625--TestR2:-7.122920989990234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:920--TrainLoss:99.11841583251953--TrainR2:-5.511667728424072--TestLoss:108.35095977783203--TestR2:-7.121516227722168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:921--TrainLoss:99.10077667236328--TrainR2:-5.510509014129639--TestLoss:108.33222961425781--TestR2:-7.120112419128418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:922--TrainLoss:99.08322143554688--TrainR2:-5.509355545043945--TestLoss:108.3134994506836--TestR2:-7.118708610534668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:923--TrainLoss:99.06560516357422--TrainR2:-5.508198261260986--TestLoss:108.2947998046875--TestR2:-7.117306709289551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:924--TrainLoss:99.04804992675781--TrainR2:-5.507044792175293--TestLoss:108.27605438232422--TestR2:-7.115901947021484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:925--TrainLoss:99.03043365478516--TrainR2:-5.505887508392334--TestLoss:108.2573471069336--TestR2:-7.114500045776367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:926--TrainLoss:99.0128402709961--TrainR2:-5.504732131958008--TestLoss:108.23863220214844--TestR2:-7.113097190856934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:927--TrainLoss:98.99527740478516--TrainR2:-5.503578186035156--TestLoss:108.21992492675781--TestR2:-7.111695289611816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:928--TrainLoss:98.9776840209961--TrainR2:-5.502422332763672--TestLoss:108.2011947631836--TestR2:-7.11029052734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:929--TrainLoss:98.96012115478516--TrainR2:-5.50126838684082--TestLoss:108.18247985839844--TestR2:-7.108887672424316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:930--TrainLoss:98.9425277709961--TrainR2:-5.500112533569336--TestLoss:108.16378784179688--TestR2:-7.107486724853516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:931--TrainLoss:98.92493438720703--TrainR2:-5.498956680297852--TestLoss:108.14508056640625--TestR2:-7.106084823608398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:932--TrainLoss:98.90737915039062--TrainR2:-5.497803688049316--TestLoss:108.12638854980469--TestR2:-7.104683876037598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:933--TrainLoss:98.8897933959961--TrainR2:-5.49664831161499--TestLoss:108.10769653320312--TestR2:-7.103282928466797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:934--TrainLoss:98.87223815917969--TrainR2:-5.495494842529297--TestLoss:108.08900451660156--TestR2:-7.101881980895996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:935--TrainLoss:98.85469818115234--TrainR2:-5.49434232711792--TestLoss:108.0703125--TestR2:-7.100480079650879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:936--TrainLoss:98.8371353149414--TrainR2:-5.493188858032227--TestLoss:108.05162048339844--TestR2:-7.099079132080078\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:937--TrainLoss:98.81954956054688--TrainR2:-5.4920334815979--TestLoss:108.03292846679688--TestR2:-7.097678184509277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:938--TrainLoss:98.802001953125--TrainR2:-5.490880489349365--TestLoss:108.01424407958984--TestR2:-7.096278190612793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:939--TrainLoss:98.78446960449219--TrainR2:-5.4897284507751465--TestLoss:107.99555206298828--TestR2:-7.094877243041992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:940--TrainLoss:98.76688385009766--TrainR2:-5.4885735511779785--TestLoss:107.97687530517578--TestR2:-7.093477249145508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:941--TrainLoss:98.7493896484375--TrainR2:-5.487423896789551--TestLoss:107.95820617675781--TestR2:-7.092077255249023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:942--TrainLoss:98.73176574707031--TrainR2:-5.486266136169434--TestLoss:107.93952941894531--TestR2:-7.090677261352539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:943--TrainLoss:98.71429443359375--TrainR2:-5.485118389129639--TestLoss:107.92086029052734--TestR2:-7.089278221130371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:944--TrainLoss:98.69673156738281--TrainR2:-5.483964443206787--TestLoss:107.90218353271484--TestR2:-7.087878227233887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:945--TrainLoss:98.67919921875--TrainR2:-5.482813358306885--TestLoss:107.88351440429688--TestR2:-7.086479187011719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:946--TrainLoss:98.6616439819336--TrainR2:-5.481659412384033--TestLoss:107.8648681640625--TestR2:-7.085081100463867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:947--TrainLoss:98.64411926269531--TrainR2:-5.480508327484131--TestLoss:107.8462142944336--TestR2:-7.083683013916016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:948--TrainLoss:98.62657928466797--TrainR2:-5.479356288909912--TestLoss:107.8275375366211--TestR2:-7.082283020019531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:949--TrainLoss:98.60905456542969--TrainR2:-5.478204727172852--TestLoss:107.80889129638672--TestR2:-7.080885887145996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:950--TrainLoss:98.5915298461914--TrainR2:-5.477053165435791--TestLoss:107.79023742675781--TestR2:-7.0794878005981445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:951--TrainLoss:98.57398986816406--TrainR2:-5.475901126861572--TestLoss:107.7715835571289--TestR2:-7.078088760375977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:952--TrainLoss:98.55648803710938--TrainR2:-5.4747514724731445--TestLoss:107.75291442871094--TestR2:-7.076689720153809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:953--TrainLoss:98.53895568847656--TrainR2:-5.473599433898926--TestLoss:107.73429107666016--TestR2:-7.07529354095459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:954--TrainLoss:98.52145385742188--TrainR2:-5.472449779510498--TestLoss:107.71564483642578--TestR2:-7.073896408081055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:955--TrainLoss:98.5039291381836--TrainR2:-5.471298694610596--TestLoss:107.69700622558594--TestR2:-7.072498321533203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:956--TrainLoss:98.48644256591797--TrainR2:-5.470149517059326--TestLoss:107.67837524414062--TestR2:-7.071102142333984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:957--TrainLoss:98.46892547607422--TrainR2:-5.468998432159424--TestLoss:107.65972900390625--TestR2:-7.069705009460449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:958--TrainLoss:98.45142364501953--TrainR2:-5.467848777770996--TestLoss:107.64109802246094--TestR2:-7.0683088302612305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:959--TrainLoss:98.43389892578125--TrainR2:-5.466697692871094--TestLoss:107.6224594116211--TestR2:-7.066911697387695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:960--TrainLoss:98.41641235351562--TrainR2:-5.465548992156982--TestLoss:107.60384368896484--TestR2:-7.065515518188477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:961--TrainLoss:98.39888763427734--TrainR2:-5.464397430419922--TestLoss:107.58522033691406--TestR2:-7.064120292663574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:962--TrainLoss:98.38145446777344--TrainR2:-5.463252067565918--TestLoss:107.56659698486328--TestR2:-7.0627241134643555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:963--TrainLoss:98.36392211914062--TrainR2:-5.462100982666016--TestLoss:107.5479736328125--TestR2:-7.061328887939453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:964--TrainLoss:98.34642791748047--TrainR2:-5.460951328277588--TestLoss:107.52935791015625--TestR2:-7.059932708740234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:965--TrainLoss:98.32896423339844--TrainR2:-5.459803581237793--TestLoss:107.5107421875--TestR2:-7.058537483215332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:966--TrainLoss:98.31146240234375--TrainR2:-5.458653926849365--TestLoss:107.49212646484375--TestR2:-7.05714225769043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:967--TrainLoss:98.29397583007812--TrainR2:-5.457505702972412--TestLoss:107.4735107421875--TestR2:-7.055747032165527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:968--TrainLoss:98.27649688720703--TrainR2:-5.456357002258301--TestLoss:107.45491790771484--TestR2:-7.054352760314941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:969--TrainLoss:98.25899505615234--TrainR2:-5.455207347869873--TestLoss:107.43630981445312--TestR2:-7.0529584884643555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:970--TrainLoss:98.24158477783203--TrainR2:-5.454063892364502--TestLoss:107.41770935058594--TestR2:-7.0515642166137695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:971--TrainLoss:98.22404479980469--TrainR2:-5.452910900115967--TestLoss:107.39910125732422--TestR2:-7.050168991088867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:972--TrainLoss:98.20661926269531--TrainR2:-5.451766490936279--TestLoss:107.38050842285156--TestR2:-7.048775672912598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:973--TrainLoss:98.18911743164062--TrainR2:-5.450616359710693--TestLoss:107.36193084716797--TestR2:-7.0473833084106445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:974--TrainLoss:98.17166900634766--TrainR2:-5.449470520019531--TestLoss:107.34332275390625--TestR2:-7.045989036560059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:975--TrainLoss:98.15418243408203--TrainR2:-5.448321342468262--TestLoss:107.3247299194336--TestR2:-7.044594764709473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:976--TrainLoss:98.13673400878906--TrainR2:-5.447175025939941--TestLoss:107.30613708496094--TestR2:-7.043201446533203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:977--TrainLoss:98.11927795410156--TrainR2:-5.446028709411621--TestLoss:107.28755950927734--TestR2:-7.04180908203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:978--TrainLoss:98.10183715820312--TrainR2:-5.444882869720459--TestLoss:107.26900482177734--TestR2:-7.040417671203613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:979--TrainLoss:98.08436584472656--TrainR2:-5.443735122680664--TestLoss:107.25040435791016--TestR2:-7.039023399353027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:980--TrainLoss:98.06692504882812--TrainR2:-5.442588806152344--TestLoss:107.2318344116211--TestR2:-7.037631988525391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:981--TrainLoss:98.04947662353516--TrainR2:-5.441442966461182--TestLoss:107.21324157714844--TestR2:-7.036237716674805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:982--TrainLoss:98.03202056884766--TrainR2:-5.440296173095703--TestLoss:107.19467163085938--TestR2:-7.034846305847168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:983--TrainLoss:98.01460266113281--TrainR2:-5.439151763916016--TestLoss:107.17611694335938--TestR2:-7.033455848693848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:984--TrainLoss:97.99714660644531--TrainR2:-5.438004970550537--TestLoss:107.15755462646484--TestR2:-7.0320634841918945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:985--TrainLoss:97.97972106933594--TrainR2:-5.436860084533691--TestLoss:107.13896942138672--TestR2:-7.030671119689941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:986--TrainLoss:97.96227264404297--TrainR2:-5.435713768005371--TestLoss:107.12042999267578--TestR2:-7.0292816162109375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:987--TrainLoss:97.9448471069336--TrainR2:-5.434568881988525--TestLoss:107.10186004638672--TestR2:-7.027889251708984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:988--TrainLoss:97.92737579345703--TrainR2:-5.433421611785889--TestLoss:107.08330535888672--TestR2:-7.026498794555664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:989--TrainLoss:97.90998077392578--TrainR2:-5.432278633117676--TestLoss:107.06475067138672--TestR2:-7.025108337402344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:990--TrainLoss:97.89256286621094--TrainR2:-5.431134223937988--TestLoss:107.04621124267578--TestR2:-7.023717880249023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:991--TrainLoss:97.87513732910156--TrainR2:-5.429989337921143--TestLoss:107.02765655517578--TestR2:-7.022327423095703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:992--TrainLoss:97.85770416259766--TrainR2:-5.428843975067139--TestLoss:107.00911712646484--TestR2:-7.020937919616699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:993--TrainLoss:97.84030151367188--TrainR2:-5.427700996398926--TestLoss:106.99057006835938--TestR2:-7.019547462463379\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:994--TrainLoss:97.82289123535156--TrainR2:-5.4265570640563965--TestLoss:106.97201538085938--TestR2:-7.018157005310059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:995--TrainLoss:97.80546569824219--TrainR2:-5.425412178039551--TestLoss:106.9534912109375--TestR2:-7.016768455505371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:996--TrainLoss:97.78805541992188--TrainR2:-5.4242682456970215--TestLoss:106.93494415283203--TestR2:-7.015377998352051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:997--TrainLoss:97.77069091796875--TrainR2:-5.423128128051758--TestLoss:106.91641998291016--TestR2:-7.013989448547363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:998--TrainLoss:97.75321197509766--TrainR2:-5.4219794273376465--TestLoss:106.89788818359375--TestR2:-7.012600898742676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:999--TrainLoss:97.73587799072266--TrainR2:-5.420840263366699--TestLoss:106.87935638427734--TestR2:-7.011211395263672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1000--TrainLoss:97.71842956542969--TrainR2:-5.419694423675537--TestLoss:106.86083984375--TestR2:-7.009823799133301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1001--TrainLoss:97.7010498046875--TrainR2:-5.418552875518799--TestLoss:106.8423080444336--TestR2:-7.008434295654297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1002--TrainLoss:97.68368530273438--TrainR2:-5.417411804199219--TestLoss:106.82379150390625--TestR2:-7.007046699523926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1003--TrainLoss:97.6662368774414--TrainR2:-5.416265487670898--TestLoss:106.80525970458984--TestR2:-7.005658149719238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1004--TrainLoss:97.64886474609375--TrainR2:-5.415124416351318--TestLoss:106.78675842285156--TestR2:-7.004270553588867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1005--TrainLoss:97.63146209716797--TrainR2:-5.413980960845947--TestLoss:106.76825714111328--TestR2:-7.0028839111328125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1006--TrainLoss:97.61409759521484--TrainR2:-5.412840366363525--TestLoss:106.74974822998047--TestR2:-7.001497268676758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1007--TrainLoss:97.59671020507812--TrainR2:-5.411697864532471--TestLoss:106.73123168945312--TestR2:-7.00010871887207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1008--TrainLoss:97.57933044433594--TrainR2:-5.410556316375732--TestLoss:106.71273040771484--TestR2:-6.998721599578857\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1009--TrainLoss:97.56195068359375--TrainR2:-5.409414291381836--TestLoss:106.69422149658203--TestR2:-6.997334957122803\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1010--TrainLoss:97.54454803466797--TrainR2:-5.408271312713623--TestLoss:106.67572021484375--TestR2:-6.99594783782959\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1011--TrainLoss:97.52720642089844--TrainR2:-5.407132148742676--TestLoss:106.65721130371094--TestR2:-6.994560718536377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1012--TrainLoss:97.50984954833984--TrainR2:-5.405991554260254--TestLoss:106.63872528076172--TestR2:-6.9931745529174805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1013--TrainLoss:97.49243927001953--TrainR2:-5.404847621917725--TestLoss:106.6202392578125--TestR2:-6.9917893409729\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1014--TrainLoss:97.47510528564453--TrainR2:-5.4037089347839355--TestLoss:106.60173797607422--TestR2:-6.990402698516846\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1015--TrainLoss:97.45774841308594--TrainR2:-5.402568817138672--TestLoss:106.58325958251953--TestR2:-6.989017486572266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1016--TrainLoss:97.44036865234375--TrainR2:-5.401427268981934--TestLoss:106.56476593017578--TestR2:-6.987631320953369\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1017--TrainLoss:97.42302703857422--TrainR2:-5.400288105010986--TestLoss:106.54630279541016--TestR2:-6.9862470626831055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1018--TrainLoss:97.40564727783203--TrainR2:-5.39914608001709--TestLoss:106.5278091430664--TestR2:-6.984860897064209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1019--TrainLoss:97.38833618164062--TrainR2:-5.398008346557617--TestLoss:106.50935363769531--TestR2:-6.983477592468262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1020--TrainLoss:97.37097930908203--TrainR2:-5.3968682289123535--TestLoss:106.49085998535156--TestR2:-6.982091426849365\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1021--TrainLoss:97.3536148071289--TrainR2:-5.395727634429932--TestLoss:106.47238159179688--TestR2:-6.980706691741943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1022--TrainLoss:97.33627319335938--TrainR2:-5.394587993621826--TestLoss:106.45392608642578--TestR2:-6.979323387145996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1023--TrainLoss:97.31898498535156--TrainR2:-5.3934526443481445--TestLoss:106.43546295166016--TestR2:-6.977939128875732\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1024--TrainLoss:97.30155944824219--TrainR2:-5.392307758331299--TestLoss:106.4169921875--TestR2:-6.976554870605469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1025--TrainLoss:97.28426361083984--TrainR2:-5.391171455383301--TestLoss:106.3985366821289--TestR2:-6.9751715660095215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1026--TrainLoss:97.26693725585938--TrainR2:-5.390032768249512--TestLoss:106.38007354736328--TestR2:-6.973787307739258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1027--TrainLoss:97.24958801269531--TrainR2:-5.388893127441406--TestLoss:106.36161804199219--TestR2:-6.9724040031433105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1028--TrainLoss:97.23224639892578--TrainR2:-5.387753963470459--TestLoss:106.3431625366211--TestR2:-6.971020698547363\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1029--TrainLoss:97.21492767333984--TrainR2:-5.386616230010986--TestLoss:106.32470703125--TestR2:-6.969637393951416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1030--TrainLoss:97.19758605957031--TrainR2:-5.385477066040039--TestLoss:106.30626678466797--TestR2:-6.968255043029785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1031--TrainLoss:97.1802749633789--TrainR2:-5.384339809417725--TestLoss:106.28782653808594--TestR2:-6.966872692108154\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1032--TrainLoss:97.16297912597656--TrainR2:-5.383203506469727--TestLoss:106.26937103271484--TestR2:-6.965489387512207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1033--TrainLoss:97.14564514160156--TrainR2:-5.3820648193359375--TestLoss:106.25092315673828--TestR2:-6.964107036590576\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1034--TrainLoss:97.12834167480469--TrainR2:-5.380928039550781--TestLoss:106.23250579833984--TestR2:-6.962726593017578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1035--TrainLoss:97.11103057861328--TrainR2:-5.379790782928467--TestLoss:106.21405792236328--TestR2:-6.961343288421631\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1036--TrainLoss:97.09369659423828--TrainR2:-5.378652095794678--TestLoss:106.19560241699219--TestR2:-6.959959983825684\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1037--TrainLoss:97.0763931274414--TrainR2:-5.3775153160095215--TestLoss:106.17716979980469--TestR2:-6.958578586578369\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1038--TrainLoss:97.05909729003906--TrainR2:-5.376379013061523--TestLoss:106.15875244140625--TestR2:-6.957198143005371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1039--TrainLoss:97.041748046875--TrainR2:-5.375239372253418--TestLoss:106.14032745361328--TestR2:-6.955816745758057\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1040--TrainLoss:97.02449035644531--TrainR2:-5.374105453491211--TestLoss:106.12191009521484--TestR2:-6.954436302185059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1041--TrainLoss:97.00720977783203--TrainR2:-5.372970104217529--TestLoss:106.10347747802734--TestR2:-6.953054904937744\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1042--TrainLoss:96.98990631103516--TrainR2:-5.371833801269531--TestLoss:106.08507537841797--TestR2:-6.9516754150390625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1043--TrainLoss:96.97260284423828--TrainR2:-5.370697021484375--TestLoss:106.06665802001953--TestR2:-6.9502949714660645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1044--TrainLoss:96.95532989501953--TrainR2:-5.369562149047852--TestLoss:106.04822540283203--TestR2:-6.94891357421875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1045--TrainLoss:96.93804168701172--TrainR2:-5.368426322937012--TestLoss:106.02983856201172--TestR2:-6.947535037994385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1046--TrainLoss:96.92070770263672--TrainR2:-5.3672871589660645--TestLoss:106.01142120361328--TestR2:-6.946154594421387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1047--TrainLoss:96.90345764160156--TrainR2:-5.366154193878174--TestLoss:105.99300384521484--TestR2:-6.944774150848389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1048--TrainLoss:96.88622283935547--TrainR2:-5.365021705627441--TestLoss:105.974609375--TestR2:-6.943395614624023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1049--TrainLoss:96.86888885498047--TrainR2:-5.3638834953308105--TestLoss:105.95619201660156--TestR2:-6.942015171051025\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1050--TrainLoss:96.85161590576172--TrainR2:-5.362748146057129--TestLoss:105.93781280517578--TestR2:-6.940637588500977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1051--TrainLoss:96.8343276977539--TrainR2:-5.361612796783447--TestLoss:105.91941833496094--TestR2:-6.939259052276611\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1052--TrainLoss:96.81709289550781--TrainR2:-5.360480308532715--TestLoss:105.90101623535156--TestR2:-6.93787956237793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1053--TrainLoss:96.7998275756836--TrainR2:-5.35934591293335--TestLoss:105.88262176513672--TestR2:-6.9365010261535645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1054--TrainLoss:96.78254699707031--TrainR2:-5.358210563659668--TestLoss:105.86424255371094--TestR2:-6.935123443603516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1055--TrainLoss:96.7652816772461--TrainR2:-5.357076644897461--TestLoss:105.84586334228516--TestR2:-6.933745861053467\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1056--TrainLoss:96.74803161621094--TrainR2:-5.35594367980957--TestLoss:105.82747650146484--TestR2:-6.932366847991943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1057--TrainLoss:96.73075866699219--TrainR2:-5.354808807373047--TestLoss:105.8091049194336--TestR2:-6.930990219116211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1058--TrainLoss:96.71351623535156--TrainR2:-5.353675842285156--TestLoss:105.79070281982422--TestR2:-6.929610729217529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1059--TrainLoss:96.69625854492188--TrainR2:-5.352541923522949--TestLoss:105.77232360839844--TestR2:-6.9282331466674805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1060--TrainLoss:96.67899322509766--TrainR2:-5.351407527923584--TestLoss:105.75395965576172--TestR2:-6.926856517791748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1061--TrainLoss:96.66173553466797--TrainR2:-5.350273609161377--TestLoss:105.73558044433594--TestR2:-6.925478935241699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1062--TrainLoss:96.6445083618164--TrainR2:-5.349142074584961--TestLoss:105.71722412109375--TestR2:-6.924103260040283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1063--TrainLoss:96.62727355957031--TrainR2:-5.348010063171387--TestLoss:105.69886779785156--TestR2:-6.922727108001709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1064--TrainLoss:96.61003875732422--TrainR2:-5.346877574920654--TestLoss:105.68050384521484--TestR2:-6.921350479125977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1065--TrainLoss:96.59280395507812--TrainR2:-5.34574556350708--TestLoss:105.6621322631836--TestR2:-6.919973850250244\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1066--TrainLoss:96.57556915283203--TrainR2:-5.344613552093506--TestLoss:105.64378356933594--TestR2:-6.918598175048828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1067--TrainLoss:96.55831909179688--TrainR2:-5.343480110168457--TestLoss:105.62543487548828--TestR2:-6.9172234535217285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1068--TrainLoss:96.5411148071289--TrainR2:-5.342349529266357--TestLoss:105.6070556640625--TestR2:-6.91584587097168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1069--TrainLoss:96.52384948730469--TrainR2:-5.341215133666992--TestLoss:105.58871459960938--TestR2:-6.914470672607422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1070--TrainLoss:96.50662231445312--TrainR2:-5.340084075927734--TestLoss:105.57037353515625--TestR2:-6.913095951080322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1071--TrainLoss:96.48944091796875--TrainR2:-5.338954925537109--TestLoss:105.55201721191406--TestR2:-6.911720275878906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1072--TrainLoss:96.47220611572266--TrainR2:-5.337822437286377--TestLoss:105.53367614746094--TestR2:-6.910345554351807\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1073--TrainLoss:96.4549560546875--TrainR2:-5.336689472198486--TestLoss:105.51533508300781--TestR2:-6.908970832824707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1074--TrainLoss:96.43775939941406--TrainR2:-5.335559844970703--TestLoss:105.49699401855469--TestR2:-6.907595634460449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1075--TrainLoss:96.42053985595703--TrainR2:-5.334428310394287--TestLoss:105.47867584228516--TestR2:-6.906222820281982\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1076--TrainLoss:96.40333557128906--TrainR2:-5.333298206329346--TestLoss:105.46033477783203--TestR2:-6.904848098754883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1077--TrainLoss:96.38615417480469--TrainR2:-5.332169532775879--TestLoss:105.4419937133789--TestR2:-6.903473377227783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1078--TrainLoss:96.36891174316406--TrainR2:-5.331036567687988--TestLoss:105.42369079589844--TestR2:-6.902101039886475\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1079--TrainLoss:96.35171508789062--TrainR2:-5.329906940460205--TestLoss:105.40534973144531--TestR2:-6.900726318359375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1080--TrainLoss:96.33454132080078--TrainR2:-5.3287787437438965--TestLoss:105.38703155517578--TestR2:-6.899353504180908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1081--TrainLoss:96.31732177734375--TrainR2:-5.3276472091674805--TestLoss:105.36871337890625--TestR2:-6.897980690002441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1082--TrainLoss:96.30009460449219--TrainR2:-5.3265156745910645--TestLoss:105.35038757324219--TestR2:-6.8966064453125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1083--TrainLoss:96.2829360961914--TrainR2:-5.325388431549072--TestLoss:105.33208465576172--TestR2:-6.89523458480835\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1084--TrainLoss:96.26573181152344--TrainR2:-5.324258327484131--TestLoss:105.31376647949219--TestR2:-6.893861770629883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1085--TrainLoss:96.2485580444336--TrainR2:-5.323130130767822--TestLoss:105.29546356201172--TestR2:-6.892489433288574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1086--TrainLoss:96.23138427734375--TrainR2:-5.3220014572143555--TestLoss:105.27715301513672--TestR2:-6.891117572784424\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1087--TrainLoss:96.21417236328125--TrainR2:-5.320870876312256--TestLoss:105.25882720947266--TestR2:-6.889743804931641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1088--TrainLoss:96.19700622558594--TrainR2:-5.3197431564331055--TestLoss:105.24053192138672--TestR2:-6.888372421264648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1089--TrainLoss:96.17982482910156--TrainR2:-5.318614482879639--TestLoss:105.22224426269531--TestR2:-6.8870015144348145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1090--TrainLoss:96.16265106201172--TrainR2:-5.31748628616333--TestLoss:105.20394134521484--TestR2:-6.885629653930664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1091--TrainLoss:96.1454849243164--TrainR2:-5.3163580894470215--TestLoss:105.1856460571289--TestR2:-6.884258270263672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1092--TrainLoss:96.1283187866211--TrainR2:-5.315230846405029--TestLoss:105.16736602783203--TestR2:-6.882888317108154\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1093--TrainLoss:96.11107635498047--TrainR2:-5.314097881317139--TestLoss:105.14906311035156--TestR2:-6.881516456604004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1094--TrainLoss:96.09395599365234--TrainR2:-5.3129730224609375--TestLoss:105.13077545166016--TestR2:-6.880145072937012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1095--TrainLoss:96.0768051147461--TrainR2:-5.3118462562561035--TestLoss:105.11248016357422--TestR2:-6.878774166107178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1096--TrainLoss:96.05962371826172--TrainR2:-5.310717582702637--TestLoss:105.0942153930664--TestR2:-6.877405166625977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1097--TrainLoss:96.04249572753906--TrainR2:-5.3095927238464355--TestLoss:105.07593536376953--TestR2:-6.876034736633301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1098--TrainLoss:96.02533721923828--TrainR2:-5.308465480804443--TestLoss:105.05765533447266--TestR2:-6.874664783477783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1099--TrainLoss:96.00817108154297--TrainR2:-5.307337284088135--TestLoss:105.03937530517578--TestR2:-6.873294830322266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1100--TrainLoss:95.99100494384766--TrainR2:-5.306210041046143--TestLoss:105.02110290527344--TestR2:-6.871924877166748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1101--TrainLoss:95.97386932373047--TrainR2:-5.305084228515625--TestLoss:105.00283813476562--TestR2:-6.870555877685547\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1102--TrainLoss:95.95672607421875--TrainR2:-5.303957939147949--TestLoss:104.98457336425781--TestR2:-6.869186878204346\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1103--TrainLoss:95.93958282470703--TrainR2:-5.302831649780273--TestLoss:104.96629333496094--TestR2:-6.867816925048828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1104--TrainLoss:95.92245483398438--TrainR2:-5.301706314086914--TestLoss:104.94805145263672--TestR2:-6.866449356079102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1105--TrainLoss:95.90528106689453--TrainR2:-5.3005781173706055--TestLoss:104.9297866821289--TestR2:-6.8650803565979\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1106--TrainLoss:95.88817596435547--TrainR2:-5.299454212188721--TestLoss:104.91152954101562--TestR2:-6.863711833953857\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1107--TrainLoss:95.87101745605469--TrainR2:-5.2983269691467285--TestLoss:104.89326477050781--TestR2:-6.862342834472656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1108--TrainLoss:95.85388946533203--TrainR2:-5.297202110290527--TestLoss:104.87501525878906--TestR2:-6.8609747886657715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1109--TrainLoss:95.83677673339844--TrainR2:-5.296077251434326--TestLoss:104.85676574707031--TestR2:-6.859606742858887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1110--TrainLoss:95.81962585449219--TrainR2:-5.29495096206665--TestLoss:104.8385238647461--TestR2:-6.858239650726318\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1111--TrainLoss:95.80252075195312--TrainR2:-5.293827056884766--TestLoss:104.82028198242188--TestR2:-6.856872081756592\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1112--TrainLoss:95.78540802001953--TrainR2:-5.292703151702881--TestLoss:104.80204010009766--TestR2:-6.855504989624023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1113--TrainLoss:95.76826477050781--TrainR2:-5.291576862335205--TestLoss:104.7837905883789--TestR2:-6.854136943817139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1114--TrainLoss:95.75111389160156--TrainR2:-5.290449619293213--TestLoss:104.76556396484375--TestR2:-6.852770805358887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1115--TrainLoss:95.7340087890625--TrainR2:-5.289326190948486--TestLoss:104.74732208251953--TestR2:-6.85140323638916\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1116--TrainLoss:95.71693420410156--TrainR2:-5.288204193115234--TestLoss:104.72908782958984--TestR2:-6.850037097930908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1117--TrainLoss:95.69982147216797--TrainR2:-5.28708028793335--TestLoss:104.71086120605469--TestR2:-6.848670482635498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1118--TrainLoss:95.6827163696289--TrainR2:-5.285956382751465--TestLoss:104.69261932373047--TestR2:-6.84730339050293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1119--TrainLoss:95.66560363769531--TrainR2:-5.28483247756958--TestLoss:104.67439270019531--TestR2:-6.8459367752075195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1120--TrainLoss:95.64852905273438--TrainR2:-5.283710479736328--TestLoss:104.65618896484375--TestR2:-6.8445725440979\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1121--TrainLoss:95.63139343261719--TrainR2:-5.282585144042969--TestLoss:104.63795471191406--TestR2:-6.84320592880249\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1122--TrainLoss:95.6142807006836--TrainR2:-5.281460762023926--TestLoss:104.6197280883789--TestR2:-6.841839790344238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1123--TrainLoss:95.59719848632812--TrainR2:-5.280338287353516--TestLoss:104.60153198242188--TestR2:-6.840475559234619\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1124--TrainLoss:95.58013153076172--TrainR2:-5.279216766357422--TestLoss:104.58330535888672--TestR2:-6.839109420776367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1125--TrainLoss:95.56304931640625--TrainR2:-5.27809476852417--TestLoss:104.56510162353516--TestR2:-6.83774471282959\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1126--TrainLoss:95.54594421386719--TrainR2:-5.276970863342285--TestLoss:104.54689025878906--TestR2:-6.836380481719971\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1127--TrainLoss:95.52886199951172--TrainR2:-5.275848865509033--TestLoss:104.52869415283203--TestR2:-6.835016250610352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1128--TrainLoss:95.51178741455078--TrainR2:-5.2747273445129395--TestLoss:104.51048278808594--TestR2:-6.833651542663574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1129--TrainLoss:95.49470520019531--TrainR2:-5.273604869842529--TestLoss:104.49227905273438--TestR2:-6.832286834716797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1130--TrainLoss:95.47761535644531--TrainR2:-5.272482395172119--TestLoss:104.47408294677734--TestR2:-6.830922603607178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1131--TrainLoss:95.4605484008789--TrainR2:-5.271360874176025--TestLoss:104.45589447021484--TestR2:-6.829559326171875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1132--TrainLoss:95.44345092773438--TrainR2:-5.270237445831299--TestLoss:104.43770599365234--TestR2:-6.828196048736572\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1133--TrainLoss:95.42639923095703--TrainR2:-5.269117832183838--TestLoss:104.41950988769531--TestR2:-6.826832294464111\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1134--TrainLoss:95.40933227539062--TrainR2:-5.267996311187744--TestLoss:104.40132904052734--TestR2:-6.825469493865967\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1135--TrainLoss:95.39222717285156--TrainR2:-5.266872406005859--TestLoss:104.38314056396484--TestR2:-6.824106216430664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1136--TrainLoss:95.37516784667969--TrainR2:-5.265751838684082--TestLoss:104.36494445800781--TestR2:-6.822742462158203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1137--TrainLoss:95.35813903808594--TrainR2:-5.2646331787109375--TestLoss:104.34678649902344--TestR2:-6.821381092071533\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1138--TrainLoss:95.34107971191406--TrainR2:-5.26351261138916--TestLoss:104.32859802246094--TestR2:-6.8200178146362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1139--TrainLoss:95.32400512695312--TrainR2:-5.262391090393066--TestLoss:104.31043243408203--TestR2:-6.8186564445495605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1140--TrainLoss:95.30696868896484--TrainR2:-5.2612714767456055--TestLoss:104.29226684570312--TestR2:-6.817294597625732\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1141--TrainLoss:95.28992462158203--TrainR2:-5.2601518630981445--TestLoss:104.27407836914062--TestR2:-6.81593132019043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1142--TrainLoss:95.27288055419922--TrainR2:-5.259032249450684--TestLoss:104.25591278076172--TestR2:-6.81456995010376\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1143--TrainLoss:95.2558364868164--TrainR2:-5.2579121589660645--TestLoss:104.23773956298828--TestR2:-6.813207626342773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1144--TrainLoss:95.23880004882812--TrainR2:-5.256793022155762--TestLoss:104.21958923339844--TestR2:-6.81184720993042\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1145--TrainLoss:95.22175598144531--TrainR2:-5.255673408508301--TestLoss:104.20142364501953--TestR2:-6.810485363006592\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1146--TrainLoss:95.20472717285156--TrainR2:-5.254554748535156--TestLoss:104.18326568603516--TestR2:-6.80912446975708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1147--TrainLoss:95.18768310546875--TrainR2:-5.253435134887695--TestLoss:104.16510772705078--TestR2:-6.80776309967041\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1148--TrainLoss:95.17066192626953--TrainR2:-5.252316474914551--TestLoss:104.14696502685547--TestR2:-6.806403636932373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1149--TrainLoss:95.15359497070312--TrainR2:-5.251195430755615--TestLoss:104.12882232666016--TestR2:-6.8050432205200195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1150--TrainLoss:95.13655853271484--TrainR2:-5.2500762939453125--TestLoss:104.11067199707031--TestR2:-6.803682804107666\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1151--TrainLoss:95.11954498291016--TrainR2:-5.248958587646484--TestLoss:104.0925064086914--TestR2:-6.802321434020996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1152--TrainLoss:95.10250854492188--TrainR2:-5.247838973999023--TestLoss:104.07437896728516--TestR2:-6.800962924957275\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1153--TrainLoss:95.08549499511719--TrainR2:-5.2467217445373535--TestLoss:104.05622863769531--TestR2:-6.799602031707764\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1154--TrainLoss:95.06847381591797--TrainR2:-5.245603561401367--TestLoss:104.03810119628906--TestR2:-6.798243045806885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1155--TrainLoss:95.05143737792969--TrainR2:-5.2444844245910645--TestLoss:104.01994323730469--TestR2:-6.796882629394531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1156--TrainLoss:95.0344009399414--TrainR2:-5.2433648109436035--TestLoss:104.0018310546875--TestR2:-6.795524597167969\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1157--TrainLoss:95.01741790771484--TrainR2:-5.242249011993408--TestLoss:103.98369598388672--TestR2:-6.79416561126709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1158--TrainLoss:95.00041961669922--TrainR2:-5.241132736206055--TestLoss:103.96556854248047--TestR2:-6.792806625366211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1159--TrainLoss:94.9834213256836--TrainR2:-5.240015983581543--TestLoss:103.94744110107422--TestR2:-6.79144811630249\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1160--TrainLoss:94.96642303466797--TrainR2:-5.238898754119873--TestLoss:103.9293212890625--TestR2:-6.7900896072387695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1161--TrainLoss:94.94941711425781--TrainR2:-5.237782001495361--TestLoss:103.91120910644531--TestR2:-6.788732051849365\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1162--TrainLoss:94.93241119384766--TrainR2:-5.236664772033691--TestLoss:103.89307403564453--TestR2:-6.787372589111328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1163--TrainLoss:94.91541290283203--TrainR2:-5.23554801940918--TestLoss:103.87496185302734--TestR2:-6.786015510559082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1164--TrainLoss:94.8984146118164--TrainR2:-5.23443078994751--TestLoss:103.85684967041016--TestR2:-6.784657955169678\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1165--TrainLoss:94.88142395019531--TrainR2:-5.2333149909973145--TestLoss:103.8387451171875--TestR2:-6.783300399780273\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1166--TrainLoss:94.86444091796875--TrainR2:-5.232199192047119--TestLoss:103.82064056396484--TestR2:-6.781943321228027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1167--TrainLoss:94.84745788574219--TrainR2:-5.231083393096924--TestLoss:103.80252838134766--TestR2:-6.780585765838623\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1168--TrainLoss:94.83045196533203--TrainR2:-5.229966163635254--TestLoss:103.78443145751953--TestR2:-6.779229164123535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1169--TrainLoss:94.81349182128906--TrainR2:-5.228851795196533--TestLoss:103.7663345336914--TestR2:-6.7778730392456055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1170--TrainLoss:94.79650115966797--TrainR2:-5.22773551940918--TestLoss:103.74821472167969--TestR2:-6.776515007019043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1171--TrainLoss:94.779541015625--TrainR2:-5.226621627807617--TestLoss:103.73014068603516--TestR2:-6.775160312652588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1172--TrainLoss:94.7625503540039--TrainR2:-5.225505352020264--TestLoss:103.71205139160156--TestR2:-6.773804187774658\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1173--TrainLoss:94.7455825805664--TrainR2:-5.224390983581543--TestLoss:103.6939468383789--TestR2:-6.772447109222412\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1174--TrainLoss:94.7286148071289--TrainR2:-5.223276138305664--TestLoss:103.67586517333984--TestR2:-6.771091461181641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1175--TrainLoss:94.71160888671875--TrainR2:-5.222158908843994--TestLoss:103.65779113769531--TestR2:-6.7697367668151855\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1176--TrainLoss:94.69464874267578--TrainR2:-5.221044540405273--TestLoss:103.63970947265625--TestR2:-6.7683820724487305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1177--TrainLoss:94.6777114868164--TrainR2:-5.2199320793151855--TestLoss:103.6216049194336--TestR2:-6.767024993896484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1178--TrainLoss:94.6607437133789--TrainR2:-5.218817234039307--TestLoss:103.60353088378906--TestR2:-6.765669822692871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1179--TrainLoss:94.64383697509766--TrainR2:-5.217706203460693--TestLoss:103.58546447753906--TestR2:-6.764315605163574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1180--TrainLoss:94.62686920166016--TrainR2:-5.216591835021973--TestLoss:103.56739044189453--TestR2:-6.762960910797119\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1181--TrainLoss:94.60990905761719--TrainR2:-5.215477466583252--TestLoss:103.54930877685547--TestR2:-6.761605739593506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1182--TrainLoss:94.59297943115234--TrainR2:-5.214365005493164--TestLoss:103.53123474121094--TestR2:-6.760251045227051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1183--TrainLoss:94.57601928710938--TrainR2:-5.213251113891602--TestLoss:103.51317596435547--TestR2:-6.758897304534912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1184--TrainLoss:94.55905151367188--TrainR2:-5.212136745452881--TestLoss:103.4951171875--TestR2:-6.757543563842773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1185--TrainLoss:94.5421142578125--TrainR2:-5.211023807525635--TestLoss:103.47705078125--TestR2:-6.756189346313477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1186--TrainLoss:94.5251693725586--TrainR2:-5.2099103927612305--TestLoss:103.4590072631836--TestR2:-6.7548370361328125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1187--TrainLoss:94.50822448730469--TrainR2:-5.208797454833984--TestLoss:103.44093322753906--TestR2:-6.753482818603516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1188--TrainLoss:94.49130249023438--TrainR2:-5.207685470581055--TestLoss:103.42288970947266--TestR2:-6.752130031585693\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1189--TrainLoss:94.474365234375--TrainR2:-5.206573009490967--TestLoss:103.40483856201172--TestR2:-6.750776767730713\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1190--TrainLoss:94.45744323730469--TrainR2:-5.205461502075195--TestLoss:103.38679504394531--TestR2:-6.749424457550049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1191--TrainLoss:94.4405288696289--TrainR2:-5.204349517822266--TestLoss:103.36875915527344--TestR2:-6.748072624206543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1192--TrainLoss:94.423583984375--TrainR2:-5.203237056732178--TestLoss:103.35070037841797--TestR2:-6.746718883514404\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1193--TrainLoss:94.40664672851562--TrainR2:-5.202124118804932--TestLoss:103.3326644897461--TestR2:-6.745367050170898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1194--TrainLoss:94.38969421386719--TrainR2:-5.201010227203369--TestLoss:103.31463623046875--TestR2:-6.744015693664551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1195--TrainLoss:94.37274169921875--TrainR2:-5.199896335601807--TestLoss:103.29657745361328--TestR2:-6.74266242980957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1196--TrainLoss:94.35588073730469--TrainR2:-5.198788642883301--TestLoss:103.27854919433594--TestR2:-6.7413105964660645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1197--TrainLoss:94.3389892578125--TrainR2:-5.197679042816162--TestLoss:103.26053619384766--TestR2:-6.739960670471191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1198--TrainLoss:94.32207489013672--TrainR2:-5.196568012237549--TestLoss:103.24250030517578--TestR2:-6.7386088371276855\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1199--TrainLoss:94.30516052246094--TrainR2:-5.1954569816589355--TestLoss:103.22445678710938--TestR2:-6.7372565269470215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1200--TrainLoss:94.28825378417969--TrainR2:-5.194345951080322--TestLoss:103.2064437866211--TestR2:-6.73590612411499\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1201--TrainLoss:94.27134704589844--TrainR2:-5.193235874176025--TestLoss:103.18843078613281--TestR2:-6.734556198120117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1202--TrainLoss:94.25443267822266--TrainR2:-5.192124366760254--TestLoss:103.17040252685547--TestR2:-6.7332048416137695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1203--TrainLoss:94.23755645751953--TrainR2:-5.191015720367432--TestLoss:103.15239715576172--TestR2:-6.7318549156188965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1204--TrainLoss:94.22064208984375--TrainR2:-5.189904689788818--TestLoss:103.13436126708984--TestR2:-6.730503559112549\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1205--TrainLoss:94.2037353515625--TrainR2:-5.188793659210205--TestLoss:103.11637115478516--TestR2:-6.729154109954834\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1206--TrainLoss:94.18685913085938--TrainR2:-5.187685012817383--TestLoss:103.09834289550781--TestR2:-6.7278032302856445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1207--TrainLoss:94.16997528076172--TrainR2:-5.186575412750244--TestLoss:103.0803451538086--TestR2:-6.726454257965088\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1208--TrainLoss:94.15311431884766--TrainR2:-5.1854681968688965--TestLoss:103.06233978271484--TestR2:-6.725104808807373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1209--TrainLoss:94.13617706298828--TrainR2:-5.18435525894165--TestLoss:103.04434967041016--TestR2:-6.723755836486816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1210--TrainLoss:94.11931610107422--TrainR2:-5.1832475662231445--TestLoss:103.02633666992188--TestR2:-6.722405910491943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1211--TrainLoss:94.10244750976562--TrainR2:-5.1821393966674805--TestLoss:103.00834655761719--TestR2:-6.721057415008545\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1212--TrainLoss:94.0855484008789--TrainR2:-5.181029319763184--TestLoss:102.99034881591797--TestR2:-6.719708442687988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1213--TrainLoss:94.06865692138672--TrainR2:-5.179919719696045--TestLoss:102.97234344482422--TestR2:-6.718358993530273\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1214--TrainLoss:94.0517578125--TrainR2:-5.17880916595459--TestLoss:102.9543685913086--TestR2:-6.71701192855835\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1215--TrainLoss:94.03491973876953--TrainR2:-5.177703380584717--TestLoss:102.93638610839844--TestR2:-6.715663433074951\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1216--TrainLoss:94.01808166503906--TrainR2:-5.1765971183776855--TestLoss:102.91840362548828--TestR2:-6.714315891265869\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1217--TrainLoss:94.001220703125--TrainR2:-5.1754889488220215--TestLoss:102.9004135131836--TestR2:-6.712967395782471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1218--TrainLoss:93.9843521118164--TrainR2:-5.174380779266357--TestLoss:102.88243103027344--TestR2:-6.7116193771362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1219--TrainLoss:93.96748352050781--TrainR2:-5.173272609710693--TestLoss:102.86444854736328--TestR2:-6.71027135848999\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1220--TrainLoss:93.95061492919922--TrainR2:-5.1721649169921875--TestLoss:102.84647369384766--TestR2:-6.708924293518066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1221--TrainLoss:93.93376159667969--TrainR2:-5.171057224273682--TestLoss:102.82850646972656--TestR2:-6.707577705383301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1222--TrainLoss:93.9168930053711--TrainR2:-5.169949531555176--TestLoss:102.81053161621094--TestR2:-6.706230163574219\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1223--TrainLoss:93.90007019042969--TrainR2:-5.168844223022461--TestLoss:102.79257202148438--TestR2:-6.704884052276611\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1224--TrainLoss:93.88323211669922--TrainR2:-5.16773796081543--TestLoss:102.77461242675781--TestR2:-6.703537940979004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1225--TrainLoss:93.86637878417969--TrainR2:-5.166630744934082--TestLoss:102.75666046142578--TestR2:-6.702192306518555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1226--TrainLoss:93.84954071044922--TrainR2:-5.165524482727051--TestLoss:102.73867797851562--TestR2:-6.7008442878723145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1227--TrainLoss:93.83268737792969--TrainR2:-5.164417266845703--TestLoss:102.7207260131836--TestR2:-6.699499130249023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1228--TrainLoss:93.81584167480469--TrainR2:-5.163311004638672--TestLoss:102.70277404785156--TestR2:-6.698153018951416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1229--TrainLoss:93.79898071289062--TrainR2:-5.162202835083008--TestLoss:102.68480682373047--TestR2:-6.69680643081665\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1230--TrainLoss:93.78218078613281--TrainR2:-5.161099433898926--TestLoss:102.6668701171875--TestR2:-6.695462226867676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1231--TrainLoss:93.76536560058594--TrainR2:-5.159994602203369--TestLoss:102.64894104003906--TestR2:-6.694118499755859\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1232--TrainLoss:93.74852752685547--TrainR2:-5.158888339996338--TestLoss:102.6309814453125--TestR2:-6.6927714347839355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1233--TrainLoss:93.73167419433594--TrainR2:-5.15778112411499--TestLoss:102.61302185058594--TestR2:-6.691425800323486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1234--TrainLoss:93.71485137939453--TrainR2:-5.156675815582275--TestLoss:102.5950927734375--TestR2:-6.69008207321167\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1235--TrainLoss:93.69802856445312--TrainR2:-5.1555705070495605--TestLoss:102.57715606689453--TestR2:-6.688737392425537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1236--TrainLoss:93.68122863769531--TrainR2:-5.1544671058654785--TestLoss:102.55921173095703--TestR2:-6.687392234802246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1237--TrainLoss:93.6644058227539--TrainR2:-5.153361797332764--TestLoss:102.54127502441406--TestR2:-6.6860480308532715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1238--TrainLoss:93.64759063720703--TrainR2:-5.152256965637207--TestLoss:102.52335357666016--TestR2:-6.684704780578613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1239--TrainLoss:93.6307601928711--TrainR2:-5.151151657104492--TestLoss:102.50543212890625--TestR2:-6.683361530303955\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1240--TrainLoss:93.61394500732422--TrainR2:-5.1500468254089355--TestLoss:102.48750305175781--TestR2:-6.6820173263549805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1241--TrainLoss:93.5971450805664--TrainR2:-5.1489434242248535--TestLoss:102.46957397460938--TestR2:-6.680673599243164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1242--TrainLoss:93.58035278320312--TrainR2:-5.1478400230407715--TestLoss:102.45165252685547--TestR2:-6.679330348968506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1243--TrainLoss:93.56353759765625--TrainR2:-5.146735668182373--TestLoss:102.43374633789062--TestR2:-6.677988052368164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1244--TrainLoss:93.54673767089844--TrainR2:-5.145631790161133--TestLoss:102.41583251953125--TestR2:-6.676645278930664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1245--TrainLoss:93.52993774414062--TrainR2:-5.144528388977051--TestLoss:102.39791870117188--TestR2:-6.675302505493164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1246--TrainLoss:93.51312255859375--TrainR2:-5.143423080444336--TestLoss:102.3800048828125--TestR2:-6.673960208892822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1247--TrainLoss:93.49636840820312--TrainR2:-5.142322540283203--TestLoss:102.36209869384766--TestR2:-6.672617435455322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1248--TrainLoss:93.47958374023438--TrainR2:-5.141219615936279--TestLoss:102.34418487548828--TestR2:-6.671274662017822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1249--TrainLoss:93.46276092529297--TrainR2:-5.140114784240723--TestLoss:102.32628631591797--TestR2:-6.669933319091797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1250--TrainLoss:93.44593811035156--TrainR2:-5.139009475708008--TestLoss:102.30838012695312--TestR2:-6.668591022491455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1251--TrainLoss:93.42918395996094--TrainR2:-5.137908935546875--TestLoss:102.29048919677734--TestR2:-6.667250156402588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1252--TrainLoss:93.41243743896484--TrainR2:-5.136808395385742--TestLoss:102.27259063720703--TestR2:-6.6659088134765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1253--TrainLoss:93.39563751220703--TrainR2:-5.13570499420166--TestLoss:102.25472259521484--TestR2:-6.664568901062012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1254--TrainLoss:93.37887573242188--TrainR2:-5.134603500366211--TestLoss:102.23681640625--TestR2:-6.663227081298828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1255--TrainLoss:93.36207580566406--TrainR2:-5.133500099182129--TestLoss:102.21891021728516--TestR2:-6.661884784698486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1256--TrainLoss:93.34532928466797--TrainR2:-5.132399559020996--TestLoss:102.20103454589844--TestR2:-6.6605448722839355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1257--TrainLoss:93.32857513427734--TrainR2:-5.1312994956970215--TestLoss:102.18315124511719--TestR2:-6.659204483032227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1258--TrainLoss:93.3117904663086--TrainR2:-5.130196571350098--TestLoss:102.1652603149414--TestR2:-6.657863140106201\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1259--TrainLoss:93.2950210571289--TrainR2:-5.12909460067749--TestLoss:102.14738464355469--TestR2:-6.656523704528809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1260--TrainLoss:93.27825164794922--TrainR2:-5.127993106842041--TestLoss:102.1295166015625--TestR2:-6.655183792114258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1261--TrainLoss:93.26150512695312--TrainR2:-5.126893043518066--TestLoss:102.11163330078125--TestR2:-6.653843879699707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1262--TrainLoss:93.24474334716797--TrainR2:-5.125791549682617--TestLoss:102.0937728881836--TestR2:-6.652505397796631\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1263--TrainLoss:93.22798919677734--TrainR2:-5.124691486358643--TestLoss:102.07591247558594--TestR2:-6.651165962219238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1264--TrainLoss:93.21121215820312--TrainR2:-5.123589038848877--TestLoss:102.05802917480469--TestR2:-6.6498260498046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1265--TrainLoss:93.19448852539062--TrainR2:-5.122490406036377--TestLoss:102.0401611328125--TestR2:-6.648486614227295\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1266--TrainLoss:93.177734375--TrainR2:-5.121389389038086--TestLoss:102.0223159790039--TestR2:-6.647149085998535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1267--TrainLoss:93.16096496582031--TrainR2:-5.120287895202637--TestLoss:102.00444793701172--TestR2:-6.645809650421143\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1268--TrainLoss:93.14423370361328--TrainR2:-5.1191887855529785--TestLoss:101.98658752441406--TestR2:-6.644471168518066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1269--TrainLoss:93.12750244140625--TrainR2:-5.11808967590332--TestLoss:101.96873474121094--TestR2:-6.64313268661499\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1270--TrainLoss:93.11076354980469--TrainR2:-5.116989612579346--TestLoss:101.95088958740234--TestR2:-6.6417951583862305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1271--TrainLoss:93.09404754638672--TrainR2:-5.115891933441162--TestLoss:101.93302154541016--TestR2:-6.640456199645996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1272--TrainLoss:93.07728576660156--TrainR2:-5.114790439605713--TestLoss:101.91519165039062--TestR2:-6.6391191482543945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1273--TrainLoss:93.06057739257812--TrainR2:-5.113692760467529--TestLoss:101.8973388671875--TestR2:-6.637781143188477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1274--TrainLoss:93.04383850097656--TrainR2:-5.112593173980713--TestLoss:101.87950897216797--TestR2:-6.636444568634033\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1275--TrainLoss:93.0271224975586--TrainR2:-5.111495018005371--TestLoss:101.86165618896484--TestR2:-6.635106563568115\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1276--TrainLoss:93.01042175292969--TrainR2:-5.110397815704346--TestLoss:101.84382629394531--TestR2:-6.633769989013672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1277--TrainLoss:92.99369049072266--TrainR2:-5.1092987060546875--TestLoss:101.82598114013672--TestR2:-6.63243293762207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1278--TrainLoss:92.97694396972656--TrainR2:-5.108198642730713--TestLoss:101.80815887451172--TestR2:-6.631096839904785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1279--TrainLoss:92.96022033691406--TrainR2:-5.107100009918213--TestLoss:101.79033660888672--TestR2:-6.6297607421875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1280--TrainLoss:92.94351196289062--TrainR2:-5.106002330780029--TestLoss:101.7724838256836--TestR2:-6.628422737121582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1281--TrainLoss:92.92681884765625--TrainR2:-5.104905128479004--TestLoss:101.7546615600586--TestR2:-6.627087116241455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1282--TrainLoss:92.91009521484375--TrainR2:-5.103806972503662--TestLoss:101.73683166503906--TestR2:-6.625750541687012\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1283--TrainLoss:92.89337921142578--TrainR2:-5.102708339691162--TestLoss:101.7190170288086--TestR2:-6.624414920806885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1284--TrainLoss:92.8767318725586--TrainR2:-5.101614952087402--TestLoss:101.70122528076172--TestR2:-6.623081207275391\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1285--TrainLoss:92.8599853515625--TrainR2:-5.100514888763428--TestLoss:101.68338775634766--TestR2:-6.621744155883789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1286--TrainLoss:92.84327697753906--TrainR2:-5.099417209625244--TestLoss:101.66558074951172--TestR2:-6.620409965515137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1287--TrainLoss:92.82662200927734--TrainR2:-5.098322868347168--TestLoss:101.64778900146484--TestR2:-6.619075775146484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1288--TrainLoss:92.80989074707031--TrainR2:-5.09722375869751--TestLoss:101.62993621826172--TestR2:-6.617738246917725\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1289--TrainLoss:92.79319763183594--TrainR2:-5.096127033233643--TestLoss:101.61215209960938--TestR2:-6.616405010223389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1290--TrainLoss:92.77655029296875--TrainR2:-5.095033645629883--TestLoss:101.5943603515625--TestR2:-6.615070819854736\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1291--TrainLoss:92.75983428955078--TrainR2:-5.093935489654541--TestLoss:101.5765609741211--TestR2:-6.613737106323242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1292--TrainLoss:92.74314880371094--TrainR2:-5.092839241027832--TestLoss:101.55875396728516--TestR2:-6.612401962280273\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1293--TrainLoss:92.72647857666016--TrainR2:-5.0917439460754395--TestLoss:101.54095458984375--TestR2:-6.611068248748779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1294--TrainLoss:92.70978546142578--TrainR2:-5.0906476974487305--TestLoss:101.52315521240234--TestR2:-6.609734058380127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1295--TrainLoss:92.69309997558594--TrainR2:-5.089550971984863--TestLoss:101.50537109375--TestR2:-6.608400821685791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1296--TrainLoss:92.67639923095703--TrainR2:-5.088454246520996--TestLoss:101.48757934570312--TestR2:-6.607067584991455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1297--TrainLoss:92.65977478027344--TrainR2:-5.087361812591553--TestLoss:101.46978759765625--TestR2:-6.605733871459961\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1298--TrainLoss:92.6430892944336--TrainR2:-5.086265563964844--TestLoss:101.45199584960938--TestR2:-6.604400634765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1299--TrainLoss:92.62639617919922--TrainR2:-5.085169315338135--TestLoss:101.43423461914062--TestR2:-6.603068828582764\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1300--TrainLoss:92.60977172851562--TrainR2:-5.084076881408691--TestLoss:101.41644287109375--TestR2:-6.6017351150512695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1301--TrainLoss:92.59307098388672--TrainR2:-5.082979679107666--TestLoss:101.39866638183594--TestR2:-6.600403308868408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1302--TrainLoss:92.57640838623047--TrainR2:-5.081884860992432--TestLoss:101.38091278076172--TestR2:-6.599072456359863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1303--TrainLoss:92.55977630615234--TrainR2:-5.080792427062988--TestLoss:101.36312866210938--TestR2:-6.597739219665527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1304--TrainLoss:92.54309844970703--TrainR2:-5.0796966552734375--TestLoss:101.34536743164062--TestR2:-6.596407890319824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1305--TrainLoss:92.52643585205078--TrainR2:-5.078602313995361--TestLoss:101.32758331298828--TestR2:-6.5950751304626465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1306--TrainLoss:92.50981140136719--TrainR2:-5.077509880065918--TestLoss:101.30982971191406--TestR2:-6.593743801116943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1307--TrainLoss:92.4931411743164--TrainR2:-5.076414585113525--TestLoss:101.29206848144531--TestR2:-6.59241247177124\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1308--TrainLoss:92.4764633178711--TrainR2:-5.075319290161133--TestLoss:101.27430725097656--TestR2:-6.591081619262695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1309--TrainLoss:92.45985412597656--TrainR2:-5.074227809906006--TestLoss:101.25654602050781--TestR2:-6.589750289916992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1310--TrainLoss:92.44322204589844--TrainR2:-5.0731353759765625--TestLoss:101.2387924194336--TestR2:-6.588419437408447\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1311--TrainLoss:92.42660522460938--TrainR2:-5.072043418884277--TestLoss:101.22105407714844--TestR2:-6.587090015411377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1312--TrainLoss:92.40995788574219--TrainR2:-5.070950031280518--TestLoss:101.20329284667969--TestR2:-6.585758686065674\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1313--TrainLoss:92.39330291748047--TrainR2:-5.069855690002441--TestLoss:101.18555450439453--TestR2:-6.584428787231445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1314--TrainLoss:92.37667083740234--TrainR2:-5.068763256072998--TestLoss:101.16780853271484--TestR2:-6.583098411560059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1315--TrainLoss:92.36004638671875--TrainR2:-5.067670822143555--TestLoss:101.15005493164062--TestR2:-6.581768035888672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1316--TrainLoss:92.3434066772461--TrainR2:-5.066577434539795--TestLoss:101.13233947753906--TestR2:-6.580440044403076\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1317--TrainLoss:92.32682800292969--TrainR2:-5.065488815307617--TestLoss:101.11458587646484--TestR2:-6.5791096687316895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1318--TrainLoss:92.31015014648438--TrainR2:-5.064393043518066--TestLoss:101.09683990478516--TestR2:-6.577779293060303\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1319--TrainLoss:92.29352569580078--TrainR2:-5.063300609588623--TestLoss:101.07911682128906--TestR2:-6.576450824737549\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1320--TrainLoss:92.27692413330078--TrainR2:-5.0622100830078125--TestLoss:101.06138610839844--TestR2:-6.575121879577637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1321--TrainLoss:92.26033020019531--TrainR2:-5.061119556427002--TestLoss:101.04364776611328--TestR2:-6.573792457580566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1322--TrainLoss:92.24370574951172--TrainR2:-5.060028076171875--TestLoss:101.02593231201172--TestR2:-6.572464466094971\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1323--TrainLoss:92.22708892822266--TrainR2:-5.05893611907959--TestLoss:101.00819396972656--TestR2:-6.5711350440979\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1324--TrainLoss:92.21045684814453--TrainR2:-5.0578436851501465--TestLoss:100.99049377441406--TestR2:-6.569808483123779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1325--TrainLoss:92.1938705444336--TrainR2:-5.056753635406494--TestLoss:100.9727783203125--TestR2:-6.568480014801025\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1326--TrainLoss:92.17728424072266--TrainR2:-5.0556640625--TestLoss:100.95504760742188--TestR2:-6.567151069641113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1327--TrainLoss:92.16068267822266--TrainR2:-5.0545735359191895--TestLoss:100.93733978271484--TestR2:-6.565823554992676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1328--TrainLoss:92.1440658569336--TrainR2:-5.0534820556640625--TestLoss:100.91962432861328--TestR2:-6.56449556350708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1329--TrainLoss:92.12744903564453--TrainR2:-5.0523905754089355--TestLoss:100.90192413330078--TestR2:-6.563169479370117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1330--TrainLoss:92.11087799072266--TrainR2:-5.0513014793396--TestLoss:100.88420867919922--TestR2:-6.5618414878845215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1331--TrainLoss:92.09428405761719--TrainR2:-5.050211429595947--TestLoss:100.86650848388672--TestR2:-6.560514450073242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1332--TrainLoss:92.07770538330078--TrainR2:-5.049122333526611--TestLoss:100.84879302978516--TestR2:-6.559186935424805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1333--TrainLoss:92.06111145019531--TrainR2:-5.048031806945801--TestLoss:100.83109283447266--TestR2:-6.557859897613525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1334--TrainLoss:92.04447174072266--TrainR2:-5.046938896179199--TestLoss:100.81340026855469--TestR2:-6.5565338134765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1335--TrainLoss:92.02794647216797--TrainR2:-5.045853614807129--TestLoss:100.79571533203125--TestR2:-6.555208206176758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1336--TrainLoss:92.01134490966797--TrainR2:-5.04476261138916--TestLoss:100.77802276611328--TestR2:-6.553882122039795\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1337--TrainLoss:91.99478912353516--TrainR2:-5.043675422668457--TestLoss:100.76032257080078--TestR2:-6.552555561065674\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1338--TrainLoss:91.97821044921875--TrainR2:-5.042585849761963--TestLoss:100.74263763427734--TestR2:-6.551229953765869\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1339--TrainLoss:91.96159362792969--TrainR2:-5.041494369506836--TestLoss:100.72496032714844--TestR2:-6.549904823303223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1340--TrainLoss:91.94506072998047--TrainR2:-5.040408134460449--TestLoss:100.70728302001953--TestR2:-6.548579692840576\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1341--TrainLoss:91.92848205566406--TrainR2:-5.039319038391113--TestLoss:100.68961334228516--TestR2:-6.547255039215088\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1342--TrainLoss:91.91194152832031--TrainR2:-5.038232326507568--TestLoss:100.67192840576172--TestR2:-6.545929908752441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1343--TrainLoss:91.89534759521484--TrainR2:-5.037142276763916--TestLoss:100.65424346923828--TestR2:-6.544604301452637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1344--TrainLoss:91.87882232666016--TrainR2:-5.0360565185546875--TestLoss:100.63658142089844--TestR2:-6.543280124664307\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1345--TrainLoss:91.86222839355469--TrainR2:-5.034966468811035--TestLoss:100.61890411376953--TestR2:-6.54195499420166\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1346--TrainLoss:91.84566497802734--TrainR2:-5.033878326416016--TestLoss:100.60123443603516--TestR2:-6.540631294250488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1347--TrainLoss:91.82913208007812--TrainR2:-5.032792091369629--TestLoss:100.58357238769531--TestR2:-6.539307117462158\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1348--TrainLoss:91.81256866455078--TrainR2:-5.031703948974609--TestLoss:100.56590270996094--TestR2:-6.53798246383667\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1349--TrainLoss:91.79605102539062--TrainR2:-5.030618667602539--TestLoss:100.54824829101562--TestR2:-6.536659240722656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1350--TrainLoss:91.7794418334961--TrainR2:-5.02952766418457--TestLoss:100.53057098388672--TestR2:-6.535334587097168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1351--TrainLoss:91.76293182373047--TrainR2:-5.028442859649658--TestLoss:100.51293182373047--TestR2:-6.534012317657471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1352--TrainLoss:91.74638366699219--TrainR2:-5.027355670928955--TestLoss:100.49527740478516--TestR2:-6.532688617706299\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1353--TrainLoss:91.72986602783203--TrainR2:-5.026270389556885--TestLoss:100.47762298583984--TestR2:-6.531365394592285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1354--TrainLoss:91.71330261230469--TrainR2:-5.025182723999023--TestLoss:100.45997619628906--TestR2:-6.530043125152588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1355--TrainLoss:91.69677734375--TrainR2:-5.024096965789795--TestLoss:100.44234466552734--TestR2:-6.528721332550049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1356--TrainLoss:91.68024444580078--TrainR2:-5.023010730743408--TestLoss:100.42469787597656--TestR2:-6.527398586273193\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1357--TrainLoss:91.66368865966797--TrainR2:-5.021923542022705--TestLoss:100.40704345703125--TestR2:-6.52607536315918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1358--TrainLoss:91.64717864990234--TrainR2:-5.020838737487793--TestLoss:100.38941192626953--TestR2:-6.524753570556641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1359--TrainLoss:91.63066101074219--TrainR2:-5.019753456115723--TestLoss:100.37177276611328--TestR2:-6.523431301116943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1360--TrainLoss:91.61415100097656--TrainR2:-5.0186686515808105--TestLoss:100.3541259765625--TestR2:-6.522109031677246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1361--TrainLoss:91.59761810302734--TrainR2:-5.017582416534424--TestLoss:100.33651733398438--TestR2:-6.52078914642334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1362--TrainLoss:91.58109283447266--TrainR2:-5.0164971351623535--TestLoss:100.31887817382812--TestR2:-6.519466876983643\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1363--TrainLoss:91.5645751953125--TrainR2:-5.015411853790283--TestLoss:100.3012466430664--TestR2:-6.5181450843811035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1364--TrainLoss:91.548095703125--TrainR2:-5.014328956604004--TestLoss:100.28363800048828--TestR2:-6.516825199127197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1365--TrainLoss:91.53152465820312--TrainR2:-5.013240337371826--TestLoss:100.26600646972656--TestR2:-6.515503883361816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1366--TrainLoss:91.51506042480469--TrainR2:-5.0121588706970215--TestLoss:100.24837493896484--TestR2:-6.5141825675964355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1367--TrainLoss:91.49852752685547--TrainR2:-5.011072635650635--TestLoss:100.23077392578125--TestR2:-6.512862682342529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1368--TrainLoss:91.48204803466797--TrainR2:-5.009990215301514--TestLoss:100.21316528320312--TestR2:-6.511542797088623\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1369--TrainLoss:91.46553039550781--TrainR2:-5.008904933929443--TestLoss:100.19554138183594--TestR2:-6.5102219581604\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1370--TrainLoss:91.44904327392578--TrainR2:-5.007821559906006--TestLoss:100.17792510986328--TestR2:-6.508901119232178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1371--TrainLoss:91.43252563476562--TrainR2:-5.006736755371094--TestLoss:100.16032409667969--TestR2:-6.507582187652588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1372--TrainLoss:91.41604614257812--TrainR2:-5.005654335021973--TestLoss:100.14273834228516--TestR2:-6.506263732910156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1373--TrainLoss:91.39955139160156--TrainR2:-5.004570007324219--TestLoss:100.12511444091797--TestR2:-6.504942893981934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1374--TrainLoss:91.383056640625--TrainR2:-5.003486633300781--TestLoss:100.1075210571289--TestR2:-6.503624439239502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1375--TrainLoss:91.36656188964844--TrainR2:-5.002403259277344--TestLoss:100.08992767333984--TestR2:-6.502305507659912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1376--TrainLoss:91.35009765625--TrainR2:-5.001321315765381--TestLoss:100.07233428955078--TestR2:-6.5009870529174805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1377--TrainLoss:91.33361053466797--TrainR2:-5.000237941741943--TestLoss:100.05472564697266--TestR2:-6.499666690826416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1378--TrainLoss:91.31712341308594--TrainR2:-4.999155044555664--TestLoss:100.03713989257812--TestR2:-6.498348712921143\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1379--TrainLoss:91.30064392089844--TrainR2:-4.998072147369385--TestLoss:100.01954650878906--TestR2:-6.497030258178711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1380--TrainLoss:91.2842025756836--TrainR2:-4.996992111206055--TestLoss:100.00196838378906--TestR2:-6.4957122802734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1381--TrainLoss:91.26766967773438--TrainR2:-4.995906352996826--TestLoss:99.984375--TestR2:-6.494393825531006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1382--TrainLoss:91.25122833251953--TrainR2:-4.994826316833496--TestLoss:99.96678924560547--TestR2:-6.493075847625732\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1383--TrainLoss:91.23474884033203--TrainR2:-4.993743896484375--TestLoss:99.94921875--TestR2:-6.491758823394775\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1384--TrainLoss:91.21827697753906--TrainR2:-4.992661476135254--TestLoss:99.931640625--TestR2:-6.490440845489502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1385--TrainLoss:91.20181274414062--TrainR2:-4.991580009460449--TestLoss:99.9140625--TestR2:-6.489123344421387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1386--TrainLoss:91.18535614013672--TrainR2:-4.9904985427856445--TestLoss:99.89649963378906--TestR2:-6.487806797027588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1387--TrainLoss:91.16886901855469--TrainR2:-4.989415645599365--TestLoss:99.87892150878906--TestR2:-6.486489772796631\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1388--TrainLoss:91.15247344970703--TrainR2:-4.988338470458984--TestLoss:99.86135864257812--TestR2:-6.485173225402832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1389--TrainLoss:91.13599395751953--TrainR2:-4.987256050109863--TestLoss:99.84378814697266--TestR2:-6.483855724334717\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1390--TrainLoss:91.11951446533203--TrainR2:-4.986173152923584--TestLoss:99.82623291015625--TestR2:-6.482540130615234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1391--TrainLoss:91.10310363769531--TrainR2:-4.985095024108887--TestLoss:99.80867767333984--TestR2:-6.481224536895752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1392--TrainLoss:91.08663177490234--TrainR2:-4.984013080596924--TestLoss:99.79113006591797--TestR2:-6.4799089431762695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1393--TrainLoss:91.0701904296875--TrainR2:-4.9829325675964355--TestLoss:99.7735595703125--TestR2:-6.478592395782471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1394--TrainLoss:91.05374145507812--TrainR2:-4.981852054595947--TestLoss:99.7560043334961--TestR2:-6.477275848388672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1395--TrainLoss:91.03732299804688--TrainR2:-4.98077392578125--TestLoss:99.73845672607422--TestR2:-6.475960731506348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1396--TrainLoss:91.02082061767578--TrainR2:-4.979689121246338--TestLoss:99.72090911865234--TestR2:-6.474645614624023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1397--TrainLoss:91.00444030761719--TrainR2:-4.978613376617432--TestLoss:99.70336151123047--TestR2:-6.473330497741699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1398--TrainLoss:90.98802947998047--TrainR2:-4.977534770965576--TestLoss:99.68582916259766--TestR2:-6.472015857696533\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1399--TrainLoss:90.9715576171875--TrainR2:-4.976452827453613--TestLoss:99.66828155517578--TestR2:-6.470700740814209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1400--TrainLoss:90.95513916015625--TrainR2:-4.975374698638916--TestLoss:99.6507339477539--TestR2:-6.469385623931885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1401--TrainLoss:90.93869018554688--TrainR2:-4.974294185638428--TestLoss:99.6332015991211--TestR2:-6.468071460723877\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1402--TrainLoss:90.92230987548828--TrainR2:-4.973217487335205--TestLoss:99.61566162109375--TestR2:-6.466756343841553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1403--TrainLoss:90.90583038330078--TrainR2:-4.972135066986084--TestLoss:99.5981674194336--TestR2:-6.465445518493652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1404--TrainLoss:90.88945007324219--TrainR2:-4.9710588455200195--TestLoss:99.58061981201172--TestR2:-6.464130401611328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1405--TrainLoss:90.873046875--TrainR2:-4.9699811935424805--TestLoss:99.56309509277344--TestR2:-6.46281623840332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1406--TrainLoss:90.85658264160156--TrainR2:-4.968899726867676--TestLoss:99.54557037353516--TestR2:-6.461503028869629\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1407--TrainLoss:90.8401870727539--TrainR2:-4.967822551727295--TestLoss:99.52804565429688--TestR2:-6.460189342498779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1408--TrainLoss:90.82379150390625--TrainR2:-4.966745376586914--TestLoss:99.5105209350586--TestR2:-6.458876132965088\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1409--TrainLoss:90.8073959350586--TrainR2:-4.965668201446533--TestLoss:99.49301147460938--TestR2:-6.457563400268555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1410--TrainLoss:90.79095458984375--TrainR2:-4.964588165283203--TestLoss:99.47549438476562--TestR2:-6.456250190734863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1411--TrainLoss:90.7745590209961--TrainR2:-4.963510990142822--TestLoss:99.45799255371094--TestR2:-6.4549384117126465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1412--TrainLoss:90.75818634033203--TrainR2:-4.962435245513916--TestLoss:99.44048309326172--TestR2:-6.453625679016113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1413--TrainLoss:90.74177551269531--TrainR2:-4.961357116699219--TestLoss:99.42295837402344--TestR2:-6.452312469482422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1414--TrainLoss:90.72537231445312--TrainR2:-4.960279941558838--TestLoss:99.40544891357422--TestR2:-6.451000213623047\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1415--TrainLoss:90.70896911621094--TrainR2:-4.959202289581299--TestLoss:99.3879623413086--TestR2:-6.4496893882751465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1416--TrainLoss:90.69259643554688--TrainR2:-4.958126068115234--TestLoss:99.3704605102539--TestR2:-6.44837760925293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1417--TrainLoss:90.67620086669922--TrainR2:-4.957049369812012--TestLoss:99.35295867919922--TestR2:-6.447065353393555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1418--TrainLoss:90.65980529785156--TrainR2:-4.955972194671631--TestLoss:99.33546447753906--TestR2:-6.445754528045654\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1419--TrainLoss:90.6434326171875--TrainR2:-4.954896450042725--TestLoss:99.31798553466797--TestR2:-6.444444179534912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1420--TrainLoss:90.62704467773438--TrainR2:-4.953819751739502--TestLoss:99.30048370361328--TestR2:-6.443131923675537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1421--TrainLoss:90.61067199707031--TrainR2:-4.952744007110596--TestLoss:99.28301239013672--TestR2:-6.441822528839111\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1422--TrainLoss:90.59428405761719--TrainR2:-4.951667785644531--TestLoss:99.26551818847656--TestR2:-6.440511226654053\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1423--TrainLoss:90.57791137695312--TrainR2:-4.950592041015625--TestLoss:99.24803161621094--TestR2:-6.439200401306152\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1424--TrainLoss:90.56153106689453--TrainR2:-4.9495158195495605--TestLoss:99.23053741455078--TestR2:-6.437889575958252\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1425--TrainLoss:90.54522705078125--TrainR2:-4.948444843292236--TestLoss:99.21308135986328--TestR2:-6.436581134796143\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1426--TrainLoss:90.5287857055664--TrainR2:-4.947364807128906--TestLoss:99.19558715820312--TestR2:-6.435269832611084\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1427--TrainLoss:90.5124282836914--TrainR2:-4.946290016174316--TestLoss:99.1781234741211--TestR2:-6.433960437774658\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1428--TrainLoss:90.49606323242188--TrainR2:-4.945214748382568--TestLoss:99.16065216064453--TestR2:-6.432651042938232\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1429--TrainLoss:90.47972106933594--TrainR2:-4.944141387939453--TestLoss:99.14318084716797--TestR2:-6.431341171264648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1430--TrainLoss:90.46336364746094--TrainR2:-4.943066596984863--TestLoss:99.12571716308594--TestR2:-6.430032730102539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1431--TrainLoss:90.44698333740234--TrainR2:-4.941990852355957--TestLoss:99.10823822021484--TestR2:-6.428722858428955\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1432--TrainLoss:90.4306411743164--TrainR2:-4.940917491912842--TestLoss:99.09078216552734--TestR2:-6.4274139404296875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1433--TrainLoss:90.41429138183594--TrainR2:-4.93984317779541--TestLoss:99.07332611083984--TestR2:-6.426105976104736\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1434--TrainLoss:90.3979263305664--TrainR2:-4.938767910003662--TestLoss:99.05587005615234--TestR2:-6.424797058105469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1435--TrainLoss:90.38159942626953--TrainR2:-4.937695026397705--TestLoss:99.03841400146484--TestR2:-6.423488616943359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1436--TrainLoss:90.3652572631836--TrainR2:-4.936621189117432--TestLoss:99.02096557617188--TestR2:-6.422180652618408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1437--TrainLoss:90.34888458251953--TrainR2:-4.935546398162842--TestLoss:99.00352478027344--TestR2:-6.420873641967773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1438--TrainLoss:90.33256530761719--TrainR2:-4.934473991394043--TestLoss:98.9860610961914--TestR2:-6.419564247131348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1439--TrainLoss:90.31623840332031--TrainR2:-4.933401584625244--TestLoss:98.96862030029297--TestR2:-6.418257236480713\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1440--TrainLoss:90.29986572265625--TrainR2:-4.932325839996338--TestLoss:98.95117950439453--TestR2:-6.41694974899292\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1441--TrainLoss:90.28357696533203--TrainR2:-4.931255340576172--TestLoss:98.93373107910156--TestR2:-6.415642261505127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1442--TrainLoss:90.26725006103516--TrainR2:-4.930182933807373--TestLoss:98.91630554199219--TestR2:-6.41433572769165\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1443--TrainLoss:90.25090789794922--TrainR2:-4.9291090965271--TestLoss:98.89886474609375--TestR2:-6.413028717041016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1444--TrainLoss:90.2345962524414--TrainR2:-4.928037643432617--TestLoss:98.88143157958984--TestR2:-6.411722183227539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1445--TrainLoss:90.21825408935547--TrainR2:-4.926963806152344--TestLoss:98.86399841308594--TestR2:-6.4104156494140625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1446--TrainLoss:90.20193481445312--TrainR2:-4.925891876220703--TestLoss:98.8465805053711--TestR2:-6.409109592437744\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1447--TrainLoss:90.18559265136719--TrainR2:-4.924818515777588--TestLoss:98.82913208007812--TestR2:-6.407801628112793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1448--TrainLoss:90.16930389404297--TrainR2:-4.923748016357422--TestLoss:98.81171417236328--TestR2:-6.406496524810791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1449--TrainLoss:90.15296936035156--TrainR2:-4.922675132751465--TestLoss:98.7943115234375--TestR2:-6.405191421508789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1450--TrainLoss:90.13666534423828--TrainR2:-4.921604156494141--TestLoss:98.77687072753906--TestR2:-6.4038848876953125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1451--TrainLoss:90.12034606933594--TrainR2:-4.920531749725342--TestLoss:98.75946807861328--TestR2:-6.4025797843933105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1452--TrainLoss:90.10404205322266--TrainR2:-4.919460773468018--TestLoss:98.74203491210938--TestR2:-6.401273727416992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1453--TrainLoss:90.08773040771484--TrainR2:-4.918389320373535--TestLoss:98.72463989257812--TestR2:-6.399969577789307\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1454--TrainLoss:90.07149505615234--TrainR2:-4.917322635650635--TestLoss:98.70723724365234--TestR2:-6.398664951324463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1455--TrainLoss:90.05513763427734--TrainR2:-4.916248321533203--TestLoss:98.68983459472656--TestR2:-6.397360801696777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1456--TrainLoss:90.03884887695312--TrainR2:-4.915178298950195--TestLoss:98.67241668701172--TestR2:-6.396054744720459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1457--TrainLoss:90.02252960205078--TrainR2:-4.9141058921813965--TestLoss:98.65501403808594--TestR2:-6.394750595092773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1458--TrainLoss:90.00627136230469--TrainR2:-4.9130377769470215--TestLoss:98.63761138916016--TestR2:-6.39344596862793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1459--TrainLoss:89.98995208740234--TrainR2:-4.911965847015381--TestLoss:98.6202163696289--TestR2:-6.392142295837402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1460--TrainLoss:89.97369384765625--TrainR2:-4.910897254943848--TestLoss:98.60283660888672--TestR2:-6.390839576721191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1461--TrainLoss:89.95741271972656--TrainR2:-4.909828186035156--TestLoss:98.58543395996094--TestR2:-6.389534950256348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1462--TrainLoss:89.94110870361328--TrainR2:-4.908756732940674--TestLoss:98.56803131103516--TestR2:-6.388230800628662\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1463--TrainLoss:89.92481994628906--TrainR2:-4.907686710357666--TestLoss:98.55064392089844--TestR2:-6.386927604675293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1464--TrainLoss:89.90856170654297--TrainR2:-4.906618595123291--TestLoss:98.53325653076172--TestR2:-6.385624408721924\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1465--TrainLoss:89.89227294921875--TrainR2:-4.905548572540283--TestLoss:98.51588439941406--TestR2:-6.384321689605713\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1466--TrainLoss:89.8759994506836--TrainR2:-4.904479503631592--TestLoss:98.49849700927734--TestR2:-6.383018970489502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1467--TrainLoss:89.8597183227539--TrainR2:-4.903409957885742--TestLoss:98.48112487792969--TestR2:-6.381716728210449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1468--TrainLoss:89.84349822998047--TrainR2:-4.902344226837158--TestLoss:98.4637451171875--TestR2:-6.38041353225708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1469--TrainLoss:89.8271713256836--TrainR2:-4.901271343231201--TestLoss:98.44637298583984--TestR2:-6.379112243652344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1470--TrainLoss:89.81092071533203--TrainR2:-4.900203704833984--TestLoss:98.42900085449219--TestR2:-6.377809524536133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1471--TrainLoss:89.7946548461914--TrainR2:-4.899135112762451--TestLoss:98.41162872314453--TestR2:-6.37650728225708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1472--TrainLoss:89.77841186523438--TrainR2:-4.898067951202393--TestLoss:98.39427185058594--TestR2:-6.375206470489502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1473--TrainLoss:89.76213073730469--TrainR2:-4.896998405456543--TestLoss:98.37689971923828--TestR2:-6.373904705047607\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1474--TrainLoss:89.74588012695312--TrainR2:-4.895931243896484--TestLoss:98.35953521728516--TestR2:-6.372602462768555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1475--TrainLoss:89.72962188720703--TrainR2:-4.894862651824951--TestLoss:98.34217834472656--TestR2:-6.371301651000977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1476--TrainLoss:89.7134017944336--TrainR2:-4.893797397613525--TestLoss:98.3248291015625--TestR2:-6.370001792907715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1477--TrainLoss:89.69711303710938--TrainR2:-4.892727375030518--TestLoss:98.30745697021484--TestR2:-6.368699073791504\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1478--TrainLoss:89.68087768554688--TrainR2:-4.891660690307617--TestLoss:98.29011535644531--TestR2:-6.367399215698242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1479--TrainLoss:89.66462707519531--TrainR2:-4.890593528747559--TestLoss:98.27275848388672--TestR2:-6.366098403930664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1480--TrainLoss:89.64839935302734--TrainR2:-4.889526844024658--TestLoss:98.25541687011719--TestR2:-6.364798545837402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1481--TrainLoss:89.63215637207031--TrainR2:-4.8884596824646--TestLoss:98.2380599975586--TestR2:-6.363497734069824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1482--TrainLoss:89.61590576171875--TrainR2:-4.887392044067383--TestLoss:98.22071838378906--TestR2:-6.3621978759765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1483--TrainLoss:89.5997085571289--TrainR2:-4.886327743530273--TestLoss:98.20338439941406--TestR2:-6.360898494720459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1484--TrainLoss:89.58346557617188--TrainR2:-4.885261058807373--TestLoss:98.18604278564453--TestR2:-6.359598636627197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1485--TrainLoss:89.56722259521484--TrainR2:-4.8841938972473145--TestLoss:98.16870880126953--TestR2:-6.358299255371094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1486--TrainLoss:89.55098724365234--TrainR2:-4.883127212524414--TestLoss:98.15135955810547--TestR2:-6.356998920440674\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1487--TrainLoss:89.53477478027344--TrainR2:-4.8820624351501465--TestLoss:98.13404846191406--TestR2:-6.355701446533203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1488--TrainLoss:89.5185546875--TrainR2:-4.8809967041015625--TestLoss:98.11671447753906--TestR2:-6.3544020652771\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1489--TrainLoss:89.50232696533203--TrainR2:-4.87993049621582--TestLoss:98.09938049316406--TestR2:-6.353102684020996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1490--TrainLoss:89.48607635498047--TrainR2:-4.8788628578186035--TestLoss:98.08206939697266--TestR2:-6.351805210113525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1491--TrainLoss:89.46988677978516--TrainR2:-4.8777995109558105--TestLoss:98.06474304199219--TestR2:-6.35050630569458\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1492--TrainLoss:89.45368194580078--TrainR2:-4.876734733581543--TestLoss:98.04741668701172--TestR2:-6.349207878112793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1493--TrainLoss:89.43746185302734--TrainR2:-4.875669479370117--TestLoss:98.03010559082031--TestR2:-6.347909927368164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1494--TrainLoss:89.42125701904297--TrainR2:-4.87460470199585--TestLoss:98.0127944946289--TestR2:-6.346612930297852\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1495--TrainLoss:89.40502166748047--TrainR2:-4.873538017272949--TestLoss:97.9954833984375--TestR2:-6.345315456390381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1496--TrainLoss:89.38884735107422--TrainR2:-4.872475624084473--TestLoss:97.97815704345703--TestR2:-6.3440165519714355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1497--TrainLoss:89.37262725830078--TrainR2:-4.871409893035889--TestLoss:97.96086120605469--TestR2:-6.342720031738281\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1498--TrainLoss:89.3564682006836--TrainR2:-4.87034797668457--TestLoss:97.94355010986328--TestR2:-6.341422080993652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1499--TrainLoss:89.34025573730469--TrainR2:-4.869283199310303--TestLoss:97.92625427246094--TestR2:-6.340125560760498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1500--TrainLoss:89.32402038574219--TrainR2:-4.868216514587402--TestLoss:97.90895080566406--TestR2:-6.338829040527344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1501--TrainLoss:89.30786895751953--TrainR2:-4.8671555519104--TestLoss:97.89165496826172--TestR2:-6.3375325202941895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1502--TrainLoss:89.29165649414062--TrainR2:-4.866090297698975--TestLoss:97.87435150146484--TestR2:-6.336236000061035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1503--TrainLoss:89.2754898071289--TrainR2:-4.865028381347656--TestLoss:97.8570556640625--TestR2:-6.334939479827881\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1504--TrainLoss:89.25928497314453--TrainR2:-4.863964080810547--TestLoss:97.83976745605469--TestR2:-6.333643436431885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1505--TrainLoss:89.24309539794922--TrainR2:-4.862900257110596--TestLoss:97.82247924804688--TestR2:-6.332347393035889\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1506--TrainLoss:89.22693634033203--TrainR2:-4.861838340759277--TestLoss:97.80519104003906--TestR2:-6.331051826477051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1507--TrainLoss:89.21076202392578--TrainR2:-4.860775947570801--TestLoss:97.78791046142578--TestR2:-6.329756259918213\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1508--TrainLoss:89.19457244873047--TrainR2:-4.859712600708008--TestLoss:97.77063751220703--TestR2:-6.328461647033691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1509--TrainLoss:89.17833709716797--TrainR2:-4.858645439147949--TestLoss:97.75335693359375--TestR2:-6.3271660804748535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1510--TrainLoss:89.16219329833984--TrainR2:-4.857585430145264--TestLoss:97.73606872558594--TestR2:-6.325870513916016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1511--TrainLoss:89.14605712890625--TrainR2:-4.85652494430542--TestLoss:97.71880340576172--TestR2:-6.324575901031494\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1512--TrainLoss:89.12986755371094--TrainR2:-4.855461597442627--TestLoss:97.70153045654297--TestR2:-6.323281764984131\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1513--TrainLoss:89.11373138427734--TrainR2:-4.854401111602783--TestLoss:97.68425750732422--TestR2:-6.321987152099609\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1514--TrainLoss:89.09756469726562--TrainR2:-4.853339672088623--TestLoss:97.6669921875--TestR2:-6.320693016052246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1515--TrainLoss:89.08137512207031--TrainR2:-4.852275848388672--TestLoss:97.64971160888672--TestR2:-6.319397449493408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1516--TrainLoss:89.06521606445312--TrainR2:-4.8512139320373535--TestLoss:97.63245391845703--TestR2:-6.318104267120361\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1517--TrainLoss:89.04906463623047--TrainR2:-4.850152969360352--TestLoss:97.61519622802734--TestR2:-6.316810607910156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1518--TrainLoss:89.0329360961914--TrainR2:-4.849093437194824--TestLoss:97.59793090820312--TestR2:-6.315516471862793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1519--TrainLoss:89.01677703857422--TrainR2:-4.848032474517822--TestLoss:97.58067321777344--TestR2:-6.314222812652588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1520--TrainLoss:89.00061798095703--TrainR2:-4.846970558166504--TestLoss:97.56343841552734--TestR2:-6.312931060791016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1521--TrainLoss:88.9844970703125--TrainR2:-4.845911502838135--TestLoss:97.5461654663086--TestR2:-6.311636447906494\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1522--TrainLoss:88.96829986572266--TrainR2:-4.844847202301025--TestLoss:97.52892303466797--TestR2:-6.3103437423706055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1523--TrainLoss:88.9521484375--TrainR2:-4.843786239624023--TestLoss:97.51167297363281--TestR2:-6.309051036834717\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1524--TrainLoss:88.93604278564453--TrainR2:-4.842728137969971--TestLoss:97.49443817138672--TestR2:-6.3077592849731445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1525--TrainLoss:88.9198989868164--TrainR2:-4.841667652130127--TestLoss:97.47718048095703--TestR2:-6.3064656257629395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1526--TrainLoss:88.9037857055664--TrainR2:-4.840609073638916--TestLoss:97.45995330810547--TestR2:-6.305174350738525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1527--TrainLoss:88.88765716552734--TrainR2:-4.839549541473389--TestLoss:97.44271087646484--TestR2:-6.303881645202637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1528--TrainLoss:88.87152099609375--TrainR2:-4.838489532470703--TestLoss:97.42548370361328--TestR2:-6.302590847015381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1529--TrainLoss:88.8553466796875--TrainR2:-4.837426662445068--TestLoss:97.40824127197266--TestR2:-6.301298141479492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1530--TrainLoss:88.83924102783203--TrainR2:-4.836369037628174--TestLoss:97.39102172851562--TestR2:-6.300007343292236\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1531--TrainLoss:88.8231201171875--TrainR2:-4.8353095054626465--TestLoss:97.37379455566406--TestR2:-6.298716068267822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1532--TrainLoss:88.8070068359375--TrainR2:-4.8342509269714355--TestLoss:97.3565673828125--TestR2:-6.297424793243408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1533--TrainLoss:88.7908935546875--TrainR2:-4.833192348480225--TestLoss:97.33934020996094--TestR2:-6.296133041381836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1534--TrainLoss:88.77476501464844--TrainR2:-4.832132816314697--TestLoss:97.3221206665039--TestR2:-6.294842720031738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1535--TrainLoss:88.75868225097656--TrainR2:-4.831076145172119--TestLoss:97.30489349365234--TestR2:-6.293551445007324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1536--TrainLoss:88.7425765991211--TrainR2:-4.830018043518066--TestLoss:97.28768157958984--TestR2:-6.292261600494385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1537--TrainLoss:88.72642517089844--TrainR2:-4.828957557678223--TestLoss:97.27047729492188--TestR2:-6.290971755981445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1538--TrainLoss:88.7103271484375--TrainR2:-4.82789945602417--TestLoss:97.2532730102539--TestR2:-6.289682388305664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1539--TrainLoss:88.6942138671875--TrainR2:-4.826840877532959--TestLoss:97.23604583740234--TestR2:-6.28839111328125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1540--TrainLoss:88.67810821533203--TrainR2:-4.825782775878906--TestLoss:97.2188491821289--TestR2:-6.287102222442627\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1541--TrainLoss:88.66201782226562--TrainR2:-4.824726104736328--TestLoss:97.2016372680664--TestR2:-6.285811901092529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1542--TrainLoss:88.64591217041016--TrainR2:-4.823667526245117--TestLoss:97.18444061279297--TestR2:-6.284522533416748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1543--TrainLoss:88.62982177734375--TrainR2:-4.822610378265381--TestLoss:97.16722869873047--TestR2:-6.283232688903809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1544--TrainLoss:88.61375427246094--TrainR2:-4.821555137634277--TestLoss:97.15003967285156--TestR2:-6.281944274902344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1545--TrainLoss:88.59764099121094--TrainR2:-4.820496559143066--TestLoss:97.1328353881836--TestR2:-6.2806549072265625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1546--TrainLoss:88.58154296875--TrainR2:-4.819438934326172--TestLoss:97.11566162109375--TestR2:-6.279367446899414\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1547--TrainLoss:88.56544494628906--TrainR2:-4.818381309509277--TestLoss:97.09846496582031--TestR2:-6.278078556060791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1548--TrainLoss:88.54936218261719--TrainR2:-4.817325115203857--TestLoss:97.08126831054688--TestR2:-6.276789665222168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1549--TrainLoss:88.53326416015625--TrainR2:-4.816267013549805--TestLoss:97.0640869140625--TestR2:-6.275501728057861\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1550--TrainLoss:88.51720428466797--TrainR2:-4.815212249755859--TestLoss:97.04690551757812--TestR2:-6.274213790893555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1551--TrainLoss:88.50114440917969--TrainR2:-4.814157009124756--TestLoss:97.02973175048828--TestR2:-6.272926330566406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1552--TrainLoss:88.48506164550781--TrainR2:-4.813100337982178--TestLoss:97.01254272460938--TestR2:-6.2716383934021\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1553--TrainLoss:88.46900177001953--TrainR2:-4.812045097351074--TestLoss:96.99536895751953--TestR2:-6.270350933074951\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1554--TrainLoss:88.45293426513672--TrainR2:-4.810989856719971--TestLoss:96.97818756103516--TestR2:-6.2690629959106445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1555--TrainLoss:88.43685150146484--TrainR2:-4.809933185577393--TestLoss:96.96102142333984--TestR2:-6.267776012420654\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1556--TrainLoss:88.42079162597656--TrainR2:-4.808877944946289--TestLoss:96.94385528564453--TestR2:-6.266489505767822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1557--TrainLoss:88.40474700927734--TrainR2:-4.807824611663818--TestLoss:96.92667388916016--TestR2:-6.265202045440674\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1558--TrainLoss:88.38865661621094--TrainR2:-4.806767463684082--TestLoss:96.90950775146484--TestR2:-6.263915538787842\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1559--TrainLoss:88.37261199951172--TrainR2:-4.805713176727295--TestLoss:96.8923568725586--TestR2:-6.262629508972168\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1560--TrainLoss:88.35652160644531--TrainR2:-4.804656028747559--TestLoss:96.87520599365234--TestR2:-6.261343955993652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1561--TrainLoss:88.3404769897461--TrainR2:-4.8036017417907715--TestLoss:96.85803985595703--TestR2:-6.26005744934082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1562--TrainLoss:88.32437896728516--TrainR2:-4.802544116973877--TestLoss:96.84089660644531--TestR2:-6.258772373199463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1563--TrainLoss:88.3083724975586--TrainR2:-4.801492691040039--TestLoss:96.82373046875--TestR2:-6.257485389709473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1564--TrainLoss:88.29232788085938--TrainR2:-4.800438404083252--TestLoss:96.80659484863281--TestR2:-6.256201267242432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1565--TrainLoss:88.27627563476562--TrainR2:-4.799384117126465--TestLoss:96.7894287109375--TestR2:-6.2549147605896\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1566--TrainLoss:88.26022338867188--TrainR2:-4.798329830169678--TestLoss:96.77229309082031--TestR2:-6.2536301612854\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1567--TrainLoss:88.24420166015625--TrainR2:-4.797276973724365--TestLoss:96.75514221191406--TestR2:-6.252344608306885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1568--TrainLoss:88.22817993164062--TrainR2:-4.796224117279053--TestLoss:96.73800659179688--TestR2:-6.2510600090026855\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1569--TrainLoss:88.21212768554688--TrainR2:-4.795170307159424--TestLoss:96.72086334228516--TestR2:-6.249775409698486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1570--TrainLoss:88.19609069824219--TrainR2:-4.794116497039795--TestLoss:96.7037353515625--TestR2:-6.248491287231445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1571--TrainLoss:88.18006896972656--TrainR2:-4.793064117431641--TestLoss:96.68659210205078--TestR2:-6.247206211090088\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1572--TrainLoss:88.1640396118164--TrainR2:-4.79201078414917--TestLoss:96.66944885253906--TestR2:-6.245921611785889\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1573--TrainLoss:88.14801788330078--TrainR2:-4.790957927703857--TestLoss:96.65232849121094--TestR2:-6.244638442993164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1574--TrainLoss:88.13199615478516--TrainR2:-4.789905548095703--TestLoss:96.63520050048828--TestR2:-6.243354320526123\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1575--TrainLoss:88.1159896850586--TrainR2:-4.788854122161865--TestLoss:96.6180648803711--TestR2:-6.242069721221924\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1576--TrainLoss:88.09992980957031--TrainR2:-4.787798881530762--TestLoss:96.60094451904297--TestR2:-6.240786552429199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1577--TrainLoss:88.08392333984375--TrainR2:-4.786747455596924--TestLoss:96.58383178710938--TestR2:-6.239503860473633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1578--TrainLoss:88.06790924072266--TrainR2:-4.7856950759887695--TestLoss:96.56671905517578--TestR2:-6.238221168518066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1579--TrainLoss:88.05189514160156--TrainR2:-4.784643650054932--TestLoss:96.54960632324219--TestR2:-6.2369384765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1580--TrainLoss:88.03587341308594--TrainR2:-4.783590793609619--TestLoss:96.5324935913086--TestR2:-6.235655784606934\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1581--TrainLoss:88.01988220214844--TrainR2:-4.782540321350098--TestLoss:96.515380859375--TestR2:-6.234373092651367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1582--TrainLoss:88.00386810302734--TrainR2:-4.781487941741943--TestLoss:96.4982681274414--TestR2:-6.233090400695801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1583--TrainLoss:87.98786926269531--TrainR2:-4.780437469482422--TestLoss:96.4811782836914--TestR2:-6.231809139251709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1584--TrainLoss:87.97185516357422--TrainR2:-4.779384613037109--TestLoss:96.46405792236328--TestR2:-6.230525970458984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1585--TrainLoss:87.95585632324219--TrainR2:-4.778334140777588--TestLoss:96.44696044921875--TestR2:-6.229244232177734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1586--TrainLoss:87.93987274169922--TrainR2:-4.777284145355225--TestLoss:96.42984771728516--TestR2:-6.227962017059326\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1587--TrainLoss:87.92390441894531--TrainR2:-4.7762346267700195--TestLoss:96.41275787353516--TestR2:-6.226680755615234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1588--TrainLoss:87.90789031982422--TrainR2:-4.775182723999023--TestLoss:96.3956527709961--TestR2:-6.225398540496826\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1589--TrainLoss:87.89188385009766--TrainR2:-4.7741312980651855--TestLoss:96.37857055664062--TestR2:-6.224118232727051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1590--TrainLoss:87.8758773803711--TrainR2:-4.773079872131348--TestLoss:96.3614730834961--TestR2:-6.222836971282959\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1591--TrainLoss:87.85992431640625--TrainR2:-4.772031784057617--TestLoss:96.34437561035156--TestR2:-6.221555233001709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1592--TrainLoss:87.84392547607422--TrainR2:-4.7709808349609375--TestLoss:96.32730865478516--TestR2:-6.22027587890625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1593--TrainLoss:87.82794189453125--TrainR2:-4.769930362701416--TestLoss:96.31021118164062--TestR2:-6.218994140625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1594--TrainLoss:87.81195831298828--TrainR2:-4.768880367279053--TestLoss:96.29313659667969--TestR2:-6.217714786529541\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1595--TrainLoss:87.7959976196289--TrainR2:-4.767832279205322--TestLoss:96.27604675292969--TestR2:-6.216433525085449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1596--TrainLoss:87.78002166748047--TrainR2:-4.766782283782959--TestLoss:96.25897979736328--TestR2:-6.215154647827148\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1597--TrainLoss:87.76404571533203--TrainR2:-4.765732765197754--TestLoss:96.24190521240234--TestR2:-6.213874340057373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1598--TrainLoss:87.7480697631836--TrainR2:-4.764682769775391--TestLoss:96.22484588623047--TestR2:-6.2125959396362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1599--TrainLoss:87.73210144042969--TrainR2:-4.763634204864502--TestLoss:96.20777893066406--TestR2:-6.2113165855407715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1600--TrainLoss:87.71614837646484--TrainR2:-4.7625861167907715--TestLoss:96.19068908691406--TestR2:-6.210035800933838\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1601--TrainLoss:87.70018005371094--TrainR2:-4.761537075042725--TestLoss:96.17363739013672--TestR2:-6.2087578773498535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1602--TrainLoss:87.68419647216797--TrainR2:-4.760487079620361--TestLoss:96.15656280517578--TestR2:-6.207477569580078\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1603--TrainLoss:87.66826629638672--TrainR2:-4.7594404220581055--TestLoss:96.1395034790039--TestR2:-6.2061991691589355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1604--TrainLoss:87.65228271484375--TrainR2:-4.758390426635742--TestLoss:96.1224594116211--TestR2:-6.204921245574951\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1605--TrainLoss:87.63634490966797--TrainR2:-4.757343292236328--TestLoss:96.10539245605469--TestR2:-6.20364236831665\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1606--TrainLoss:87.62039184570312--TrainR2:-4.756295204162598--TestLoss:96.0883560180664--TestR2:-6.202364921569824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1607--TrainLoss:87.60444641113281--TrainR2:-4.755248069763184--TestLoss:96.07130432128906--TestR2:-6.20108699798584\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1608--TrainLoss:87.58848571777344--TrainR2:-4.754199028015137--TestLoss:96.05423736572266--TestR2:-6.199807643890381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1609--TrainLoss:87.57254791259766--TrainR2:-4.753151893615723--TestLoss:96.03719329833984--TestR2:-6.198530197143555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1610--TrainLoss:87.55658721923828--TrainR2:-4.752103805541992--TestLoss:96.02015686035156--TestR2:-6.197253227233887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1611--TrainLoss:87.54064178466797--TrainR2:-4.75105619430542--TestLoss:96.00312805175781--TestR2:-6.195976734161377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1612--TrainLoss:87.52470397949219--TrainR2:-4.750009059906006--TestLoss:95.986083984375--TestR2:-6.194699287414551\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1613--TrainLoss:87.5088119506836--TrainR2:-4.748964786529541--TestLoss:95.96903991699219--TestR2:-6.193421363830566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1614--TrainLoss:87.49287414550781--TrainR2:-4.747917652130127--TestLoss:95.95199584960938--TestR2:-6.192144393920898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1615--TrainLoss:87.47693634033203--TrainR2:-4.746870517730713--TestLoss:95.93498229980469--TestR2:-6.190868854522705\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1616--TrainLoss:87.46101379394531--TrainR2:-4.745824813842773--TestLoss:95.9179458618164--TestR2:-6.189591884613037\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1617--TrainLoss:87.44509887695312--TrainR2:-4.744779109954834--TestLoss:95.90091705322266--TestR2:-6.188315391540527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1618--TrainLoss:87.42916107177734--TrainR2:-4.74373197555542--TestLoss:95.88388061523438--TestR2:-6.187038898468018\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1619--TrainLoss:87.41322326660156--TrainR2:-4.742684841156006--TestLoss:95.86688232421875--TestR2:-6.185764312744141\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1620--TrainLoss:87.39729309082031--TrainR2:-4.741638660430908--TestLoss:95.84984588623047--TestR2:-6.184487342834473\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1621--TrainLoss:87.38139343261719--TrainR2:-4.740594387054443--TestLoss:95.83281707763672--TestR2:-6.183211326599121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1622--TrainLoss:87.365478515625--TrainR2:-4.739548683166504--TestLoss:95.81583404541016--TestR2:-6.1819376945495605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1623--TrainLoss:87.34954833984375--TrainR2:-4.738502502441406--TestLoss:95.79881286621094--TestR2:-6.180662155151367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1624--TrainLoss:87.33364868164062--TrainR2:-4.737457275390625--TestLoss:95.78178405761719--TestR2:-6.179385662078857\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1625--TrainLoss:87.31771850585938--TrainR2:-4.736410617828369--TestLoss:95.76478576660156--TestR2:-6.178111553192139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1626--TrainLoss:87.30184173583984--TrainR2:-4.735367774963379--TestLoss:95.74777221679688--TestR2:-6.1768364906311035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1627--TrainLoss:87.28594207763672--TrainR2:-4.734323501586914--TestLoss:95.73076629638672--TestR2:-6.175561904907227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1628--TrainLoss:87.27003479003906--TrainR2:-4.733278274536133--TestLoss:95.7137680053711--TestR2:-6.174287796020508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1629--TrainLoss:87.25414276123047--TrainR2:-4.732234477996826--TestLoss:95.69677734375--TestR2:-6.173014163970947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1630--TrainLoss:87.23824310302734--TrainR2:-4.731189727783203--TestLoss:95.67977142333984--TestR2:-6.171739101409912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1631--TrainLoss:87.22233581542969--TrainR2:-4.73014497756958--TestLoss:95.66277313232422--TestR2:-6.170465469360352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1632--TrainLoss:87.20642852783203--TrainR2:-4.729099750518799--TestLoss:95.64578247070312--TestR2:-6.169191837310791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1633--TrainLoss:87.1905517578125--TrainR2:-4.72805643081665--TestLoss:95.62881469726562--TestR2:-6.167919635772705\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1634--TrainLoss:87.17466735839844--TrainR2:-4.727013111114502--TestLoss:95.61180877685547--TestR2:-6.166645526885986\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1635--TrainLoss:87.15876007080078--TrainR2:-4.725967884063721--TestLoss:95.59481811523438--TestR2:-6.165371894836426\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1636--TrainLoss:87.14291381835938--TrainR2:-4.724926948547363--TestLoss:95.57783508300781--TestR2:-6.164098739624023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1637--TrainLoss:87.12705993652344--TrainR2:-4.723885536193848--TestLoss:95.56084442138672--TestR2:-6.162825584411621\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1638--TrainLoss:87.11114501953125--TrainR2:-4.722839832305908--TestLoss:95.54386138916016--TestR2:-6.161552429199219\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1639--TrainLoss:87.0952377319336--TrainR2:-4.721795082092285--TestLoss:95.5268783569336--TestR2:-6.160279273986816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1640--TrainLoss:87.07939147949219--TrainR2:-4.720754146575928--TestLoss:95.50992584228516--TestR2:-6.159008502960205\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1641--TrainLoss:87.06351470947266--TrainR2:-4.719710826873779--TestLoss:95.49293518066406--TestR2:-6.157735347747803\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1642--TrainLoss:87.04763793945312--TrainR2:-4.718667984008789--TestLoss:95.47599029541016--TestR2:-6.156464576721191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1643--TrainLoss:87.03175354003906--TrainR2:-4.717624187469482--TestLoss:95.4590072631836--TestR2:-6.155191898345947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1644--TrainLoss:87.01593780517578--TrainR2:-4.716585159301758--TestLoss:95.44205474853516--TestR2:-6.153921127319336\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1645--TrainLoss:87.00005340576172--TrainR2:-4.715541362762451--TestLoss:95.42507934570312--TestR2:-6.15264892578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1646--TrainLoss:86.98419189453125--TrainR2:-4.714499473571777--TestLoss:95.40811920166016--TestR2:-6.151377201080322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1647--TrainLoss:86.96833038330078--TrainR2:-4.7134575843811035--TestLoss:95.39117431640625--TestR2:-6.150107383728027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1648--TrainLoss:86.95246887207031--TrainR2:-4.71241569519043--TestLoss:95.37419891357422--TestR2:-6.148835182189941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1649--TrainLoss:86.93657684326172--TrainR2:-4.711371421813965--TestLoss:95.35725402832031--TestR2:-6.147564888000488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1650--TrainLoss:86.92076110839844--TrainR2:-4.71033239364624--TestLoss:95.3403091430664--TestR2:-6.146294593811035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1651--TrainLoss:86.90493774414062--TrainR2:-4.709293365478516--TestLoss:95.3233642578125--TestR2:-6.145024299621582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1652--TrainLoss:86.8890609741211--TrainR2:-4.708250045776367--TestLoss:95.30641174316406--TestR2:-6.143754482269287\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1653--TrainLoss:86.87324523925781--TrainR2:-4.707211017608643--TestLoss:95.28946685791016--TestR2:-6.142483711242676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1654--TrainLoss:86.85739135742188--TrainR2:-4.706169128417969--TestLoss:95.27252197265625--TestR2:-6.141213893890381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1655--TrainLoss:86.841552734375--TrainR2:-4.705129146575928--TestLoss:95.25557708740234--TestR2:-6.139943599700928\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1656--TrainLoss:86.82569885253906--TrainR2:-4.704087257385254--TestLoss:95.2386474609375--TestR2:-6.138674259185791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1657--TrainLoss:86.80990600585938--TrainR2:-4.703050136566162--TestLoss:95.22171783447266--TestR2:-6.137405872344971\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1658--TrainLoss:86.79402923583984--TrainR2:-4.702006816864014--TestLoss:95.20478057861328--TestR2:-6.136136054992676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1659--TrainLoss:86.7782211303711--TrainR2:-4.700967788696289--TestLoss:95.1878433227539--TestR2:-6.134866714477539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1660--TrainLoss:86.76235961914062--TrainR2:-4.699926376342773--TestLoss:95.17091369628906--TestR2:-6.1335978507995605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1661--TrainLoss:86.74652099609375--TrainR2:-4.698885440826416--TestLoss:95.15399169921875--TestR2:-6.132328987121582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1662--TrainLoss:86.73072052001953--TrainR2:-4.697847366333008--TestLoss:95.1370620727539--TestR2:-6.1310601234436035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1663--TrainLoss:86.71492767333984--TrainR2:-4.696810245513916--TestLoss:95.12014770507812--TestR2:-6.1297926902771\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1664--TrainLoss:86.69908142089844--TrainR2:-4.695769309997559--TestLoss:95.10323333740234--TestR2:-6.1285247802734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1665--TrainLoss:86.68328094482422--TrainR2:-4.69473123550415--TestLoss:95.0863037109375--TestR2:-6.127255916595459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1666--TrainLoss:86.66744995117188--TrainR2:-4.693691253662109--TestLoss:95.06938934326172--TestR2:-6.125988006591797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1667--TrainLoss:86.6516342163086--TrainR2:-4.692652225494385--TestLoss:95.052490234375--TestR2:-6.124721050262451\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1668--TrainLoss:86.63582611083984--TrainR2:-4.69161319732666--TestLoss:95.03558349609375--TestR2:-6.123453617095947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1669--TrainLoss:86.62004852294922--TrainR2:-4.690577030181885--TestLoss:95.0186538696289--TestR2:-6.122185230255127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1670--TrainLoss:86.60421752929688--TrainR2:-4.689537048339844--TestLoss:95.00176239013672--TestR2:-6.1209187507629395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1671--TrainLoss:86.58840942382812--TrainR2:-4.688498497009277--TestLoss:94.98485565185547--TestR2:-6.119651794433594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1672--TrainLoss:86.57259368896484--TrainR2:-4.6874589920043945--TestLoss:94.96796417236328--TestR2:-6.118385314941406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1673--TrainLoss:86.5567626953125--TrainR2:-4.686419486999512--TestLoss:94.9510498046875--TestR2:-6.117117881774902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1674--TrainLoss:86.541015625--TrainR2:-4.685384750366211--TestLoss:94.93417358398438--TestR2:-6.1158528327941895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1675--TrainLoss:86.52521514892578--TrainR2:-4.684346675872803--TestLoss:94.9172592163086--TestR2:-6.114584922790527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1676--TrainLoss:86.50942993164062--TrainR2:-4.683309555053711--TestLoss:94.9003677368164--TestR2:-6.11331844329834\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1677--TrainLoss:86.49361419677734--TrainR2:-4.6822710037231445--TestLoss:94.88349151611328--TestR2:-6.112053394317627\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1678--TrainLoss:86.47782897949219--TrainR2:-4.6812334060668945--TestLoss:94.86660766601562--TestR2:-6.110787868499756\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1679--TrainLoss:86.46208953857422--TrainR2:-4.68019962310791--TestLoss:94.84972381591797--TestR2:-6.109522342681885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1680--TrainLoss:86.4462661743164--TrainR2:-4.6791605949401855--TestLoss:94.83283996582031--TestR2:-6.108256816864014\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1681--TrainLoss:86.43048095703125--TrainR2:-4.6781229972839355--TestLoss:94.81595611572266--TestR2:-6.106991767883301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1682--TrainLoss:86.41471099853516--TrainR2:-4.677087306976318--TestLoss:94.79908752441406--TestR2:-6.105727672576904\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1683--TrainLoss:86.3989486694336--TrainR2:-4.676051616668701--TestLoss:94.7822036743164--TestR2:-6.104461669921875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1684--TrainLoss:86.38318634033203--TrainR2:-4.675016403198242--TestLoss:94.76532745361328--TestR2:-6.103196620941162\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1685--TrainLoss:86.36740112304688--TrainR2:-4.673978805541992--TestLoss:94.74845123291016--TestR2:-6.101931571960449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1686--TrainLoss:86.35160064697266--TrainR2:-4.672941207885742--TestLoss:94.7315902709961--TestR2:-6.100667953491211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1687--TrainLoss:86.33584594726562--TrainR2:-4.671905994415283--TestLoss:94.7147216796875--TestR2:-6.099403381347656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1688--TrainLoss:86.32009887695312--TrainR2:-4.670871734619141--TestLoss:94.6978530883789--TestR2:-6.09813928604126\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1689--TrainLoss:86.3043441772461--TrainR2:-4.669836521148682--TestLoss:94.68099212646484--TestR2:-6.0968756675720215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1690--TrainLoss:86.28857421875--TrainR2:-4.668800354003906--TestLoss:94.66414642333984--TestR2:-6.095612525939941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1691--TrainLoss:86.27279663085938--TrainR2:-4.667763710021973--TestLoss:94.64727783203125--TestR2:-6.094348430633545\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1692--TrainLoss:86.2570571899414--TrainR2:-4.666729927062988--TestLoss:94.63042449951172--TestR2:-6.093084812164307\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1693--TrainLoss:86.24130249023438--TrainR2:-4.665694713592529--TestLoss:94.61357116699219--TestR2:-6.091821670532227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1694--TrainLoss:86.22554016113281--TrainR2:-4.66465950012207--TestLoss:94.59671020507812--TestR2:-6.090558052062988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1695--TrainLoss:86.20977783203125--TrainR2:-4.663623809814453--TestLoss:94.57986450195312--TestR2:-6.089295387268066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1696--TrainLoss:86.19408416748047--TrainR2:-4.66259241104126--TestLoss:94.5630111694336--TestR2:-6.088032245635986\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1697--TrainLoss:86.17831420898438--TrainR2:-4.661556720733643--TestLoss:94.54617309570312--TestR2:-6.086770057678223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1698--TrainLoss:86.16255950927734--TrainR2:-4.660521507263184--TestLoss:94.52933502197266--TestR2:-6.085507869720459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1699--TrainLoss:86.14682006835938--TrainR2:-4.659487724304199--TestLoss:94.51250457763672--TestR2:-6.0842461585998535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1700--TrainLoss:86.131103515625--TrainR2:-4.6584553718566895--TestLoss:94.49567413330078--TestR2:-6.082984447479248\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1701--TrainLoss:86.11534881591797--TrainR2:-4.6574201583862305--TestLoss:94.47884368896484--TestR2:-6.081723213195801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1702--TrainLoss:86.09961700439453--TrainR2:-4.656386852264404--TestLoss:94.46199798583984--TestR2:-6.080460071563721\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1703--TrainLoss:86.08385467529297--TrainR2:-4.655351161956787--TestLoss:94.44515228271484--TestR2:-6.079197883605957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1704--TrainLoss:86.06814575195312--TrainR2:-4.6543192863464355--TestLoss:94.4283447265625--TestR2:-6.077938079833984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1705--TrainLoss:86.05240631103516--TrainR2:-4.653285503387451--TestLoss:94.41150665283203--TestR2:-6.076675891876221\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1706--TrainLoss:86.03668975830078--TrainR2:-4.652252674102783--TestLoss:94.39469146728516--TestR2:-6.07541561126709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1707--TrainLoss:86.02098846435547--TrainR2:-4.65122127532959--TestLoss:94.37786102294922--TestR2:-6.074153900146484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1708--TrainLoss:86.00527954101562--TrainR2:-4.65018892288208--TestLoss:94.3610610961914--TestR2:-6.07289457321167\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1709--TrainLoss:85.9895248413086--TrainR2:-4.649154186248779--TestLoss:94.34423828125--TestR2:-6.071633815765381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1710--TrainLoss:85.97381591796875--TrainR2:-4.648122310638428--TestLoss:94.32743072509766--TestR2:-6.070374011993408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1711--TrainLoss:85.95809173583984--TrainR2:-4.647089004516602--TestLoss:94.31061553955078--TestR2:-6.069113254547119\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1712--TrainLoss:85.94239807128906--TrainR2:-4.646058082580566--TestLoss:94.29379272460938--TestR2:-6.06785249710083\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1713--TrainLoss:85.92667388916016--TrainR2:-4.645025253295898--TestLoss:94.2769775390625--TestR2:-6.066592216491699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1714--TrainLoss:85.9109878540039--TrainR2:-4.643994331359863--TestLoss:94.26020050048828--TestR2:-6.065334320068359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1715--TrainLoss:85.89527130126953--TrainR2:-4.6429619789123535--TestLoss:94.2433853149414--TestR2:-6.0640740394592285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1716--TrainLoss:85.87955474853516--TrainR2:-4.641929626464844--TestLoss:94.2265853881836--TestR2:-6.062815189361572\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1717--TrainLoss:85.86384582519531--TrainR2:-4.640897274017334--TestLoss:94.20979309082031--TestR2:-6.061556339263916\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1718--TrainLoss:85.8481674194336--TrainR2:-4.639867305755615--TestLoss:94.1929931640625--TestR2:-6.060297012329102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1719--TrainLoss:85.83246612548828--TrainR2:-4.638835906982422--TestLoss:94.17620086669922--TestR2:-6.059038162231445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1720--TrainLoss:85.81674194335938--TrainR2:-4.637802600860596--TestLoss:94.15940856933594--TestR2:-6.057779312133789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1721--TrainLoss:85.80110168457031--TrainR2:-4.636775493621826--TestLoss:94.14261627197266--TestR2:-6.056520938873291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1722--TrainLoss:85.78538513183594--TrainR2:-4.635743141174316--TestLoss:94.12584686279297--TestR2:-6.055263996124268\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1723--TrainLoss:85.76969909667969--TrainR2:-4.634712219238281--TestLoss:94.10904693603516--TestR2:-6.054004669189453\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1724--TrainLoss:85.75401306152344--TrainR2:-4.6336822509765625--TestLoss:94.09223937988281--TestR2:-6.0527448654174805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1725--TrainLoss:85.73832702636719--TrainR2:-4.632651329040527--TestLoss:94.07548522949219--TestR2:-6.051488876342773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1726--TrainLoss:85.7226333618164--TrainR2:-4.63162088394165--TestLoss:94.05870819091797--TestR2:-6.050231456756592\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1727--TrainLoss:85.70693969726562--TrainR2:-4.630589485168457--TestLoss:94.04192352294922--TestR2:-6.048973560333252\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1728--TrainLoss:85.6913070678711--TrainR2:-4.629562854766846--TestLoss:94.02516174316406--TestR2:-6.047717094421387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1729--TrainLoss:85.67564392089844--TrainR2:-4.628533363342285--TestLoss:94.00838470458984--TestR2:-6.046459197998047\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1730--TrainLoss:85.65994262695312--TrainR2:-4.627501964569092--TestLoss:93.99162292480469--TestR2:-6.04520320892334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1731--TrainLoss:85.64432525634766--TrainR2:-4.626476287841797--TestLoss:93.97484588623047--TestR2:-6.043945789337158\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1732--TrainLoss:85.62861633300781--TrainR2:-4.625443935394287--TestLoss:93.9581069946289--TestR2:-6.042691230773926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1733--TrainLoss:85.61294555664062--TrainR2:-4.624414920806885--TestLoss:93.94132995605469--TestR2:-6.041433334350586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1734--TrainLoss:85.59730529785156--TrainR2:-4.623386859893799--TestLoss:93.92457580566406--TestR2:-6.040177345275879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1735--TrainLoss:85.58162689208984--TrainR2:-4.62235689163208--TestLoss:93.90782928466797--TestR2:-6.038922309875488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1736--TrainLoss:85.56595611572266--TrainR2:-4.621327877044678--TestLoss:93.89105987548828--TestR2:-6.037665843963623\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1737--TrainLoss:85.55030822753906--TrainR2:-4.620299339294434--TestLoss:93.87430572509766--TestR2:-6.036409378051758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1738--TrainLoss:85.5346450805664--TrainR2:-4.619270324707031--TestLoss:93.8575668334961--TestR2:-6.035154819488525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1739--TrainLoss:85.51903533935547--TrainR2:-4.618244647979736--TestLoss:93.8407974243164--TestR2:-6.033897876739502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1740--TrainLoss:85.50337982177734--TrainR2:-4.617216110229492--TestLoss:93.82405853271484--TestR2:-6.032642841339111\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1741--TrainLoss:85.4876937866211--TrainR2:-4.616186141967773--TestLoss:93.80731201171875--TestR2:-6.031387805938721\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1742--TrainLoss:85.4721450805664--TrainR2:-4.615164279937744--TestLoss:93.79058074951172--TestR2:-6.0301337242126465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1743--TrainLoss:85.4564437866211--TrainR2:-4.614132881164551--TestLoss:93.77384185791016--TestR2:-6.028879165649414\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1744--TrainLoss:85.4407958984375--TrainR2:-4.613104820251465--TestLoss:93.75711822509766--TestR2:-6.027625560760498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1745--TrainLoss:85.42514038085938--TrainR2:-4.612076759338379--TestLoss:93.74036407470703--TestR2:-6.026370048522949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1746--TrainLoss:85.40953826904297--TrainR2:-4.611051559448242--TestLoss:93.72364807128906--TestR2:-6.025116920471191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1747--TrainLoss:85.39387512207031--TrainR2:-4.61002254486084--TestLoss:93.7069091796875--TestR2:-6.023861885070801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1748--TrainLoss:85.37826538085938--TrainR2:-4.608996868133545--TestLoss:93.69017028808594--TestR2:-6.022607803344727\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1749--TrainLoss:85.36260986328125--TrainR2:-4.607968330383301--TestLoss:93.67345428466797--TestR2:-6.021354675292969\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1750--TrainLoss:85.34696960449219--TrainR2:-4.606941223144531--TestLoss:93.65672302246094--TestR2:-6.0201005935668945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1751--TrainLoss:85.33135986328125--TrainR2:-4.605915546417236--TestLoss:93.64002227783203--TestR2:-6.018848896026611\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1752--TrainLoss:85.31575012207031--TrainR2:-4.604889869689941--TestLoss:93.623291015625--TestR2:-6.017594814300537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1753--TrainLoss:85.30017852783203--TrainR2:-4.603867053985596--TestLoss:93.6065902709961--TestR2:-6.016342639923096\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1754--TrainLoss:85.28451538085938--TrainR2:-4.602838039398193--TestLoss:93.5898666381836--TestR2:-6.015089511871338\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1755--TrainLoss:85.2688980102539--TrainR2:-4.60181188583374--TestLoss:93.57315063476562--TestR2:-6.013835906982422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1756--TrainLoss:85.2532958984375--TrainR2:-4.6007866859436035--TestLoss:93.55644989013672--TestR2:-6.012584209442139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1757--TrainLoss:85.23766326904297--TrainR2:-4.599760055541992--TestLoss:93.53972625732422--TestR2:-6.011331081390381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1758--TrainLoss:85.2220687866211--TrainR2:-4.598735332489014--TestLoss:93.52303314208984--TestR2:-6.010079860687256\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1759--TrainLoss:85.2064437866211--TrainR2:-4.5977091789245605--TestLoss:93.50633239746094--TestR2:-6.0088276863098145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1760--TrainLoss:85.19087219238281--TrainR2:-4.596685886383057--TestLoss:93.4896240234375--TestR2:-6.007575511932373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1761--TrainLoss:85.1752700805664--TrainR2:-4.595661163330078--TestLoss:93.47293853759766--TestR2:-6.006324768066406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1762--TrainLoss:85.15966033935547--TrainR2:-4.594635486602783--TestLoss:93.45624542236328--TestR2:-6.0050740242004395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1763--TrainLoss:85.14408111572266--TrainR2:-4.593611717224121--TestLoss:93.4395523071289--TestR2:-6.003822326660156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1764--TrainLoss:85.12848663330078--TrainR2:-4.592587471008301--TestLoss:93.42285919189453--TestR2:-6.002571105957031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1765--TrainLoss:85.11285400390625--TrainR2:-4.591560363769531--TestLoss:93.40616607666016--TestR2:-6.001319408416748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1766--TrainLoss:85.09728240966797--TrainR2:-4.5905375480651855--TestLoss:93.38947296142578--TestR2:-6.000068664550781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1767--TrainLoss:85.08168029785156--TrainR2:-4.589512348175049--TestLoss:93.37278747558594--TestR2:-5.9988179206848145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1768--TrainLoss:85.0661392211914--TrainR2:-4.588491439819336--TestLoss:93.35611724853516--TestR2:-5.997568607330322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1769--TrainLoss:85.05052185058594--TrainR2:-4.587465286254883--TestLoss:93.33944702148438--TestR2:-5.99631929397583\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1770--TrainLoss:85.034912109375--TrainR2:-4.586440086364746--TestLoss:93.32276916503906--TestR2:-5.995068550109863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1771--TrainLoss:85.01936340332031--TrainR2:-4.585418701171875--TestLoss:93.30607604980469--TestR2:-5.9938178062438965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1772--TrainLoss:85.00377655029297--TrainR2:-4.584394931793213--TestLoss:93.28942108154297--TestR2:-5.9925689697265625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1773--TrainLoss:84.98819732666016--TrainR2:-4.583371162414551--TestLoss:93.27273559570312--TestR2:-5.991318702697754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1774--TrainLoss:84.97262573242188--TrainR2:-4.582347869873047--TestLoss:93.25607299804688--TestR2:-5.990069389343262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1775--TrainLoss:84.95707702636719--TrainR2:-4.581326484680176--TestLoss:93.2394027709961--TestR2:-5.9888200759887695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1776--TrainLoss:84.94152069091797--TrainR2:-4.5803046226501465--TestLoss:93.22273254394531--TestR2:-5.987570285797119\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1777--TrainLoss:84.9259262084961--TrainR2:-4.579280376434326--TestLoss:93.20606994628906--TestR2:-5.986321926116943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1778--TrainLoss:84.91036987304688--TrainR2:-4.578258514404297--TestLoss:93.18942260742188--TestR2:-5.985073566436768\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1779--TrainLoss:84.89481353759766--TrainR2:-4.577236175537109--TestLoss:93.17276763916016--TestR2:-5.983825206756592\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1780--TrainLoss:84.87925720214844--TrainR2:-4.57621431350708--TestLoss:93.15612030029297--TestR2:-5.982577323913574\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1781--TrainLoss:84.86370086669922--TrainR2:-4.575192451477051--TestLoss:93.13946533203125--TestR2:-5.981329441070557\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1782--TrainLoss:84.84815979003906--TrainR2:-4.57417106628418--TestLoss:93.12278747558594--TestR2:-5.980079174041748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1783--TrainLoss:84.83257293701172--TrainR2:-4.573147296905518--TestLoss:93.10615539550781--TestR2:-5.978832721710205\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1784--TrainLoss:84.81700897216797--TrainR2:-4.57212495803833--TestLoss:93.08950805664062--TestR2:-5.977584362030029\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1785--TrainLoss:84.80149841308594--TrainR2:-4.571105480194092--TestLoss:93.0728759765625--TestR2:-5.976337432861328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1786--TrainLoss:84.78589630126953--TrainR2:-4.570080757141113--TestLoss:93.05623626708984--TestR2:-5.975090503692627\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1787--TrainLoss:84.77041625976562--TrainR2:-4.569064140319824--TestLoss:93.03958129882812--TestR2:-5.973842144012451\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1788--TrainLoss:84.75485229492188--TrainR2:-4.5680413246154785--TestLoss:93.02295684814453--TestR2:-5.972596168518066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1789--TrainLoss:84.73931121826172--TrainR2:-4.567019939422607--TestLoss:93.00630950927734--TestR2:-5.971348285675049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1790--TrainLoss:84.72377014160156--TrainR2:-4.565999507904053--TestLoss:92.98969268798828--TestR2:-5.970102787017822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1791--TrainLoss:84.70825958251953--TrainR2:-4.5649800300598145--TestLoss:92.97306060791016--TestR2:-5.968856334686279\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1792--TrainLoss:84.69271087646484--TrainR2:-4.563958644866943--TestLoss:92.9564208984375--TestR2:-5.96760892868042\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1793--TrainLoss:84.67719268798828--TrainR2:-4.562939167022705--TestLoss:92.9397964477539--TestR2:-5.966362953186035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1794--TrainLoss:84.66162872314453--TrainR2:-4.561916828155518--TestLoss:92.92317962646484--TestR2:-5.965117454528809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1795--TrainLoss:84.64612579345703--TrainR2:-4.560898303985596--TestLoss:92.90657043457031--TestR2:-5.96387243270874\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1796--TrainLoss:84.63059997558594--TrainR2:-4.559878349304199--TestLoss:92.88994598388672--TestR2:-5.962625980377197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1797--TrainLoss:84.61509704589844--TrainR2:-4.5588603019714355--TestLoss:92.87332153320312--TestR2:-5.9613800048828125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1798--TrainLoss:84.59956359863281--TrainR2:-4.557839393615723--TestLoss:92.85670471191406--TestR2:-5.960134506225586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1799--TrainLoss:84.58403778076172--TrainR2:-4.556819438934326--TestLoss:92.84010314941406--TestR2:-5.958889961242676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1800--TrainLoss:84.56851959228516--TrainR2:-4.555799961090088--TestLoss:92.82350158691406--TestR2:-5.957645416259766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1801--TrainLoss:84.55298614501953--TrainR2:-4.554779529571533--TestLoss:92.806884765625--TestR2:-5.956400394439697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1802--TrainLoss:84.53751373291016--TrainR2:-4.553762912750244--TestLoss:92.79029846191406--TestR2:-5.9551568031311035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1803--TrainLoss:84.52197265625--TrainR2:-4.5527424812316895--TestLoss:92.77367401123047--TestR2:-5.953911304473877\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1804--TrainLoss:84.5064926147461--TrainR2:-4.551724910736084--TestLoss:92.75706481933594--TestR2:-5.952666282653809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1805--TrainLoss:84.49097442626953--TrainR2:-4.550705432891846--TestLoss:92.740478515625--TestR2:-5.951422691345215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1806--TrainLoss:84.4754867553711--TrainR2:-4.549688339233398--TestLoss:92.723876953125--TestR2:-5.950178623199463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1807--TrainLoss:84.45997619628906--TrainR2:-4.54866886138916--TestLoss:92.70730590820312--TestR2:-5.948936462402344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1808--TrainLoss:84.44451141357422--TrainR2:-4.5476531982421875--TestLoss:92.6906967163086--TestR2:-5.947691440582275\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1809--TrainLoss:84.4289779663086--TrainR2:-4.546632766723633--TestLoss:92.67411041259766--TestR2:-5.94644832611084\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1810--TrainLoss:84.41349029541016--TrainR2:-4.545615196228027--TestLoss:92.65753173828125--TestR2:-5.9452056884765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1811--TrainLoss:84.39800262451172--TrainR2:-4.544597625732422--TestLoss:92.64093017578125--TestR2:-5.943961143493652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1812--TrainLoss:84.38253021240234--TrainR2:-4.543581485748291--TestLoss:92.62435913085938--TestR2:-5.942718982696533\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1813--TrainLoss:84.36701965332031--TrainR2:-4.542562484741211--TestLoss:92.60775756835938--TestR2:-5.941474914550781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1814--TrainLoss:84.35154724121094--TrainR2:-4.541545867919922--TestLoss:92.59119415283203--TestR2:-5.94023323059082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1815--TrainLoss:84.3360595703125--TrainR2:-4.540528297424316--TestLoss:92.57462310791016--TestR2:-5.938991069793701\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1816--TrainLoss:84.3205795288086--TrainR2:-4.539511680603027--TestLoss:92.55804443359375--TestR2:-5.937748432159424\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1817--TrainLoss:84.30513000488281--TrainR2:-4.538496494293213--TestLoss:92.54147338867188--TestR2:-5.936506271362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1818--TrainLoss:84.28962707519531--TrainR2:-4.537477970123291--TestLoss:92.52490997314453--TestR2:-5.935264587402344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1819--TrainLoss:84.27421569824219--TrainR2:-4.536465644836426--TestLoss:92.50834655761719--TestR2:-5.934022903442383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1820--TrainLoss:84.25868225097656--TrainR2:-4.535444736480713--TestLoss:92.49178314208984--TestR2:-5.93278169631958\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1821--TrainLoss:84.24323272705078--TrainR2:-4.534430027008057--TestLoss:92.4752197265625--TestR2:-5.931540012359619\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1822--TrainLoss:84.22773742675781--TrainR2:-4.533411979675293--TestLoss:92.45865631103516--TestR2:-5.930298805236816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1823--TrainLoss:84.21231079101562--TrainR2:-4.532398700714111--TestLoss:92.44210052490234--TestR2:-5.929057598114014\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1824--TrainLoss:84.19683074951172--TrainR2:-4.531381607055664--TestLoss:92.425537109375--TestR2:-5.927816390991211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1825--TrainLoss:84.18138885498047--TrainR2:-4.530366897583008--TestLoss:92.40898895263672--TestR2:-5.926575660705566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1826--TrainLoss:84.16590118408203--TrainR2:-4.5293498039245605--TestLoss:92.39244079589844--TestR2:-5.92533540725708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1827--TrainLoss:84.15046691894531--TrainR2:-4.5283355712890625--TestLoss:92.37589263916016--TestR2:-5.924095153808594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1828--TrainLoss:84.13499450683594--TrainR2:-4.527319431304932--TestLoss:92.35933685302734--TestR2:-5.922853946685791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1829--TrainLoss:84.11956024169922--TrainR2:-4.526305198669434--TestLoss:92.34278869628906--TestR2:-5.921614170074463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1830--TrainLoss:84.10411834716797--TrainR2:-4.5252909660339355--TestLoss:92.32624816894531--TestR2:-5.920373916625977\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1831--TrainLoss:84.08867645263672--TrainR2:-4.524276256561279--TestLoss:92.3097152709961--TestR2:-5.919134616851807\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1832--TrainLoss:84.07322692871094--TrainR2:-4.523261547088623--TestLoss:92.29317474365234--TestR2:-5.9178948402404785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1833--TrainLoss:84.05776977539062--TrainR2:-4.52224588394165--TestLoss:92.27662658691406--TestR2:-5.916654586791992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1834--TrainLoss:84.04234313964844--TrainR2:-4.521232604980469--TestLoss:92.26011657714844--TestR2:-5.915416717529297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1835--TrainLoss:84.02689361572266--TrainR2:-4.5202178955078125--TestLoss:92.24357604980469--TestR2:-5.914177417755127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1836--TrainLoss:84.01146697998047--TrainR2:-4.519204139709473--TestLoss:92.22706604003906--TestR2:-5.912939548492432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1837--TrainLoss:83.99603271484375--TrainR2:-4.518189907073975--TestLoss:92.21053314208984--TestR2:-5.91170072555542\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1838--TrainLoss:83.98062133789062--TrainR2:-4.517177581787109--TestLoss:92.19400787353516--TestR2:-5.910461902618408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1839--TrainLoss:83.96517944335938--TrainR2:-4.516163349151611--TestLoss:92.177490234375--TestR2:-5.909223556518555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1840--TrainLoss:83.9497299194336--TrainR2:-4.515148162841797--TestLoss:92.16095733642578--TestR2:-5.907984256744385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1841--TrainLoss:83.93433380126953--TrainR2:-4.514136791229248--TestLoss:92.14444732666016--TestR2:-5.9067463874816895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1842--TrainLoss:83.9189224243164--TrainR2:-4.513123989105225--TestLoss:92.12792205810547--TestR2:-5.905508518218994\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1843--TrainLoss:83.90348815917969--TrainR2:-4.512110233306885--TestLoss:92.11140441894531--TestR2:-5.904270172119141\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1844--TrainLoss:83.88804626464844--TrainR2:-4.5110955238342285--TestLoss:92.09490966796875--TestR2:-5.90303373336792\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1845--TrainLoss:83.87267303466797--TrainR2:-4.510085582733154--TestLoss:92.0783920288086--TestR2:-5.901795864105225\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1846--TrainLoss:83.85724639892578--TrainR2:-4.509072303771973--TestLoss:92.06189727783203--TestR2:-5.900559425354004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1847--TrainLoss:83.8418197631836--TrainR2:-4.508059024810791--TestLoss:92.04537200927734--TestR2:-5.899320602416992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1848--TrainLoss:83.82642364501953--TrainR2:-4.507047176361084--TestLoss:92.02888488769531--TestR2:-5.89808464050293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1849--TrainLoss:83.81099700927734--TrainR2:-4.506033897399902--TestLoss:92.01237487792969--TestR2:-5.896847248077393\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1850--TrainLoss:83.79560852050781--TrainR2:-4.5050225257873535--TestLoss:91.99589538574219--TestR2:-5.895611763000488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1851--TrainLoss:83.78022766113281--TrainR2:-4.504012584686279--TestLoss:91.97937774658203--TestR2:-5.894374370574951\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1852--TrainLoss:83.76480102539062--TrainR2:-4.502999305725098--TestLoss:91.96289825439453--TestR2:-5.893138885498047\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1853--TrainLoss:83.7494125366211--TrainR2:-4.501987934112549--TestLoss:91.9464111328125--TestR2:-5.891903400421143\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1854--TrainLoss:83.73399353027344--TrainR2:-4.500975131988525--TestLoss:91.92992401123047--TestR2:-5.89066743850708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1855--TrainLoss:83.71861267089844--TrainR2:-4.499964714050293--TestLoss:91.9134521484375--TestR2:-5.889432430267334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1856--TrainLoss:83.70325469970703--TrainR2:-4.498955726623535--TestLoss:91.89695739746094--TestR2:-5.888195991516113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1857--TrainLoss:83.68782043457031--TrainR2:-4.497941493988037--TestLoss:91.8804702758789--TestR2:-5.886960506439209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1858--TrainLoss:83.67247009277344--TrainR2:-4.4969329833984375--TestLoss:91.86400604248047--TestR2:-5.885726451873779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1859--TrainLoss:83.65705108642578--TrainR2:-4.495920658111572--TestLoss:91.84751892089844--TestR2:-5.884490489959717\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1860--TrainLoss:83.64167022705078--TrainR2:-4.494909763336182--TestLoss:91.8310546875--TestR2:-5.883256435394287\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1861--TrainLoss:83.62635803222656--TrainR2:-4.493904113769531--TestLoss:91.8145751953125--TestR2:-5.882020950317383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1862--TrainLoss:83.61092376708984--TrainR2:-4.492889881134033--TestLoss:91.798095703125--TestR2:-5.880785942077637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1863--TrainLoss:83.59555053710938--TrainR2:-4.491879940032959--TestLoss:91.78163146972656--TestR2:-5.879551887512207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1864--TrainLoss:83.58019256591797--TrainR2:-4.490870952606201--TestLoss:91.76516723632812--TestR2:-5.878317832946777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1865--TrainLoss:83.56480407714844--TrainR2:-4.4898600578308105--TestLoss:91.74869537353516--TestR2:-5.8770833015441895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1866--TrainLoss:83.5494384765625--TrainR2:-4.4888505935668945--TestLoss:91.73223876953125--TestR2:-5.875849723815918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1867--TrainLoss:83.53406524658203--TrainR2:-4.48784065246582--TestLoss:91.71576690673828--TestR2:-5.87461519241333\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1868--TrainLoss:83.51869201660156--TrainR2:-4.486830711364746--TestLoss:91.6993179321289--TestR2:-5.873382091522217\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1869--TrainLoss:83.50334167480469--TrainR2:-4.485822677612305--TestLoss:91.68287658691406--TestR2:-5.87214994430542\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1870--TrainLoss:83.48798370361328--TrainR2:-4.484813213348389--TestLoss:91.66642761230469--TestR2:-5.870916843414307\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1871--TrainLoss:83.4726333618164--TrainR2:-4.483804702758789--TestLoss:91.64996337890625--TestR2:-5.869682788848877\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1872--TrainLoss:83.45730590820312--TrainR2:-4.482798099517822--TestLoss:91.63352966308594--TestR2:-5.868451118469238\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1873--TrainLoss:83.44192504882812--TrainR2:-4.481787204742432--TestLoss:91.61708068847656--TestR2:-5.867218017578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1874--TrainLoss:83.42656707763672--TrainR2:-4.480778217315674--TestLoss:91.60062408447266--TestR2:-5.8659844398498535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1875--TrainLoss:83.41124725341797--TrainR2:-4.479772090911865--TestLoss:91.58417510986328--TestR2:-5.864751815795898\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1876--TrainLoss:83.39588165283203--TrainR2:-4.478762149810791--TestLoss:91.56776428222656--TestR2:-5.863521099090576\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1877--TrainLoss:83.38052368164062--TrainR2:-4.477753639221191--TestLoss:91.55132293701172--TestR2:-5.862288475036621\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1878--TrainLoss:83.36521911621094--TrainR2:-4.476748466491699--TestLoss:91.5348892211914--TestR2:-5.861056804656982\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1879--TrainLoss:83.34984588623047--TrainR2:-4.475738048553467--TestLoss:91.5184555053711--TestR2:-5.859825134277344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1880--TrainLoss:83.33451843261719--TrainR2:-4.4747314453125--TestLoss:91.50200653076172--TestR2:-5.858592510223389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1881--TrainLoss:83.31918334960938--TrainR2:-4.473723888397217--TestLoss:91.485595703125--TestR2:-5.857362270355225\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1882--TrainLoss:83.30379486083984--TrainR2:-4.472712993621826--TestLoss:91.46916961669922--TestR2:-5.856131076812744\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1883--TrainLoss:83.28852081298828--TrainR2:-4.471709251403809--TestLoss:91.4527359008789--TestR2:-5.8548994064331055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1884--TrainLoss:83.27320098876953--TrainR2:-4.470703125--TestLoss:91.43633270263672--TestR2:-5.8536696434021\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1885--TrainLoss:83.25784301757812--TrainR2:-4.469694137573242--TestLoss:91.41991424560547--TestR2:-5.852438926696777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1886--TrainLoss:83.24253845214844--TrainR2:-4.468688488006592--TestLoss:91.40348052978516--TestR2:-5.851207256317139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1887--TrainLoss:83.22723388671875--TrainR2:-4.467682838439941--TestLoss:91.38706970214844--TestR2:-5.849977016448975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1888--TrainLoss:83.21186828613281--TrainR2:-4.466673851013184--TestLoss:91.37066650390625--TestR2:-5.848747730255127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1889--TrainLoss:83.19656372070312--TrainR2:-4.465668201446533--TestLoss:91.354248046875--TestR2:-5.847517013549805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1890--TrainLoss:83.18124389648438--TrainR2:-4.464662075042725--TestLoss:91.33784484863281--TestR2:-5.846287727355957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1891--TrainLoss:83.16592407226562--TrainR2:-4.463655471801758--TestLoss:91.32142639160156--TestR2:-5.845057010650635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1892--TrainLoss:83.15061950683594--TrainR2:-4.462649822235107--TestLoss:91.30504608154297--TestR2:-5.8438286781311035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1893--TrainLoss:83.13532257080078--TrainR2:-4.461644649505615--TestLoss:91.28864288330078--TestR2:-5.842599391937256\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1894--TrainLoss:83.12001037597656--TrainR2:-4.460638999938965--TestLoss:91.2722396850586--TestR2:-5.841370105743408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1895--TrainLoss:83.10470581054688--TrainR2:-4.459633827209473--TestLoss:91.25584411621094--TestR2:-5.8401408195495605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1896--TrainLoss:83.08943176269531--TrainR2:-4.458630084991455--TestLoss:91.23945617675781--TestR2:-5.8389129638671875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1897--TrainLoss:83.0741195678711--TrainR2:-4.4576239585876465--TestLoss:91.2230453491211--TestR2:-5.837682723999023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1898--TrainLoss:83.05878448486328--TrainR2:-4.4566168785095215--TestLoss:91.2066650390625--TestR2:-5.836454391479492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1899--TrainLoss:83.04351806640625--TrainR2:-4.455613613128662--TestLoss:91.19027709960938--TestR2:-5.835226535797119\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1900--TrainLoss:83.02821350097656--TrainR2:-4.454607963562012--TestLoss:91.17389678955078--TestR2:-5.833998680114746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1901--TrainLoss:83.0129165649414--TrainR2:-4.453603744506836--TestLoss:91.15751647949219--TestR2:-5.832770824432373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1902--TrainLoss:82.99762725830078--TrainR2:-4.452599048614502--TestLoss:91.1411361694336--TestR2:-5.831543445587158\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1903--TrainLoss:82.98237609863281--TrainR2:-4.451597213745117--TestLoss:91.12474822998047--TestR2:-5.830314636230469\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1904--TrainLoss:82.96704864501953--TrainR2:-4.450590133666992--TestLoss:91.1083755493164--TestR2:-5.829087257385254\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1905--TrainLoss:82.9517593383789--TrainR2:-4.449585437774658--TestLoss:91.09200286865234--TestR2:-5.827860355377197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1906--TrainLoss:82.93651580810547--TrainR2:-4.448584079742432--TestLoss:91.07563018798828--TestR2:-5.826633453369141\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1907--TrainLoss:82.92123413085938--TrainR2:-4.447580337524414--TestLoss:91.05924987792969--TestR2:-5.825405120849609\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1908--TrainLoss:82.90592193603516--TrainR2:-4.4465742111206055--TestLoss:91.04288482666016--TestR2:-5.824178218841553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1909--TrainLoss:82.89065551757812--TrainR2:-4.445571422576904--TestLoss:91.0265121459961--TestR2:-5.822951316833496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1910--TrainLoss:82.87537384033203--TrainR2:-4.444567680358887--TestLoss:91.01016998291016--TestR2:-5.8217267990112305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1911--TrainLoss:82.86013793945312--TrainR2:-4.443566799163818--TestLoss:90.99380493164062--TestR2:-5.820499420166016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1912--TrainLoss:82.84481048583984--TrainR2:-4.442559242248535--TestLoss:90.9774398803711--TestR2:-5.819272994995117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1913--TrainLoss:82.82958984375--TrainR2:-4.441559314727783--TestLoss:90.96109771728516--TestR2:-5.818048000335693\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1914--TrainLoss:82.8143310546875--TrainR2:-4.44055700302124--TestLoss:90.94474029541016--TestR2:-5.816821575164795\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1915--TrainLoss:82.7990493774414--TrainR2:-4.439553260803223--TestLoss:90.9283676147461--TestR2:-5.815594673156738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1916--TrainLoss:82.7837905883789--TrainR2:-4.43855094909668--TestLoss:90.91203308105469--TestR2:-5.814370632171631\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1917--TrainLoss:82.7685546875--TrainR2:-4.437549591064453--TestLoss:90.89569854736328--TestR2:-5.813146591186523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1918--TrainLoss:82.75330352783203--TrainR2:-4.436547756195068--TestLoss:90.87934112548828--TestR2:-5.811920166015625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1919--TrainLoss:82.7380142211914--TrainR2:-4.435543537139893--TestLoss:90.86299896240234--TestR2:-5.810695171356201\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1920--TrainLoss:82.72279357910156--TrainR2:-4.434543132781982--TestLoss:90.8466567993164--TestR2:-5.809470176696777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1921--TrainLoss:82.70755767822266--TrainR2:-4.433542728424072--TestLoss:90.83031463623047--TestR2:-5.8082451820373535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1922--TrainLoss:82.69232177734375--TrainR2:-4.432541370391846--TestLoss:90.81398010253906--TestR2:-5.807021141052246\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1923--TrainLoss:82.67704772949219--TrainR2:-4.431538105010986--TestLoss:90.7976303100586--TestR2:-5.805795669555664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1924--TrainLoss:82.66181945800781--TrainR2:-4.430537700653076--TestLoss:90.78131103515625--TestR2:-5.804572105407715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1925--TrainLoss:82.64656829833984--TrainR2:-4.429535865783691--TestLoss:90.76497650146484--TestR2:-5.803348064422607\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1926--TrainLoss:82.63134002685547--TrainR2:-4.428535461425781--TestLoss:90.74866485595703--TestR2:-5.802125453948975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1927--TrainLoss:82.61611938476562--TrainR2:-4.427535533905029--TestLoss:90.73233795166016--TestR2:-5.800900936126709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1928--TrainLoss:82.60086822509766--TrainR2:-4.426533222198486--TestLoss:90.71601104736328--TestR2:-5.799677848815918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1929--TrainLoss:82.58563995361328--TrainR2:-4.425532817840576--TestLoss:90.69966888427734--TestR2:-5.798452854156494\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1930--TrainLoss:82.57041931152344--TrainR2:-4.424533367156982--TestLoss:90.68336486816406--TestR2:-5.797230243682861\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1931--TrainLoss:82.55518341064453--TrainR2:-4.423532485961914--TestLoss:90.66705322265625--TestR2:-5.796008110046387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1932--TrainLoss:82.5399398803711--TrainR2:-4.4225311279296875--TestLoss:90.6507339477539--TestR2:-5.7947845458984375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1933--TrainLoss:82.52472686767578--TrainR2:-4.4215312004089355--TestLoss:90.63443756103516--TestR2:-5.793562889099121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1934--TrainLoss:82.5094985961914--TrainR2:-4.420530796051025--TestLoss:90.61811828613281--TestR2:-5.792340278625488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1935--TrainLoss:82.4942855834961--TrainR2:-4.41953182220459--TestLoss:90.601806640625--TestR2:-5.791117191314697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1936--TrainLoss:82.47908020019531--TrainR2:-4.418532371520996--TestLoss:90.58550262451172--TestR2:-5.789895534515381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1937--TrainLoss:82.46385192871094--TrainR2:-4.417532444000244--TestLoss:90.56919860839844--TestR2:-5.788672924041748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1938--TrainLoss:82.44865417480469--TrainR2:-4.416533946990967--TestLoss:90.55289459228516--TestR2:-5.787451267242432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1939--TrainLoss:82.43345642089844--TrainR2:-4.4155354499816895--TestLoss:90.53661346435547--TestR2:-5.786230564117432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1940--TrainLoss:82.41824340820312--TrainR2:-4.414535999298096--TestLoss:90.52030944824219--TestR2:-5.785008430480957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1941--TrainLoss:82.40302276611328--TrainR2:-4.4135355949401855--TestLoss:90.50402069091797--TestR2:-5.783787727355957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1942--TrainLoss:82.38780212402344--TrainR2:-4.412536144256592--TestLoss:90.48772430419922--TestR2:-5.782566070556641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1943--TrainLoss:82.37262725830078--TrainR2:-4.411539077758789--TestLoss:90.47142028808594--TestR2:-5.781344413757324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1944--TrainLoss:82.357421875--TrainR2:-4.4105401039123535--TestLoss:90.45514678955078--TestR2:-5.780124187469482\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1945--TrainLoss:82.34224700927734--TrainR2:-4.409543514251709--TestLoss:90.43885040283203--TestR2:-5.778903007507324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1946--TrainLoss:82.32703399658203--TrainR2:-4.408543586730957--TestLoss:90.42256164550781--TestR2:-5.777681827545166\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1947--TrainLoss:82.31185913085938--TrainR2:-4.407546520233154--TestLoss:90.40628814697266--TestR2:-5.776462078094482\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1948--TrainLoss:82.29666137695312--TrainR2:-4.406548023223877--TestLoss:90.3900146484375--TestR2:-5.775242328643799\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1949--TrainLoss:82.28150177001953--TrainR2:-4.405552864074707--TestLoss:90.37372589111328--TestR2:-5.774021625518799\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1950--TrainLoss:82.26629638671875--TrainR2:-4.404553413391113--TestLoss:90.35746002197266--TestR2:-5.772801876068115\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1951--TrainLoss:82.2511215209961--TrainR2:-4.403556823730469--TestLoss:90.3411865234375--TestR2:-5.771582126617432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1952--TrainLoss:82.23588562011719--TrainR2:-4.4025559425354--TestLoss:90.32492065429688--TestR2:-5.770362854003906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1953--TrainLoss:82.22074127197266--TrainR2:-4.4015607833862305--TestLoss:90.30864715576172--TestR2:-5.769143104553223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1954--TrainLoss:82.20556640625--TrainR2:-4.400563716888428--TestLoss:90.29238891601562--TestR2:-5.7679243087768555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1955--TrainLoss:82.19041442871094--TrainR2:-4.399568557739258--TestLoss:90.276123046875--TestR2:-5.766705513000488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1956--TrainLoss:82.17524719238281--TrainR2:-4.398571968078613--TestLoss:90.2598648071289--TestR2:-5.765486717224121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1957--TrainLoss:82.16007995605469--TrainR2:-4.397575378417969--TestLoss:90.24359893798828--TestR2:-5.764267921447754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1958--TrainLoss:82.1448974609375--TrainR2:-4.396577835083008--TestLoss:90.22734832763672--TestR2:-5.763049602508545\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1959--TrainLoss:82.1297378540039--TrainR2:-4.39558219909668--TestLoss:90.2110824584961--TestR2:-5.7618303298950195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1960--TrainLoss:82.11458587646484--TrainR2:-4.394586563110352--TestLoss:90.19483184814453--TestR2:-5.7606120109558105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1961--TrainLoss:82.09942626953125--TrainR2:-4.393590927124023--TestLoss:90.17859649658203--TestR2:-5.759395599365234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1962--TrainLoss:82.08430480957031--TrainR2:-4.392597198486328--TestLoss:90.16233825683594--TestR2:-5.758176803588867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1963--TrainLoss:82.0691146850586--TrainR2:-4.391599178314209--TestLoss:90.1460952758789--TestR2:-5.756959438323975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1964--TrainLoss:82.0539321899414--TrainR2:-4.390602111816406--TestLoss:90.12985229492188--TestR2:-5.755741596221924\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1965--TrainLoss:82.03877258300781--TrainR2:-4.389606475830078--TestLoss:90.11360931396484--TestR2:-5.754524230957031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1966--TrainLoss:82.02362823486328--TrainR2:-4.388611316680908--TestLoss:90.0973892211914--TestR2:-5.753308296203613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1967--TrainLoss:82.00846099853516--TrainR2:-4.387615203857422--TestLoss:90.08113861083984--TestR2:-5.752089977264404\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1968--TrainLoss:81.99334716796875--TrainR2:-4.386621952056885--TestLoss:90.06491088867188--TestR2:-5.750874042510986\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1969--TrainLoss:81.97822570800781--TrainR2:-4.3856282234191895--TestLoss:90.04866790771484--TestR2:-5.7496562004089355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1970--TrainLoss:81.96308135986328--TrainR2:-4.384633541107178--TestLoss:90.03242492675781--TestR2:-5.748438835144043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1971--TrainLoss:81.94790649414062--TrainR2:-4.383636951446533--TestLoss:90.01620483398438--TestR2:-5.747223377227783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1972--TrainLoss:81.93275451660156--TrainR2:-4.382640838623047--TestLoss:89.9999771118164--TestR2:-5.746006488800049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1973--TrainLoss:81.91764068603516--TrainR2:-4.381648540496826--TestLoss:89.9837646484375--TestR2:-5.7447919845581055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1974--TrainLoss:81.90251922607422--TrainR2:-4.380654811859131--TestLoss:89.96754455566406--TestR2:-5.743575572967529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1975--TrainLoss:81.88739013671875--TrainR2:-4.3796610832214355--TestLoss:89.95133209228516--TestR2:-5.742360591888428\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1976--TrainLoss:81.87226104736328--TrainR2:-4.378666877746582--TestLoss:89.93510437011719--TestR2:-5.741144180297852\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1977--TrainLoss:81.8571548461914--TrainR2:-4.377674579620361--TestLoss:89.91888427734375--TestR2:-5.739928245544434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1978--TrainLoss:81.84201049804688--TrainR2:-4.376679420471191--TestLoss:89.90267181396484--TestR2:-5.738713264465332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1979--TrainLoss:81.8268814086914--TrainR2:-4.375685691833496--TestLoss:89.88646697998047--TestR2:-5.737498760223389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1980--TrainLoss:81.81177520751953--TrainR2:-4.374693393707275--TestLoss:89.87026977539062--TestR2:-5.736284255981445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1981--TrainLoss:81.796630859375--TrainR2:-4.373698711395264--TestLoss:89.85404968261719--TestR2:-5.735068321228027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1982--TrainLoss:81.78153991699219--TrainR2:-4.372707366943359--TestLoss:89.83785247802734--TestR2:-5.7338547706604\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1983--TrainLoss:81.76641082763672--TrainR2:-4.371713161468506--TestLoss:89.82164764404297--TestR2:-5.732639789581299\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1984--TrainLoss:81.75131225585938--TrainR2:-4.370720863342285--TestLoss:89.80545043945312--TestR2:-5.731425762176514\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1985--TrainLoss:81.73622131347656--TrainR2:-4.369729518890381--TestLoss:89.78925323486328--TestR2:-5.730212211608887\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1986--TrainLoss:81.72110748291016--TrainR2:-4.368736743927002--TestLoss:89.7730484008789--TestR2:-5.728997230529785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1987--TrainLoss:81.70599365234375--TrainR2:-4.367743968963623--TestLoss:89.75686645507812--TestR2:-5.727784156799316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1988--TrainLoss:81.69087982177734--TrainR2:-4.366750717163086--TestLoss:89.74066925048828--TestR2:-5.726570129394531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1989--TrainLoss:81.67579650878906--TrainR2:-4.36575984954834--TestLoss:89.7244873046875--TestR2:-5.7253570556640625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1990--TrainLoss:81.66069030761719--TrainR2:-4.364768028259277--TestLoss:89.70829010009766--TestR2:-5.724143028259277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1991--TrainLoss:81.64557647705078--TrainR2:-4.36377477645874--TestLoss:89.69212341308594--TestR2:-5.722930908203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1992--TrainLoss:81.63050842285156--TrainR2:-4.3627848625183105--TestLoss:89.67594146728516--TestR2:-5.7217183113098145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1993--TrainLoss:81.61540222167969--TrainR2:-4.36179256439209--TestLoss:89.65975189208984--TestR2:-5.7205047607421875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1994--TrainLoss:81.60032653808594--TrainR2:-4.360801696777344--TestLoss:89.64356994628906--TestR2:-5.719291687011719\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1995--TrainLoss:81.58523559570312--TrainR2:-4.359810829162598--TestLoss:89.62739562988281--TestR2:-5.718080043792725\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1996--TrainLoss:81.57017517089844--TrainR2:-4.358821392059326--TestLoss:89.61123657226562--TestR2:-5.7168684005737305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1997--TrainLoss:81.55509185791016--TrainR2:-4.357830047607422--TestLoss:89.59505462646484--TestR2:-5.715655326843262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1998--TrainLoss:81.53997039794922--TrainR2:-4.356836795806885--TestLoss:89.57889556884766--TestR2:-5.714444160461426\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:1999--TrainLoss:81.52488708496094--TrainR2:-4.355845928192139--TestLoss:89.56271362304688--TestR2:-5.713231086730957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2000--TrainLoss:81.50982666015625--TrainR2:-4.354856491088867--TestLoss:89.54656982421875--TestR2:-5.712021350860596\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2001--TrainLoss:81.4947280883789--TrainR2:-4.353864669799805--TestLoss:89.5303955078125--TestR2:-5.710809230804443\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2002--TrainLoss:81.47967529296875--TrainR2:-4.352875709533691--TestLoss:89.51423645019531--TestR2:-5.709597587585449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2003--TrainLoss:81.46460723876953--TrainR2:-4.351885795593262--TestLoss:89.49808502197266--TestR2:-5.7083868980407715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2004--TrainLoss:81.44954681396484--TrainR2:-4.35089635848999--TestLoss:89.48193359375--TestR2:-5.707176685333252\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2005--TrainLoss:81.43447875976562--TrainR2:-4.349906921386719--TestLoss:89.46577453613281--TestR2:-5.705965518951416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2006--TrainLoss:81.41939544677734--TrainR2:-4.3489155769348145--TestLoss:89.44962310791016--TestR2:-5.70475435256958\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2007--TrainLoss:81.40436553955078--TrainR2:-4.347928524017334--TestLoss:89.4334716796875--TestR2:-5.703543663024902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2008--TrainLoss:81.38931274414062--TrainR2:-4.346939563751221--TestLoss:89.41732025146484--TestR2:-5.702333450317383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2009--TrainLoss:81.3742446899414--TrainR2:-4.345949649810791--TestLoss:89.40118408203125--TestR2:-5.7011237144470215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2010--TrainLoss:81.35920715332031--TrainR2:-4.344961643218994--TestLoss:89.38504791259766--TestR2:-5.699914455413818\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2011--TrainLoss:81.34417724609375--TrainR2:-4.3439741134643555--TestLoss:89.36888885498047--TestR2:-5.698703289031982\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2012--TrainLoss:81.3291015625--TrainR2:-4.342983722686768--TestLoss:89.3527603149414--TestR2:-5.697494029998779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2013--TrainLoss:81.31405639648438--TrainR2:-4.3419952392578125--TestLoss:89.33662414550781--TestR2:-5.696284770965576\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2014--TrainLoss:81.29896545410156--TrainR2:-4.341003894805908--TestLoss:89.32048797607422--TestR2:-5.695075035095215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2015--TrainLoss:81.283935546875--TrainR2:-4.340016841888428--TestLoss:89.30435943603516--TestR2:-5.69386625289917\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2016--TrainLoss:81.26891326904297--TrainR2:-4.339029312133789--TestLoss:89.28823852539062--TestR2:-5.692657947540283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2017--TrainLoss:81.25384521484375--TrainR2:-4.338039398193359--TestLoss:89.27210235595703--TestR2:-5.69144868850708\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2018--TrainLoss:81.23882293701172--TrainR2:-4.337052822113037--TestLoss:89.2559814453125--TestR2:-5.690240383148193\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2019--TrainLoss:81.22378540039062--TrainR2:-4.33606481552124--TestLoss:89.23986053466797--TestR2:-5.689031600952148\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2020--TrainLoss:81.2087173461914--TrainR2:-4.3350749015808105--TestLoss:89.22373962402344--TestR2:-5.687823295593262\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2021--TrainLoss:81.19371795654297--TrainR2:-4.334089279174805--TestLoss:89.20761108398438--TestR2:-5.686614513397217\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2022--TrainLoss:81.17870330810547--TrainR2:-4.333103179931641--TestLoss:89.19149017333984--TestR2:-5.68540620803833\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2023--TrainLoss:81.16365814208984--TrainR2:-4.3321146965026855--TestLoss:89.1753921508789--TestR2:-5.684199333190918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2024--TrainLoss:81.14865112304688--TrainR2:-4.3311285972595215--TestLoss:89.15929412841797--TestR2:-5.682992458343506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2025--TrainLoss:81.13363647460938--TrainR2:-4.330142498016357--TestLoss:89.1431655883789--TestR2:-5.681784152984619\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2026--TrainLoss:81.11859130859375--TrainR2:-4.329154014587402--TestLoss:89.12705993652344--TestR2:-5.680576801300049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2027--TrainLoss:81.10359191894531--TrainR2:-4.328168869018555--TestLoss:89.11095428466797--TestR2:-5.6793694496154785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2028--TrainLoss:81.08859252929688--TrainR2:-4.327183246612549--TestLoss:89.09486389160156--TestR2:-5.678163528442383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2029--TrainLoss:81.07354736328125--TrainR2:-4.326194763183594--TestLoss:89.07875061035156--TestR2:-5.676955699920654\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2030--TrainLoss:81.05853271484375--TrainR2:-4.32520866394043--TestLoss:89.06265258789062--TestR2:-5.675748825073242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2031--TrainLoss:81.04353332519531--TrainR2:-4.324223041534424--TestLoss:89.04654693603516--TestR2:-5.67454195022583\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2032--TrainLoss:81.02854919433594--TrainR2:-4.323238849639893--TestLoss:89.03047180175781--TestR2:-5.673336982727051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2033--TrainLoss:81.01353454589844--TrainR2:-4.32225227355957--TestLoss:89.01437377929688--TestR2:-5.672130107879639\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2034--TrainLoss:80.99853515625--TrainR2:-4.321267127990723--TestLoss:88.99827575683594--TestR2:-5.670923709869385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2035--TrainLoss:80.98351287841797--TrainR2:-4.320280075073242--TestLoss:88.98219299316406--TestR2:-5.669718265533447\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2036--TrainLoss:80.9685287475586--TrainR2:-4.319295406341553--TestLoss:88.96611785888672--TestR2:-5.66851282119751\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2037--TrainLoss:80.95355224609375--TrainR2:-4.31831169128418--TestLoss:88.95001983642578--TestR2:-5.667306423187256\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2038--TrainLoss:80.93853759765625--TrainR2:-4.317325115203857--TestLoss:88.9339370727539--TestR2:-5.666100978851318\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2039--TrainLoss:80.92353820800781--TrainR2:-4.316339492797852--TestLoss:88.9178695678711--TestR2:-5.664896488189697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2040--TrainLoss:80.90856170654297--TrainR2:-4.3153557777404785--TestLoss:88.90179443359375--TestR2:-5.663691520690918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2041--TrainLoss:80.89358520507812--TrainR2:-4.3143720626831055--TestLoss:88.88572692871094--TestR2:-5.662487506866455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2042--TrainLoss:80.87857055664062--TrainR2:-4.313385963439941--TestLoss:88.86964416503906--TestR2:-5.661282062530518\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2043--TrainLoss:80.86359405517578--TrainR2:-4.31240177154541--TestLoss:88.85358428955078--TestR2:-5.660078048706055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2044--TrainLoss:80.84862518310547--TrainR2:-4.311418533325195--TestLoss:88.8375015258789--TestR2:-5.658872604370117\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2045--TrainLoss:80.83362579345703--TrainR2:-4.3104329109191895--TestLoss:88.82145690917969--TestR2:-5.657670021057129\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2046--TrainLoss:80.81865692138672--TrainR2:-4.309449672698975--TestLoss:88.80537414550781--TestR2:-5.656464576721191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2047--TrainLoss:80.8036880493164--TrainR2:-4.308465957641602--TestLoss:88.7893295288086--TestR2:-5.655261993408203\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2048--TrainLoss:80.78868865966797--TrainR2:-4.307480812072754--TestLoss:88.77326965332031--TestR2:-5.65405797958374\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2049--TrainLoss:80.77371978759766--TrainR2:-4.306497573852539--TestLoss:88.75719451904297--TestR2:-5.652853012084961\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2050--TrainLoss:80.7587661743164--TrainR2:-4.305514812469482--TestLoss:88.74115753173828--TestR2:-5.651651382446289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2051--TrainLoss:80.74380493164062--TrainR2:-4.304532527923584--TestLoss:88.72509765625--TestR2:-5.650447368621826\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2052--TrainLoss:80.72884368896484--TrainR2:-4.303549289703369--TestLoss:88.70905303955078--TestR2:-5.649244785308838\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2053--TrainLoss:80.71385192871094--TrainR2:-4.30256462097168--TestLoss:88.69300079345703--TestR2:-5.648041725158691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2054--TrainLoss:80.69892120361328--TrainR2:-4.301583290100098--TestLoss:88.67695617675781--TestR2:-5.646839141845703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2055--TrainLoss:80.6839828491211--TrainR2:-4.300601959228516--TestLoss:88.6609115600586--TestR2:-5.645636558532715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2056--TrainLoss:80.66901397705078--TrainR2:-4.299618721008301--TestLoss:88.64488220214844--TestR2:-5.644434928894043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2057--TrainLoss:80.65406799316406--TrainR2:-4.2986369132995605--TestLoss:88.62882995605469--TestR2:-5.643231391906738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2058--TrainLoss:80.63916015625--TrainR2:-4.297657012939453--TestLoss:88.61280059814453--TestR2:-5.642029762268066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2059--TrainLoss:80.6241683959961--TrainR2:-4.296672821044922--TestLoss:88.59674835205078--TestR2:-5.64082670211792\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2060--TrainLoss:80.60919952392578--TrainR2:-4.295689582824707--TestLoss:88.58072662353516--TestR2:-5.6396260261535645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2061--TrainLoss:80.59429168701172--TrainR2:-4.2947096824646--TestLoss:88.56468963623047--TestR2:-5.638423919677734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2062--TrainLoss:80.57933044433594--TrainR2:-4.293726921081543--TestLoss:88.54867553710938--TestR2:-5.637223720550537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2063--TrainLoss:80.56439208984375--TrainR2:-4.292745113372803--TestLoss:88.53263854980469--TestR2:-5.636021614074707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2064--TrainLoss:80.54944610595703--TrainR2:-4.291763782501221--TestLoss:88.5166244506836--TestR2:-5.63482141494751\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2065--TrainLoss:80.5345230102539--TrainR2:-4.290783405303955--TestLoss:88.50057983398438--TestR2:-5.6336188316345215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2066--TrainLoss:80.51958465576172--TrainR2:-4.289801597595215--TestLoss:88.48458099365234--TestR2:-5.632419109344482\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2067--TrainLoss:80.5046157836914--TrainR2:-4.288818359375--TestLoss:88.46855163574219--TestR2:-5.6312174797058105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2068--TrainLoss:80.48970794677734--TrainR2:-4.287838935852051--TestLoss:88.45254516601562--TestR2:-5.6300177574157715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2069--TrainLoss:80.47478485107422--TrainR2:-4.286859035491943--TestLoss:88.43653869628906--TestR2:-5.628818511962891\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2070--TrainLoss:80.4598617553711--TrainR2:-4.2858781814575195--TestLoss:88.42052459716797--TestR2:-5.627617835998535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2071--TrainLoss:80.4449462890625--TrainR2:-4.28489875793457--TestLoss:88.40450286865234--TestR2:-5.62641716003418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2072--TrainLoss:80.4300308227539--TrainR2:-4.283918380737305--TestLoss:88.38849639892578--TestR2:-5.625216960906982\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2073--TrainLoss:80.41511535644531--TrainR2:-4.282938480377197--TestLoss:88.37249755859375--TestR2:-5.624017715454102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2074--TrainLoss:80.40019989013672--TrainR2:-4.28195858001709--TestLoss:88.35649871826172--TestR2:-5.622818470001221\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2075--TrainLoss:80.38526916503906--TrainR2:-4.280977725982666--TestLoss:88.34049987792969--TestR2:-5.621619701385498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2076--TrainLoss:80.37036895751953--TrainR2:-4.279998779296875--TestLoss:88.32449340820312--TestR2:-5.620419979095459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2077--TrainLoss:80.3554458618164--TrainR2:-4.279018878936768--TestLoss:88.30850982666016--TestR2:-5.619221210479736\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2078--TrainLoss:80.34053802490234--TrainR2:-4.278039455413818--TestLoss:88.29249572753906--TestR2:-5.618021011352539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2079--TrainLoss:80.32563018798828--TrainR2:-4.277059555053711--TestLoss:88.2765121459961--TestR2:-5.616823196411133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2080--TrainLoss:80.31072235107422--TrainR2:-4.276080131530762--TestLoss:88.2605209350586--TestR2:-5.615624904632568\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2081--TrainLoss:80.29579162597656--TrainR2:-4.275099277496338--TestLoss:88.24454498291016--TestR2:-5.61442756652832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2082--TrainLoss:80.28096008300781--TrainR2:-4.274125099182129--TestLoss:88.22854614257812--TestR2:-5.613227844238281\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2083--TrainLoss:80.26604461669922--TrainR2:-4.2731451988220215--TestLoss:88.21257019042969--TestR2:-5.612030506134033\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2084--TrainLoss:80.25113677978516--TrainR2:-4.272165775299072--TestLoss:88.19660186767578--TestR2:-5.610833644866943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2085--TrainLoss:80.23625183105469--TrainR2:-4.271188259124756--TestLoss:88.18061065673828--TestR2:-5.609634876251221\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2086--TrainLoss:80.22135162353516--TrainR2:-4.270209312438965--TestLoss:88.16463470458984--TestR2:-5.608437538146973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2087--TrainLoss:80.20645904541016--TrainR2:-4.269230842590332--TestLoss:88.14866638183594--TestR2:-5.607240676879883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2088--TrainLoss:80.19156646728516--TrainR2:-4.268252372741699--TestLoss:88.1326904296875--TestR2:-5.606043338775635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2089--TrainLoss:80.17666625976562--TrainR2:-4.267273426055908--TestLoss:88.11671447753906--TestR2:-5.604846000671387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2090--TrainLoss:80.16177368164062--TrainR2:-4.266295433044434--TestLoss:88.10074615478516--TestR2:-5.6036481857299805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2091--TrainLoss:80.14694213867188--TrainR2:-4.265320777893066--TestLoss:88.08477783203125--TestR2:-5.602451801300049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2092--TrainLoss:80.13207244873047--TrainR2:-4.264343738555908--TestLoss:88.06883239746094--TestR2:-5.601256370544434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2093--TrainLoss:80.1171646118164--TrainR2:-4.263364791870117--TestLoss:88.05286407470703--TestR2:-5.600059986114502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2094--TrainLoss:80.10228729248047--TrainR2:-4.262387275695801--TestLoss:88.03691101074219--TestR2:-5.59886360168457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2095--TrainLoss:80.08741760253906--TrainR2:-4.261410236358643--TestLoss:88.02095031738281--TestR2:-5.597667694091797\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2096--TrainLoss:80.07256317138672--TrainR2:-4.260434627532959--TestLoss:88.0049819946289--TestR2:-5.596470832824707\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2097--TrainLoss:80.05770111083984--TrainR2:-4.259458065032959--TestLoss:87.98904418945312--TestR2:-5.59527587890625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2098--TrainLoss:80.04280090332031--TrainR2:-4.258479118347168--TestLoss:87.97309112548828--TestR2:-5.594080448150635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2099--TrainLoss:80.0279769897461--TrainR2:-4.257504940032959--TestLoss:87.95714569091797--TestR2:-5.5928850173950195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2100--TrainLoss:80.01310729980469--TrainR2:-4.256528377532959--TestLoss:87.94120025634766--TestR2:-5.591689586639404\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2101--TrainLoss:79.99825286865234--TrainR2:-4.255552291870117--TestLoss:87.92524719238281--TestR2:-5.590494155883789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2102--TrainLoss:79.98339080810547--TrainR2:-4.254576206207275--TestLoss:87.90930938720703--TestR2:-5.58929967880249\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2103--TrainLoss:79.968505859375--TrainR2:-4.253598213195801--TestLoss:87.89336395263672--TestR2:-5.588104248046875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2104--TrainLoss:79.95361328125--TrainR2:-4.252620220184326--TestLoss:87.87745666503906--TestR2:-5.586912155151367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2105--TrainLoss:79.93885040283203--TrainR2:-4.251650333404541--TestLoss:87.86150360107422--TestR2:-5.585716247558594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2106--TrainLoss:79.92398834228516--TrainR2:-4.250673770904541--TestLoss:87.8455581665039--TestR2:-5.5845208168029785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2107--TrainLoss:79.90913391113281--TrainR2:-4.249697685241699--TestLoss:87.82964324951172--TestR2:-5.583327770233154\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2108--TrainLoss:79.89429473876953--TrainR2:-4.248723030090332--TestLoss:87.81370544433594--TestR2:-5.582133769989014\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2109--TrainLoss:79.87943267822266--TrainR2:-4.247746467590332--TestLoss:87.79778289794922--TestR2:-5.580939769744873\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2110--TrainLoss:79.86459350585938--TrainR2:-4.246771335601807--TestLoss:87.7818603515625--TestR2:-5.579746723175049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2111--TrainLoss:79.84974670410156--TrainR2:-4.2457966804504395--TestLoss:87.76593780517578--TestR2:-5.578552722930908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2112--TrainLoss:79.83494567871094--TrainR2:-4.244823932647705--TestLoss:87.75000762939453--TestR2:-5.577358722686768\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2113--TrainLoss:79.82013702392578--TrainR2:-4.2438507080078125--TestLoss:87.73409271240234--TestR2:-5.576166152954102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2114--TrainLoss:79.8052749633789--TrainR2:-4.242874622344971--TestLoss:87.71819305419922--TestR2:-5.574974060058594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2115--TrainLoss:79.79043579101562--TrainR2:-4.241899490356445--TestLoss:87.7022705078125--TestR2:-5.573780536651611\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2116--TrainLoss:79.77560424804688--TrainR2:-4.240925312042236--TestLoss:87.68636322021484--TestR2:-5.572587966918945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2117--TrainLoss:79.76080322265625--TrainR2:-4.23995304107666--TestLoss:87.67044067382812--TestR2:-5.571394920349121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2118--TrainLoss:79.7459716796875--TrainR2:-4.238978862762451--TestLoss:87.65453338623047--TestR2:-5.570202827453613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2119--TrainLoss:79.73116302490234--TrainR2:-4.238006114959717--TestLoss:87.63864135742188--TestR2:-5.569011211395264\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2120--TrainLoss:79.71634674072266--TrainR2:-4.237032413482666--TestLoss:87.62273406982422--TestR2:-5.567819118499756\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2121--TrainLoss:79.70150756835938--TrainR2:-4.236057281494141--TestLoss:87.60684967041016--TestR2:-5.566628456115723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2122--TrainLoss:79.68663787841797--TrainR2:-4.235080718994141--TestLoss:87.5909423828125--TestR2:-5.565436363220215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2123--TrainLoss:79.6718978881836--TrainR2:-4.234112739562988--TestLoss:87.57503509521484--TestR2:-5.564243793487549\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2124--TrainLoss:79.65706634521484--TrainR2:-4.233138084411621--TestLoss:87.55914306640625--TestR2:-5.563052654266357\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2125--TrainLoss:79.64226531982422--TrainR2:-4.232165813446045--TestLoss:87.54325866699219--TestR2:-5.561861515045166\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2126--TrainLoss:79.6274642944336--TrainR2:-4.2311930656433105--TestLoss:87.5273666381836--TestR2:-5.560670852661133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2127--TrainLoss:79.61266326904297--TrainR2:-4.230220794677734--TestLoss:87.51148223876953--TestR2:-5.559479713439941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2128--TrainLoss:79.59788513183594--TrainR2:-4.229249954223633--TestLoss:87.49559020996094--TestR2:-5.558289051055908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2129--TrainLoss:79.58305358886719--TrainR2:-4.228275775909424--TestLoss:87.47972106933594--TestR2:-5.557099342346191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2130--TrainLoss:79.5682601928711--TrainR2:-4.227303981781006--TestLoss:87.46383666992188--TestR2:-5.555908679962158\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2131--TrainLoss:79.55342864990234--TrainR2:-4.226329326629639--TestLoss:87.44795989990234--TestR2:-5.554718494415283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2132--TrainLoss:79.53868103027344--TrainR2:-4.22536039352417--TestLoss:87.43207550048828--TestR2:-5.55352783203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2133--TrainLoss:79.52388763427734--TrainR2:-4.224388599395752--TestLoss:87.41619873046875--TestR2:-5.552337646484375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2134--TrainLoss:79.50911712646484--TrainR2:-4.223418235778809--TestLoss:87.40033721923828--TestR2:-5.551149368286133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2135--TrainLoss:79.49431610107422--TrainR2:-4.222445964813232--TestLoss:87.38446807861328--TestR2:-5.549959182739258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2136--TrainLoss:79.47953033447266--TrainR2:-4.221474647521973--TestLoss:87.36859130859375--TestR2:-5.548769474029541\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2137--TrainLoss:79.46478271484375--TrainR2:-4.220505714416504--TestLoss:87.35272216796875--TestR2:-5.547579765319824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2138--TrainLoss:79.44996643066406--TrainR2:-4.219532489776611--TestLoss:87.33686065673828--TestR2:-5.546391010284424\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2139--TrainLoss:79.4351806640625--TrainR2:-4.218561172485352--TestLoss:87.32100677490234--TestR2:-5.545202732086182\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2140--TrainLoss:79.42037200927734--TrainR2:-4.217588424682617--TestLoss:87.3051528930664--TestR2:-5.5440144538879395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2141--TrainLoss:79.40563201904297--TrainR2:-4.216619491577148--TestLoss:87.2892837524414--TestR2:-5.542824745178223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2142--TrainLoss:79.39087677001953--TrainR2:-4.2156500816345215--TestLoss:87.27342987060547--TestR2:-5.541636943817139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2143--TrainLoss:79.37611389160156--TrainR2:-4.214680194854736--TestLoss:87.25758361816406--TestR2:-5.5404486656188965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2144--TrainLoss:79.361328125--TrainR2:-4.213709354400635--TestLoss:87.2417221069336--TestR2:-5.539259910583496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2145--TrainLoss:79.3465576171875--TrainR2:-4.212738990783691--TestLoss:87.22587585449219--TestR2:-5.538072109222412\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2146--TrainLoss:79.33182525634766--TrainR2:-4.211771011352539--TestLoss:87.21003723144531--TestR2:-5.536884784698486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2147--TrainLoss:79.31703186035156--TrainR2:-4.210799217224121--TestLoss:87.19418334960938--TestR2:-5.535696983337402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2148--TrainLoss:79.30226135253906--TrainR2:-4.209828853607178--TestLoss:87.1783447265625--TestR2:-5.534509181976318\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2149--TrainLoss:79.2874755859375--TrainR2:-4.20885705947876--TestLoss:87.16250610351562--TestR2:-5.533321857452393\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2150--TrainLoss:79.27277374267578--TrainR2:-4.207891464233398--TestLoss:87.14666748046875--TestR2:-5.532135009765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2151--TrainLoss:79.25801849365234--TrainR2:-4.2069220542907715--TestLoss:87.13082885742188--TestR2:-5.530947685241699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2152--TrainLoss:79.24325561523438--TrainR2:-4.205952167510986--TestLoss:87.11499786376953--TestR2:-5.52976131439209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2153--TrainLoss:79.22850799560547--TrainR2:-4.204983711242676--TestLoss:87.09915924072266--TestR2:-5.528573989868164\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2154--TrainLoss:79.21378326416016--TrainR2:-4.204016208648682--TestLoss:87.08334350585938--TestR2:-5.527388572692871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2155--TrainLoss:79.19904327392578--TrainR2:-4.203047752380371--TestLoss:87.06752014160156--TestR2:-5.52620267868042\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2156--TrainLoss:79.18431091308594--TrainR2:-4.202079772949219--TestLoss:87.05169677734375--TestR2:-5.5250163078308105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2157--TrainLoss:79.16954803466797--TrainR2:-4.201109886169434--TestLoss:87.03587341308594--TestR2:-5.523830413818359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2158--TrainLoss:79.15486145019531--TrainR2:-4.200145244598389--TestLoss:87.02003479003906--TestR2:-5.522643566131592\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2159--TrainLoss:79.14008331298828--TrainR2:-4.199174404144287--TestLoss:87.00424194335938--TestR2:-5.521459579467773\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2160--TrainLoss:79.12533569335938--TrainR2:-4.19820499420166--TestLoss:86.9884033203125--TestR2:-5.520272731781006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2161--TrainLoss:79.11058807373047--TrainR2:-4.19723653793335--TestLoss:86.97260284423828--TestR2:-5.519088268280029\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2162--TrainLoss:79.09588623046875--TrainR2:-4.196270942687988--TestLoss:86.95677947998047--TestR2:-5.51790189743042\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2163--TrainLoss:79.0811538696289--TrainR2:-4.195302486419678--TestLoss:86.94098663330078--TestR2:-5.516717910766602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2164--TrainLoss:79.06640625--TrainR2:-4.194334030151367--TestLoss:86.9251708984375--TestR2:-5.515532493591309\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2165--TrainLoss:79.05171203613281--TrainR2:-4.193368911743164--TestLoss:86.90936279296875--TestR2:-5.514347553253174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2166--TrainLoss:79.03699493408203--TrainR2:-4.192401885986328--TestLoss:86.89356994628906--TestR2:-5.513164043426514\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2167--TrainLoss:79.02224731445312--TrainR2:-4.191432952880859--TestLoss:86.87776947021484--TestR2:-5.511979579925537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2168--TrainLoss:79.00755310058594--TrainR2:-4.190467357635498--TestLoss:86.8619613647461--TestR2:-5.510794639587402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2169--TrainLoss:78.99285888671875--TrainR2:-4.189502239227295--TestLoss:86.84617614746094--TestR2:-5.5096116065979\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2170--TrainLoss:78.9781265258789--TrainR2:-4.188534259796143--TestLoss:86.83036041259766--TestR2:-5.508426189422607\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2171--TrainLoss:78.96340942382812--TrainR2:-4.187567234039307--TestLoss:86.8145751953125--TestR2:-5.507242679595947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2172--TrainLoss:78.94872283935547--TrainR2:-4.186602592468262--TestLoss:86.79878234863281--TestR2:-5.506059169769287\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2173--TrainLoss:78.93400573730469--TrainR2:-4.185636043548584--TestLoss:86.78299713134766--TestR2:-5.504875659942627\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2174--TrainLoss:78.91930389404297--TrainR2:-4.1846699714660645--TestLoss:86.7672119140625--TestR2:-5.503693103790283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2175--TrainLoss:78.90458679199219--TrainR2:-4.1837029457092285--TestLoss:86.75143432617188--TestR2:-5.502510070800781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2176--TrainLoss:78.8899154663086--TrainR2:-4.1827392578125--TestLoss:86.73564147949219--TestR2:-5.501326084136963\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2177--TrainLoss:78.87520599365234--TrainR2:-4.181772708892822--TestLoss:86.71988677978516--TestR2:-5.500144958496094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2178--TrainLoss:78.86050415039062--TrainR2:-4.180807113647461--TestLoss:86.7041015625--TestR2:-5.49896240234375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2179--TrainLoss:78.8458480834961--TrainR2:-4.179844379425049--TestLoss:86.68831634521484--TestR2:-5.49777889251709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2180--TrainLoss:78.83111572265625--TrainR2:-4.1788763999938965--TestLoss:86.67253875732422--TestR2:-5.496596336364746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2181--TrainLoss:78.81641387939453--TrainR2:-4.177910327911377--TestLoss:86.65678405761719--TestR2:-5.495415210723877\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2182--TrainLoss:78.80174255371094--TrainR2:-4.176947116851807--TestLoss:86.64099884033203--TestR2:-5.494232654571533\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2183--TrainLoss:78.78707885742188--TrainR2:-4.175983428955078--TestLoss:86.625244140625--TestR2:-5.493051528930664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2184--TrainLoss:78.77233123779297--TrainR2:-4.175014495849609--TestLoss:86.60948181152344--TestR2:-5.491869926452637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2185--TrainLoss:78.7577133178711--TrainR2:-4.1740546226501465--TestLoss:86.59370422363281--TestR2:-5.490687370300293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2186--TrainLoss:78.7430191040039--TrainR2:-4.173088550567627--TestLoss:86.57795715332031--TestR2:-5.489506721496582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2187--TrainLoss:78.72833251953125--TrainR2:-4.172123908996582--TestLoss:86.56220245361328--TestR2:-5.488326072692871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2188--TrainLoss:78.71368408203125--TrainR2:-4.171161651611328--TestLoss:86.54644012451172--TestR2:-5.487144470214844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2189--TrainLoss:78.69898986816406--TrainR2:-4.170196056365967--TestLoss:86.53067779541016--TestR2:-5.485963344573975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2190--TrainLoss:78.68431854248047--TrainR2:-4.1692328453063965--TestLoss:86.51494598388672--TestR2:-5.48478364944458\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2191--TrainLoss:78.66967010498047--TrainR2:-4.168270111083984--TestLoss:86.49917602539062--TestR2:-5.4836015701293945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2192--TrainLoss:78.65499877929688--TrainR2:-4.167306423187256--TestLoss:86.48342895507812--TestR2:-5.482421875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2193--TrainLoss:78.64028930664062--TrainR2:-4.166340351104736--TestLoss:86.46769714355469--TestR2:-5.481242656707764\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2194--TrainLoss:78.62568664550781--TrainR2:-4.165380954742432--TestLoss:86.45195007324219--TestR2:-5.480062007904053\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2195--TrainLoss:78.61101531982422--TrainR2:-4.164416790008545--TestLoss:86.43620300292969--TestR2:-5.4788818359375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2196--TrainLoss:78.59634399414062--TrainR2:-4.163452625274658--TestLoss:86.42046356201172--TestR2:-5.4777021408081055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2197--TrainLoss:78.58171844482422--TrainR2:-4.162492275238037--TestLoss:86.40473175048828--TestR2:-5.476522445678711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2198--TrainLoss:78.56704711914062--TrainR2:-4.161528587341309--TestLoss:86.38900756835938--TestR2:-5.475344181060791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2199--TrainLoss:78.55238342285156--TrainR2:-4.16056489944458--TestLoss:86.3732681274414--TestR2:-5.4741644859313965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2200--TrainLoss:78.53775024414062--TrainR2:-4.159603595733643--TestLoss:86.35752868652344--TestR2:-5.472984790802002\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2201--TrainLoss:78.52310180664062--TrainR2:-4.1586408615112305--TestLoss:86.341796875--TestR2:-5.471805572509766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2202--TrainLoss:78.50843811035156--TrainR2:-4.15767765045166--TestLoss:86.32606506347656--TestR2:-5.470626354217529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2203--TrainLoss:78.49380493164062--TrainR2:-4.156716823577881--TestLoss:86.31035614013672--TestR2:-5.469449043273926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2204--TrainLoss:78.47916412353516--TrainR2:-4.155755043029785--TestLoss:86.29463958740234--TestR2:-5.468270778656006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2205--TrainLoss:78.46454620361328--TrainR2:-4.154794216156006--TestLoss:86.27892303466797--TestR2:-5.467092514038086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2206--TrainLoss:78.44989013671875--TrainR2:-4.153831481933594--TestLoss:86.26319885253906--TestR2:-5.465913772583008\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2207--TrainLoss:78.43524169921875--TrainR2:-4.15286922454834--TestLoss:86.24746704101562--TestR2:-5.4647345542907715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2208--TrainLoss:78.4206314086914--TrainR2:-4.151909351348877--TestLoss:86.23177337646484--TestR2:-5.463558673858643\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2209--TrainLoss:78.406005859375--TrainR2:-4.150948524475098--TestLoss:86.21604919433594--TestR2:-5.462380409240723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2210--TrainLoss:78.39136505126953--TrainR2:-4.149986743927002--TestLoss:86.20034790039062--TestR2:-5.461203098297119\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2211--TrainLoss:78.37677764892578--TrainR2:-4.149028301239014--TestLoss:86.18463897705078--TestR2:-5.460025787353516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2212--TrainLoss:78.36213684082031--TrainR2:-4.148066520690918--TestLoss:86.1689453125--TestR2:-5.4588494300842285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2213--TrainLoss:78.3475341796875--TrainR2:-4.147107124328613--TestLoss:86.15324401855469--TestR2:-5.457672595977783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2214--TrainLoss:78.3329086303711--TrainR2:-4.146146297454834--TestLoss:86.13753509521484--TestR2:-5.4564948081970215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2215--TrainLoss:78.31827545166016--TrainR2:-4.1451849937438965--TestLoss:86.12184143066406--TestR2:-5.455318450927734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2216--TrainLoss:78.30367279052734--TrainR2:-4.144225597381592--TestLoss:86.10614776611328--TestR2:-5.454142093658447\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2217--TrainLoss:78.28905487060547--TrainR2:-4.143265247344971--TestLoss:86.0904541015625--TestR2:-5.452966213226318\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2218--TrainLoss:78.2744140625--TrainR2:-4.142303466796875--TestLoss:86.07476806640625--TestR2:-5.4517903327941895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2219--TrainLoss:78.25984191894531--TrainR2:-4.141345977783203--TestLoss:86.05907440185547--TestR2:-5.450613975524902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2220--TrainLoss:78.24523162841797--TrainR2:-4.14038610458374--TestLoss:86.04338073730469--TestR2:-5.449437618255615\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2221--TrainLoss:78.23062133789062--TrainR2:-4.1394267082214355--TestLoss:86.02770233154297--TestR2:-5.4482622146606445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2222--TrainLoss:78.21601867675781--TrainR2:-4.138467311859131--TestLoss:86.01201629638672--TestR2:-5.447086334228516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2223--TrainLoss:78.2013931274414--TrainR2:-4.137506484985352--TestLoss:85.996337890625--TestR2:-5.445911407470703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2224--TrainLoss:78.18682861328125--TrainR2:-4.136549472808838--TestLoss:85.98065948486328--TestR2:-5.444736003875732\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2225--TrainLoss:78.17221069335938--TrainR2:-4.135589122772217--TestLoss:85.9649887084961--TestR2:-5.443561553955078\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2226--TrainLoss:78.15763092041016--TrainR2:-4.134631156921387--TestLoss:85.94930267333984--TestR2:-5.442386150360107\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2227--TrainLoss:78.14302825927734--TrainR2:-4.133671760559082--TestLoss:85.93362426757812--TestR2:-5.441210746765137\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2228--TrainLoss:78.12846374511719--TrainR2:-4.132715225219727--TestLoss:85.91795349121094--TestR2:-5.440036296844482\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2229--TrainLoss:78.11384582519531--TrainR2:-4.131754398345947--TestLoss:85.90229034423828--TestR2:-5.438861846923828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2230--TrainLoss:78.09927368164062--TrainR2:-4.130797863006592--TestLoss:85.88662719726562--TestR2:-5.437687873840332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2231--TrainLoss:78.084716796875--TrainR2:-4.129841327667236--TestLoss:85.87096405029297--TestR2:-5.436513900756836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2232--TrainLoss:78.07011413574219--TrainR2:-4.128881931304932--TestLoss:85.85530090332031--TestR2:-5.43533992767334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2233--TrainLoss:78.05555725097656--TrainR2:-4.127925395965576--TestLoss:85.8396224975586--TestR2:-5.434165000915527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2234--TrainLoss:78.04096984863281--TrainR2:-4.126967430114746--TestLoss:85.82398986816406--TestR2:-5.432992935180664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2235--TrainLoss:78.02637481689453--TrainR2:-4.126008033752441--TestLoss:85.8083267211914--TestR2:-5.43181848526001\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2236--TrainLoss:78.0118179321289--TrainR2:-4.125051975250244--TestLoss:85.79267120361328--TestR2:-5.430644989013672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2237--TrainLoss:77.99723052978516--TrainR2:-4.124093532562256--TestLoss:85.77701568603516--TestR2:-5.429471492767334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2238--TrainLoss:77.982666015625--TrainR2:-4.123136520385742--TestLoss:85.76138305664062--TestR2:-5.428299903869629\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2239--TrainLoss:77.96810150146484--TrainR2:-4.122179985046387--TestLoss:85.7457275390625--TestR2:-5.427126407623291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2240--TrainLoss:77.95354461669922--TrainR2:-4.1212239265441895--TestLoss:85.73006439208984--TestR2:-5.425952911376953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2241--TrainLoss:77.93896484375--TrainR2:-4.120265960693359--TestLoss:85.71443939208984--TestR2:-5.424781322479248\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2242--TrainLoss:77.92444610595703--TrainR2:-4.119312286376953--TestLoss:85.69879150390625--TestR2:-5.423608779907227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2243--TrainLoss:77.90987396240234--TrainR2:-4.118354797363281--TestLoss:85.68315887451172--TestR2:-5.4224371910095215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2244--TrainLoss:77.89529418945312--TrainR2:-4.117396831512451--TestLoss:85.66752624511719--TestR2:-5.421265125274658\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2245--TrainLoss:77.8807601928711--TrainR2:-4.1164422035217285--TestLoss:85.65188598632812--TestR2:-5.420092582702637\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2246--TrainLoss:77.86619567871094--TrainR2:-4.115485191345215--TestLoss:85.63626098632812--TestR2:-5.41892147064209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2247--TrainLoss:77.85164642333984--TrainR2:-4.114529132843018--TestLoss:85.62064361572266--TestR2:-5.417750835418701\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2248--TrainLoss:77.83708953857422--TrainR2:-4.11357307434082--TestLoss:85.6050033569336--TestR2:-5.416578769683838\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2249--TrainLoss:77.82255554199219--TrainR2:-4.112618446350098--TestLoss:85.58937072753906--TestR2:-5.415407180786133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2250--TrainLoss:77.8080062866211--TrainR2:-4.1116623878479--TestLoss:85.57376861572266--TestR2:-5.4142374992370605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2251--TrainLoss:77.79347229003906--TrainR2:-4.110707759857178--TestLoss:85.55813598632812--TestR2:-5.4130659103393555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2252--TrainLoss:77.77894592285156--TrainR2:-4.109753131866455--TestLoss:85.54249572753906--TestR2:-5.411893367767334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2253--TrainLoss:77.76444244384766--TrainR2:-4.108800411224365--TestLoss:85.52690124511719--TestR2:-5.41072416305542\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2254--TrainLoss:77.74986267089844--TrainR2:-4.107842445373535--TestLoss:85.51128387451172--TestR2:-5.409553527832031\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2255--TrainLoss:77.7353515625--TrainR2:-4.106889247894287--TestLoss:85.49567413330078--TestR2:-5.408383846282959\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2256--TrainLoss:77.7208023071289--TrainR2:-4.10593318939209--TestLoss:85.48005676269531--TestR2:-5.40721321105957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2257--TrainLoss:77.70625305175781--TrainR2:-4.104977607727051--TestLoss:85.4644546508789--TestR2:-5.406044006347656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2258--TrainLoss:77.6917495727539--TrainR2:-4.104024887084961--TestLoss:85.4488525390625--TestR2:-5.404874324798584\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2259--TrainLoss:77.67720031738281--TrainR2:-4.103068828582764--TestLoss:85.43324279785156--TestR2:-5.4037041664123535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2260--TrainLoss:77.66272735595703--TrainR2:-4.102118492126465--TestLoss:85.41764068603516--TestR2:-5.4025349617004395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2261--TrainLoss:77.6481704711914--TrainR2:-4.101161956787109--TestLoss:85.40204620361328--TestR2:-5.401365756988525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2262--TrainLoss:77.63367462158203--TrainR2:-4.100209712982178--TestLoss:85.38644409179688--TestR2:-5.400196552276611\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2263--TrainLoss:77.61914825439453--TrainR2:-4.099255084991455--TestLoss:85.37085723876953--TestR2:-5.3990278244018555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2264--TrainLoss:77.60462951660156--TrainR2:-4.098301410675049--TestLoss:85.3552474975586--TestR2:-5.397858142852783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2265--TrainLoss:77.59009552001953--TrainR2:-4.097346782684326--TestLoss:85.33965301513672--TestR2:-5.396689414978027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2266--TrainLoss:77.57559967041016--TrainR2:-4.096394062042236--TestLoss:85.32406616210938--TestR2:-5.39552116394043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2267--TrainLoss:77.56108856201172--TrainR2:-4.095440864562988--TestLoss:85.3084716796875--TestR2:-5.394351959228516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2268--TrainLoss:77.54660034179688--TrainR2:-4.094489097595215--TestLoss:85.29290008544922--TestR2:-5.393184661865234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2269--TrainLoss:77.5320816040039--TrainR2:-4.09353494644165--TestLoss:85.27729034423828--TestR2:-5.392014980316162\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2270--TrainLoss:77.51760864257812--TrainR2:-4.092584609985352--TestLoss:85.2617416381836--TestR2:-5.390849590301514\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2271--TrainLoss:77.50308227539062--TrainR2:-4.091629981994629--TestLoss:85.24615478515625--TestR2:-5.389680862426758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2272--TrainLoss:77.48858642578125--TrainR2:-4.090677738189697--TestLoss:85.2305908203125--TestR2:-5.388514041900635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2273--TrainLoss:77.47407531738281--TrainR2:-4.089724540710449--TestLoss:85.21500396728516--TestR2:-5.387346267700195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2274--TrainLoss:77.45960998535156--TrainR2:-4.08877420425415--TestLoss:85.19943237304688--TestR2:-5.386178970336914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2275--TrainLoss:77.44510650634766--TrainR2:-4.0878214836120605--TestLoss:85.18386840820312--TestR2:-5.385012149810791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2276--TrainLoss:77.4306411743164--TrainR2:-4.086871147155762--TestLoss:85.16829681396484--TestR2:-5.38384485244751\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2277--TrainLoss:77.41612243652344--TrainR2:-4.0859174728393555--TestLoss:85.15272521972656--TestR2:-5.382678031921387\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2278--TrainLoss:77.40165710449219--TrainR2:-4.084967136383057--TestLoss:85.13716888427734--TestR2:-5.381511688232422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2279--TrainLoss:77.38716125488281--TrainR2:-4.084014415740967--TestLoss:85.1216049194336--TestR2:-5.380345344543457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2280--TrainLoss:77.37267303466797--TrainR2:-4.083063125610352--TestLoss:85.10604858398438--TestR2:-5.37917947769165\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2281--TrainLoss:77.35823822021484--TrainR2:-4.0821146965026855--TestLoss:85.09049224853516--TestR2:-5.378012657165527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2282--TrainLoss:77.34374237060547--TrainR2:-4.081162452697754--TestLoss:85.07494354248047--TestR2:-5.376847743988037\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2283--TrainLoss:77.32928466796875--TrainR2:-4.080212593078613--TestLoss:85.05937957763672--TestR2:-5.375681400299072\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2284--TrainLoss:77.31478118896484--TrainR2:-4.079259395599365--TestLoss:85.04383087158203--TestR2:-5.374515533447266\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2285--TrainLoss:77.30032348632812--TrainR2:-4.078309535980225--TestLoss:85.02828216552734--TestR2:-5.373350143432617\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2286--TrainLoss:77.2858657836914--TrainR2:-4.077359676361084--TestLoss:85.01272583007812--TestR2:-5.3721842765808105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2287--TrainLoss:77.27143096923828--TrainR2:-4.076411724090576--TestLoss:84.9971923828125--TestR2:-5.3710198402404785\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2288--TrainLoss:77.2569351196289--TrainR2:-4.075459003448486--TestLoss:84.98162841796875--TestR2:-5.3698530197143555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2289--TrainLoss:77.24246978759766--TrainR2:-4.074509143829346--TestLoss:84.96611022949219--TestR2:-5.368690013885498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2290--TrainLoss:77.22798919677734--TrainR2:-4.0735578536987305--TestLoss:84.95059967041016--TestR2:-5.367527008056641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2291--TrainLoss:77.21353149414062--TrainR2:-4.072607517242432--TestLoss:84.93502807617188--TestR2:-5.366360187530518\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2292--TrainLoss:77.19908142089844--TrainR2:-4.071658611297607--TestLoss:84.91950988769531--TestR2:-5.365196704864502\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2293--TrainLoss:77.18463134765625--TrainR2:-4.070709228515625--TestLoss:84.90396118164062--TestR2:-5.3640313148498535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2294--TrainLoss:77.17019653320312--TrainR2:-4.069760799407959--TestLoss:84.88844299316406--TestR2:-5.362868309020996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2295--TrainLoss:77.15572357177734--TrainR2:-4.068809986114502--TestLoss:84.8729019165039--TestR2:-5.361703395843506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2296--TrainLoss:77.14129638671875--TrainR2:-4.067862033843994--TestLoss:84.8573989868164--TestR2:-5.360541343688965\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2297--TrainLoss:77.1268539428711--TrainR2:-4.066913604736328--TestLoss:84.84185791015625--TestR2:-5.359376907348633\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2298--TrainLoss:77.1124038696289--TrainR2:-4.065964221954346--TestLoss:84.82633209228516--TestR2:-5.358212947845459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2299--TrainLoss:77.09795379638672--TrainR2:-4.065014839172363--TestLoss:84.81082916259766--TestR2:-5.357050895690918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2300--TrainLoss:77.08353424072266--TrainR2:-4.064067363739014--TestLoss:84.79529571533203--TestR2:-5.355886459350586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2301--TrainLoss:77.06909942626953--TrainR2:-4.063119411468506--TestLoss:84.77979278564453--TestR2:-5.354724407196045\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2302--TrainLoss:77.05467224121094--TrainR2:-4.062171459197998--TestLoss:84.7642822265625--TestR2:-5.3535614013671875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2303--TrainLoss:77.04025268554688--TrainR2:-4.061224460601807--TestLoss:84.748779296875--TestR2:-5.352399826049805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2304--TrainLoss:77.0257797241211--TrainR2:-4.060273170471191--TestLoss:84.7332534790039--TestR2:-5.351236343383789\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2305--TrainLoss:77.0113754272461--TrainR2:-4.059327125549316--TestLoss:84.71775817871094--TestR2:-5.350074291229248\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2306--TrainLoss:76.99691772460938--TrainR2:-4.058377265930176--TestLoss:84.70223999023438--TestR2:-5.348911762237549\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2307--TrainLoss:76.9825210571289--TrainR2:-4.057431697845459--TestLoss:84.68673706054688--TestR2:-5.34774923324585\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2308--TrainLoss:76.96810913085938--TrainR2:-4.056484699249268--TestLoss:84.67123413085938--TestR2:-5.346587657928467\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2309--TrainLoss:76.95365142822266--TrainR2:-4.055534839630127--TestLoss:84.65574645996094--TestR2:-5.345426559448242\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2310--TrainLoss:76.93926239013672--TrainR2:-4.05458927154541--TestLoss:84.64025115966797--TestR2:-5.344264984130859\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2311--TrainLoss:76.92484283447266--TrainR2:-4.053642272949219--TestLoss:84.62476348876953--TestR2:-5.343103885650635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2312--TrainLoss:76.91044616699219--TrainR2:-4.052696228027344--TestLoss:84.60926818847656--TestR2:-5.34194278717041\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2313--TrainLoss:76.89600372314453--TrainR2:-4.051747798919678--TestLoss:84.59378814697266--TestR2:-5.340782165527344\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2314--TrainLoss:76.8816146850586--TrainR2:-4.050802230834961--TestLoss:84.57828521728516--TestR2:-5.339620113372803\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2315--TrainLoss:76.86721801757812--TrainR2:-4.049856662750244--TestLoss:84.56279754638672--TestR2:-5.338459491729736\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2316--TrainLoss:76.85279083251953--TrainR2:-4.048908710479736--TestLoss:84.54732513427734--TestR2:-5.337299823760986\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2317--TrainLoss:76.83840942382812--TrainR2:-4.047963619232178--TestLoss:84.5318374633789--TestR2:-5.336138725280762\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2318--TrainLoss:76.82396697998047--TrainR2:-4.047015190124512--TestLoss:84.516357421875--TestR2:-5.3349785804748535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2319--TrainLoss:76.80957794189453--TrainR2:-4.046069622039795--TestLoss:84.50089263916016--TestR2:-5.3338189125061035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2320--TrainLoss:76.79524230957031--TrainR2:-4.045127868652344--TestLoss:84.48540496826172--TestR2:-5.332658767700195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2321--TrainLoss:76.78079986572266--TrainR2:-4.0441789627075195--TestLoss:84.46994018554688--TestR2:-5.331499099731445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2322--TrainLoss:76.76640319824219--TrainR2:-4.043233394622803--TestLoss:84.45446014404297--TestR2:-5.330338954925537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2323--TrainLoss:76.75202178955078--TrainR2:-4.042288780212402--TestLoss:84.43899536132812--TestR2:-5.329179763793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2324--TrainLoss:76.73764038085938--TrainR2:-4.041344165802002--TestLoss:84.42353057861328--TestR2:-5.3280205726623535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2325--TrainLoss:76.72325134277344--TrainR2:-4.040398597717285--TestLoss:84.4080581665039--TestR2:-5.3268609046936035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2326--TrainLoss:76.70885467529297--TrainR2:-4.039453029632568--TestLoss:84.39260864257812--TestR2:-5.325702667236328\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2327--TrainLoss:76.69447326660156--TrainR2:-4.03850793838501--TestLoss:84.37713623046875--TestR2:-5.324542999267578\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2328--TrainLoss:76.68013763427734--TrainR2:-4.0375657081604--TestLoss:84.36168670654297--TestR2:-5.323384761810303\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2329--TrainLoss:76.66571044921875--TrainR2:-4.036618232727051--TestLoss:84.34622192382812--TestR2:-5.322226047515869\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2330--TrainLoss:76.65133666992188--TrainR2:-4.035674095153809--TestLoss:84.3307876586914--TestR2:-5.32106876373291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2331--TrainLoss:76.63700866699219--TrainR2:-4.034732818603516--TestLoss:84.31532287597656--TestR2:-5.319910049438477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2332--TrainLoss:76.62260437011719--TrainR2:-4.033786773681641--TestLoss:84.29987335205078--TestR2:-5.318751811981201\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2333--TrainLoss:76.60823822021484--TrainR2:-4.032842636108398--TestLoss:84.284423828125--TestR2:-5.317593574523926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2334--TrainLoss:76.5938491821289--TrainR2:-4.031897068023682--TestLoss:84.26898193359375--TestR2:-5.316436290740967\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2335--TrainLoss:76.57951354980469--TrainR2:-4.030955791473389--TestLoss:84.25353240966797--TestR2:-5.315278053283691\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2336--TrainLoss:76.56515502929688--TrainR2:-4.030012130737305--TestLoss:84.23809051513672--TestR2:-5.314121246337891\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2337--TrainLoss:76.5507583618164--TrainR2:-4.029066562652588--TestLoss:84.22265625--TestR2:-5.312963962554932\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2338--TrainLoss:76.5363998413086--TrainR2:-4.028123378753662--TestLoss:84.20722198486328--TestR2:-5.311806678771973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2339--TrainLoss:76.52207946777344--TrainR2:-4.027182102203369--TestLoss:84.19178771972656--TestR2:-5.31065034866333\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2340--TrainLoss:76.50769805908203--TrainR2:-4.026237487792969--TestLoss:84.17634582519531--TestR2:-5.309492588043213\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2341--TrainLoss:76.49334716796875--TrainR2:-4.025294780731201--TestLoss:84.16091918945312--TestR2:-5.30833625793457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2342--TrainLoss:76.4790267944336--TrainR2:-4.024353981018066--TestLoss:84.14549255371094--TestR2:-5.307180404663086\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2343--TrainLoss:76.46463775634766--TrainR2:-4.023408889770508--TestLoss:84.13007354736328--TestR2:-5.306024074554443\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2344--TrainLoss:76.4502944946289--TrainR2:-4.02246618270874--TestLoss:84.1146469116211--TestR2:-5.304867744445801\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2345--TrainLoss:76.43597412109375--TrainR2:-4.021525859832764--TestLoss:84.0992202758789--TestR2:-5.303711891174316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2346--TrainLoss:76.4216079711914--TrainR2:-4.0205817222595215--TestLoss:84.08380889892578--TestR2:-5.30255651473999\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2347--TrainLoss:76.40727233886719--TrainR2:-4.01963996887207--TestLoss:84.06838989257812--TestR2:-5.301400661468506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2348--TrainLoss:76.39293670654297--TrainR2:-4.018698215484619--TestLoss:84.05296325683594--TestR2:-5.300244331359863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2349--TrainLoss:76.37860107421875--TrainR2:-4.017756462097168--TestLoss:84.03755950927734--TestR2:-5.2990899085998535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2350--TrainLoss:76.36427307128906--TrainR2:-4.016815185546875--TestLoss:84.02215576171875--TestR2:-5.2979350090026855\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2351--TrainLoss:76.34992980957031--TrainR2:-4.015872955322266--TestLoss:84.0067367553711--TestR2:-5.296779632568359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2352--TrainLoss:76.33560180664062--TrainR2:-4.0149312019348145--TestLoss:83.9913330078125--TestR2:-5.295624732971191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2353--TrainLoss:76.32128143310547--TrainR2:-4.013990879058838--TestLoss:83.9759292602539--TestR2:-5.294470310211182\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2354--TrainLoss:76.30696868896484--TrainR2:-4.013050079345703--TestLoss:83.96051788330078--TestR2:-5.2933149337768555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2355--TrainLoss:76.29259490966797--TrainR2:-4.012106418609619--TestLoss:83.94512176513672--TestR2:-5.292160987854004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2356--TrainLoss:76.2782974243164--TrainR2:-4.011167049407959--TestLoss:83.92972564697266--TestR2:-5.291007041931152\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2357--TrainLoss:76.26399993896484--TrainR2:-4.010227680206299--TestLoss:83.91432189941406--TestR2:-5.289852142333984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2358--TrainLoss:76.24964904785156--TrainR2:-4.009284973144531--TestLoss:83.89891052246094--TestR2:-5.288697719573975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2359--TrainLoss:76.2353515625--TrainR2:-4.008345603942871--TestLoss:83.88353729248047--TestR2:-5.287545204162598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2360--TrainLoss:76.22105407714844--TrainR2:-4.007406234741211--TestLoss:83.86814880371094--TestR2:-5.286391735076904\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2361--TrainLoss:76.20667266845703--TrainR2:-4.0064616203308105--TestLoss:83.85275268554688--TestR2:-5.2852373123168945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2362--TrainLoss:76.1924057006836--TrainR2:-4.005524158477783--TestLoss:83.8373794555664--TestR2:-5.284085273742676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2363--TrainLoss:76.17810821533203--TrainR2:-4.004585266113281--TestLoss:83.82199096679688--TestR2:-5.282931804656982\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2364--TrainLoss:76.16381072998047--TrainR2:-4.003645420074463--TestLoss:83.80659484863281--TestR2:-5.281777858734131\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2365--TrainLoss:76.14949035644531--TrainR2:-4.002704620361328--TestLoss:83.79122924804688--TestR2:-5.280625820159912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2366--TrainLoss:76.13520050048828--TrainR2:-4.001766204833984--TestLoss:83.77586364746094--TestR2:-5.279473781585693\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2367--TrainLoss:76.12089538574219--TrainR2:-4.000826358795166--TestLoss:83.76046752929688--TestR2:-5.2783203125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2368--TrainLoss:76.10658264160156--TrainR2:-3.9998860359191895--TestLoss:83.7450942993164--TestR2:-5.277167797088623\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2369--TrainLoss:76.09229278564453--TrainR2:-3.9989471435546875--TestLoss:83.72972869873047--TestR2:-5.2760162353515625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2370--TrainLoss:76.07801055908203--TrainR2:-3.9980087280273438--TestLoss:83.71435546875--TestR2:-5.2748637199401855\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2371--TrainLoss:76.0637435913086--TrainR2:-3.9970717430114746--TestLoss:83.698974609375--TestR2:-5.273711204528809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2372--TrainLoss:76.04940795898438--TrainR2:-3.9961299896240234--TestLoss:83.68363952636719--TestR2:-5.272561550140381\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2373--TrainLoss:76.0351333618164--TrainR2:-3.995192050933838--TestLoss:83.66826629638672--TestR2:-5.271409034729004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2374--TrainLoss:76.02088165283203--TrainR2:-3.994256019592285--TestLoss:83.65291595458984--TestR2:-5.270258903503418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2375--TrainLoss:76.00658416748047--TrainR2:-3.993316173553467--TestLoss:83.63753509521484--TestR2:-5.269105911254883\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2376--TrainLoss:75.9922866821289--TrainR2:-3.9923768043518066--TestLoss:83.6221694946289--TestR2:-5.267953872680664\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2377--TrainLoss:75.97803497314453--TrainR2:-3.991441249847412--TestLoss:83.6068344116211--TestR2:-5.2668046951293945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2378--TrainLoss:75.96376037597656--TrainR2:-3.9905028343200684--TestLoss:83.59147644042969--TestR2:-5.265653610229492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2379--TrainLoss:75.94947052001953--TrainR2:-3.9895644187927246--TestLoss:83.57612609863281--TestR2:-5.264503002166748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2380--TrainLoss:75.9352035522461--TrainR2:-3.9886269569396973--TestLoss:83.56077575683594--TestR2:-5.263352394104004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2381--TrainLoss:75.92092895507812--TrainR2:-3.98768949508667--TestLoss:83.54542541503906--TestR2:-5.26220178604126\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2382--TrainLoss:75.90668487548828--TrainR2:-3.986753463745117--TestLoss:83.53009796142578--TestR2:-5.26105260848999\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2383--TrainLoss:75.89238739013672--TrainR2:-3.985814094543457--TestLoss:83.51473236083984--TestR2:-5.25990104675293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2384--TrainLoss:75.87813568115234--TrainR2:-3.984877586364746--TestLoss:83.49942016601562--TestR2:-5.258753299713135\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2385--TrainLoss:75.86389923095703--TrainR2:-3.9839425086975098--TestLoss:83.48405456542969--TestR2:-5.257601737976074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2386--TrainLoss:75.8495864868164--TrainR2:-3.9830026626586914--TestLoss:83.46873474121094--TestR2:-5.256453037261963\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2387--TrainLoss:75.83533477783203--TrainR2:-3.9820661544799805--TestLoss:83.45338439941406--TestR2:-5.255302906036377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2388--TrainLoss:75.82110595703125--TrainR2:-3.981131076812744--TestLoss:83.43807220458984--TestR2:-5.254154682159424\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2389--TrainLoss:75.80684661865234--TrainR2:-3.980194568634033--TestLoss:83.4227294921875--TestR2:-5.253004550933838\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2390--TrainLoss:75.79257202148438--TrainR2:-3.9792566299438477--TestLoss:83.40741729736328--TestR2:-5.251856803894043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2391--TrainLoss:75.77833557128906--TrainR2:-3.9783215522766113--TestLoss:83.39207458496094--TestR2:-5.250707149505615\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2392--TrainLoss:75.76409149169922--TrainR2:-3.977385997772217--TestLoss:83.37676239013672--TestR2:-5.249558925628662\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2393--TrainLoss:75.74988555908203--TrainR2:-3.976452350616455--TestLoss:83.3614501953125--TestR2:-5.248411178588867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2394--TrainLoss:75.7356185913086--TrainR2:-3.975515365600586--TestLoss:83.34610748291016--TestR2:-5.247262001037598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2395--TrainLoss:75.72136688232422--TrainR2:-3.974578857421875--TestLoss:83.33081817626953--TestR2:-5.246115684509277\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2396--TrainLoss:75.70712280273438--TrainR2:-3.9736428260803223--TestLoss:83.31548309326172--TestR2:-5.24496603012085\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2397--TrainLoss:75.69288635253906--TrainR2:-3.972707748413086--TestLoss:83.30017852783203--TestR2:-5.243819236755371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2398--TrainLoss:75.67866516113281--TrainR2:-3.971773624420166--TestLoss:83.28485107421875--TestR2:-5.242670059204102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2399--TrainLoss:75.66441345214844--TrainR2:-3.970837116241455--TestLoss:83.26956939697266--TestR2:-5.241524696350098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2400--TrainLoss:75.65019989013672--TrainR2:-3.9699034690856934--TestLoss:83.2542724609375--TestR2:-5.240378379821777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2401--TrainLoss:75.63597869873047--TrainR2:-3.9689693450927734--TestLoss:83.23893737792969--TestR2:-5.23922872543335\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2402--TrainLoss:75.62173461914062--TrainR2:-3.9680333137512207--TestLoss:83.22364044189453--TestR2:-5.238081932067871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2403--TrainLoss:75.60749053955078--TrainR2:-3.967097759246826--TestLoss:83.20835876464844--TestR2:-5.236936569213867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2404--TrainLoss:75.59327697753906--TrainR2:-3.9661641120910645--TestLoss:83.19303894042969--TestR2:-5.235788345336914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2405--TrainLoss:75.5790786743164--TrainR2:-3.965231418609619--TestLoss:83.17774963378906--TestR2:-5.234642505645752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2406--TrainLoss:75.56484985351562--TrainR2:-3.964296340942383--TestLoss:83.16246795654297--TestR2:-5.23349666595459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2407--TrainLoss:75.55062103271484--TrainR2:-3.9633617401123047--TestLoss:83.14715576171875--TestR2:-5.232348918914795\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2408--TrainLoss:75.5363998413086--TrainR2:-3.9624276161193848--TestLoss:83.13188171386719--TestR2:-5.231204032897949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2409--TrainLoss:75.52216339111328--TrainR2:-3.9614920616149902--TestLoss:83.11660766601562--TestR2:-5.2300591468811035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2410--TrainLoss:75.50799560546875--TrainR2:-3.9605612754821777--TestLoss:83.10128784179688--TestR2:-5.22891092300415\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2411--TrainLoss:75.49378967285156--TrainR2:-3.959628105163574--TestLoss:83.08601379394531--TestR2:-5.227766036987305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2412--TrainLoss:75.47956848144531--TrainR2:-3.9586939811706543--TestLoss:83.07074737548828--TestR2:-5.226622104644775\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2413--TrainLoss:75.46533966064453--TrainR2:-3.957758903503418--TestLoss:83.05545043945312--TestR2:-5.225475311279297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2414--TrainLoss:75.45115661621094--TrainR2:-3.956827163696289--TestLoss:83.04016876220703--TestR2:-5.224329948425293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2415--TrainLoss:75.43697357177734--TrainR2:-3.95589542388916--TestLoss:83.02490234375--TestR2:-5.2231855392456055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2416--TrainLoss:75.42278289794922--TrainR2:-3.954963207244873--TestLoss:83.00965118408203--TestR2:-5.222042560577393\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2417--TrainLoss:75.40863800048828--TrainR2:-3.954033851623535--TestLoss:82.99436950683594--TestR2:-5.2208967208862305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2418--TrainLoss:75.39437103271484--TrainR2:-3.953096866607666--TestLoss:82.97908782958984--TestR2:-5.219751358032227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2419--TrainLoss:75.38017272949219--TrainR2:-3.9521641731262207--TestLoss:82.96382141113281--TestR2:-5.218607425689697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2420--TrainLoss:75.36599731445312--TrainR2:-3.951232433319092--TestLoss:82.94857025146484--TestR2:-5.217464447021484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2421--TrainLoss:75.35179901123047--TrainR2:-3.9502997398376465--TestLoss:82.93329620361328--TestR2:-5.216319561004639\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2422--TrainLoss:75.337646484375--TrainR2:-3.9493703842163086--TestLoss:82.91802978515625--TestR2:-5.215174674987793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2423--TrainLoss:75.32345581054688--TrainR2:-3.9484376907348633--TestLoss:82.90278625488281--TestR2:-5.214032173156738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2424--TrainLoss:75.30925750732422--TrainR2:-3.947504997253418--TestLoss:82.88751983642578--TestR2:-5.212887763977051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2425--TrainLoss:75.29505157470703--TrainR2:-3.9465718269348145--TestLoss:82.87226867675781--TestR2:-5.211744785308838\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2426--TrainLoss:75.2809066772461--TrainR2:-3.9456424713134766--TestLoss:82.85701751708984--TestR2:-5.210601329803467\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2427--TrainLoss:75.26673889160156--TrainR2:-3.944711685180664--TestLoss:82.8417739868164--TestR2:-5.209458827972412\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2428--TrainLoss:75.25257110595703--TrainR2:-3.9437808990478516--TestLoss:82.82649993896484--TestR2:-5.208314418792725\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2429--TrainLoss:75.23841094970703--TrainR2:-3.9428505897521973--TestLoss:82.81126403808594--TestR2:-5.207172393798828\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2430--TrainLoss:75.22419738769531--TrainR2:-3.9419169425964355--TestLoss:82.79602813720703--TestR2:-5.206030368804932\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2431--TrainLoss:75.21005249023438--TrainR2:-3.9409875869750977--TestLoss:82.78079223632812--TestR2:-5.204888343811035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2432--TrainLoss:75.19587707519531--TrainR2:-3.940056800842285--TestLoss:82.76554870605469--TestR2:-5.203745365142822\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2433--TrainLoss:75.1817398071289--TrainR2:-3.9391274452209473--TestLoss:82.75031280517578--TestR2:-5.202603340148926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2434--TrainLoss:75.16756439208984--TrainR2:-3.9381966590881348--TestLoss:82.7350845336914--TestR2:-5.2014617919921875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2435--TrainLoss:75.1534194946289--TrainR2:-3.937267303466797--TestLoss:82.71984100341797--TestR2:-5.200319290161133\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2436--TrainLoss:75.13923645019531--TrainR2:-3.9363350868225098--TestLoss:82.7046127319336--TestR2:-5.199178218841553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2437--TrainLoss:75.1250991821289--TrainR2:-3.93540620803833--TestLoss:82.68939971923828--TestR2:-5.198037624359131\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2438--TrainLoss:75.11093139648438--TrainR2:-3.934475898742676--TestLoss:82.67416381835938--TestR2:-5.196895599365234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2439--TrainLoss:75.0968017578125--TrainR2:-3.9335474967956543--TestLoss:82.65892028808594--TestR2:-5.19575309753418\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2440--TrainLoss:75.0826416015625--TrainR2:-3.932617664337158--TestLoss:82.64371490478516--TestR2:-5.194613456726074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2441--TrainLoss:75.06851959228516--TrainR2:-3.9316892623901367--TestLoss:82.62849426269531--TestR2:-5.193472385406494\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2442--TrainLoss:75.05435943603516--TrainR2:-3.9307594299316406--TestLoss:82.61327362060547--TestR2:-5.192331790924072\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2443--TrainLoss:75.04020690917969--TrainR2:-3.9298295974731445--TestLoss:82.59806060791016--TestR2:-5.19119119644165\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2444--TrainLoss:75.02606964111328--TrainR2:-3.928900718688965--TestLoss:82.58283233642578--TestR2:-5.190049648284912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2445--TrainLoss:75.01191711425781--TrainR2:-3.9279708862304688--TestLoss:82.567626953125--TestR2:-5.188910007476807\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2446--TrainLoss:74.99779510498047--TrainR2:-3.9270429611206055--TestLoss:82.55240631103516--TestR2:-5.187769412994385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2447--TrainLoss:74.98365783691406--TrainR2:-3.926114559173584--TestLoss:82.5372085571289--TestR2:-5.1866302490234375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2448--TrainLoss:74.96955108642578--TrainR2:-3.925187587738037--TestLoss:82.5219955444336--TestR2:-5.185490131378174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2449--TrainLoss:74.95541381835938--TrainR2:-3.9242587089538574--TestLoss:82.50679779052734--TestR2:-5.184350490570068\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2450--TrainLoss:74.94129943847656--TrainR2:-3.9233317375183105--TestLoss:82.49158477783203--TestR2:-5.183210372924805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2451--TrainLoss:74.92711639404297--TrainR2:-3.9223999977111816--TestLoss:82.47638702392578--TestR2:-5.182071208953857\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2452--TrainLoss:74.91301727294922--TrainR2:-3.921473503112793--TestLoss:82.46119689941406--TestR2:-5.180932998657227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2453--TrainLoss:74.89888763427734--TrainR2:-3.9205451011657715--TestLoss:82.44598388671875--TestR2:-5.1797919273376465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2454--TrainLoss:74.88477325439453--TrainR2:-3.9196181297302246--TestLoss:82.43080139160156--TestR2:-5.178654193878174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2455--TrainLoss:74.87066650390625--TrainR2:-3.9186911582946777--TestLoss:82.41561126708984--TestR2:-5.177515506744385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2456--TrainLoss:74.85658264160156--TrainR2:-3.9177660942077637--TestLoss:82.40042877197266--TestR2:-5.176377773284912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2457--TrainLoss:74.84243774414062--TrainR2:-3.916837215423584--TestLoss:82.38523864746094--TestR2:-5.175239086151123\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2458--TrainLoss:74.82832336425781--TrainR2:-3.915909767150879--TestLoss:82.37006378173828--TestR2:-5.17410135269165\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2459--TrainLoss:74.81421661376953--TrainR2:-3.914982795715332--TestLoss:82.35487365722656--TestR2:-5.1729631423950195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2460--TrainLoss:74.8000717163086--TrainR2:-3.9140539169311523--TestLoss:82.33967590332031--TestR2:-5.171823978424072\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2461--TrainLoss:74.78598022460938--TrainR2:-3.913127899169922--TestLoss:82.32449340820312--TestR2:-5.170685768127441\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2462--TrainLoss:74.77189636230469--TrainR2:-3.912202835083008--TestLoss:82.30931854248047--TestR2:-5.169548511505127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2463--TrainLoss:74.7577896118164--TrainR2:-3.911276340484619--TestLoss:82.29413604736328--TestR2:-5.168410301208496\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2464--TrainLoss:74.7436752319336--TrainR2:-3.910348415374756--TestLoss:82.27898406982422--TestR2:-5.167274475097656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2465--TrainLoss:74.7295913696289--TrainR2:-3.909423351287842--TestLoss:82.26382446289062--TestR2:-5.166138172149658\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2466--TrainLoss:74.71549224853516--TrainR2:-3.9084973335266113--TestLoss:82.2486343383789--TestR2:-5.164999961853027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2467--TrainLoss:74.70142364501953--TrainR2:-3.9075732231140137--TestLoss:82.23346710205078--TestR2:-5.163863182067871\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2468--TrainLoss:74.68732452392578--TrainR2:-3.906646728515625--TestLoss:82.21830749511719--TestR2:-5.162726402282715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2469--TrainLoss:74.67324829101562--TrainR2:-3.905721664428711--TestLoss:82.20314025878906--TestR2:-5.161589622497559\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2470--TrainLoss:74.65914154052734--TrainR2:-3.9047951698303223--TestLoss:82.18797302246094--TestR2:-5.1604533195495605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2471--TrainLoss:74.6450424194336--TrainR2:-3.9038686752319336--TestLoss:82.17282104492188--TestR2:-5.1593170166015625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2472--TrainLoss:74.63093566894531--TrainR2:-3.902942180633545--TestLoss:82.15766906738281--TestR2:-5.158181667327881\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2473--TrainLoss:74.61689758300781--TrainR2:-3.902019500732422--TestLoss:82.14251708984375--TestR2:-5.157045841217041\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2474--TrainLoss:74.60279846191406--TrainR2:-3.9010939598083496--TestLoss:82.12735748291016--TestR2:-5.155909538269043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2475--TrainLoss:74.58872985839844--TrainR2:-3.9001693725585938--TestLoss:82.11222076416016--TestR2:-5.1547746658325195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2476--TrainLoss:74.57466125488281--TrainR2:-3.899244785308838--TestLoss:82.09708404541016--TestR2:-5.153639793395996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2477--TrainLoss:74.56058502197266--TrainR2:-3.898320198059082--TestLoss:82.08192443847656--TestR2:-5.152503967285156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2478--TrainLoss:74.54653930664062--TrainR2:-3.897397518157959--TestLoss:82.0667724609375--TestR2:-5.151368141174316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2479--TrainLoss:74.53246307373047--TrainR2:-3.896472930908203--TestLoss:82.0516357421875--TestR2:-5.150233268737793\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2480--TrainLoss:74.51840209960938--TrainR2:-3.8955488204956055--TestLoss:82.03649139404297--TestR2:-5.1490983963012695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2481--TrainLoss:74.50431823730469--TrainR2:-3.8946237564086914--TestLoss:82.02135467529297--TestR2:-5.147964000701904\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2482--TrainLoss:74.4902572631836--TrainR2:-3.893700122833252--TestLoss:82.0062255859375--TestR2:-5.146830081939697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2483--TrainLoss:74.47621154785156--TrainR2:-3.892777442932129--TestLoss:81.9910888671875--TestR2:-5.145695209503174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2484--TrainLoss:74.46214294433594--TrainR2:-3.8918533325195312--TestLoss:81.9759521484375--TestR2:-5.14456033706665\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2485--TrainLoss:74.4480972290039--TrainR2:-3.89093017578125--TestLoss:81.96082305908203--TestR2:-5.143426895141602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2486--TrainLoss:74.43404388427734--TrainR2:-3.8900070190429688--TestLoss:81.94570922851562--TestR2:-5.142293453216553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2487--TrainLoss:74.42001342773438--TrainR2:-3.889085292816162--TestLoss:81.93057250976562--TestR2:-5.1411590576171875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2488--TrainLoss:74.40591430664062--TrainR2:-3.8881592750549316--TestLoss:81.91547393798828--TestR2:-5.1400275230407715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2489--TrainLoss:74.3918685913086--TrainR2:-3.8872365951538086--TestLoss:81.90033721923828--TestR2:-5.138892650604248\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2490--TrainLoss:74.3778305053711--TrainR2:-3.8863139152526855--TestLoss:81.88520812988281--TestR2:-5.137759208679199\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2491--TrainLoss:74.36376953125--TrainR2:-3.885390281677246--TestLoss:81.8700942993164--TestR2:-5.13662576675415\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2492--TrainLoss:74.34973907470703--TrainR2:-3.8844685554504395--TestLoss:81.85496520996094--TestR2:-5.135492324829102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2493--TrainLoss:74.33570861816406--TrainR2:-3.883546829223633--TestLoss:81.83985900878906--TestR2:-5.134359836578369\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2494--TrainLoss:74.32167053222656--TrainR2:-3.882624626159668--TestLoss:81.82476043701172--TestR2:-5.133228302001953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2495--TrainLoss:74.3075942993164--TrainR2:-3.881700038909912--TestLoss:81.80965423583984--TestR2:-5.132095813751221\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2496--TrainLoss:74.29360961914062--TrainR2:-3.880781650543213--TestLoss:81.79454040527344--TestR2:-5.13096284866333\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2497--TrainLoss:74.27955627441406--TrainR2:-3.8798580169677734--TestLoss:81.7794418334961--TestR2:-5.129831314086914\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2498--TrainLoss:74.2655258178711--TrainR2:-3.878936290740967--TestLoss:81.76434326171875--TestR2:-5.12869930267334\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2499--TrainLoss:74.25152587890625--TrainR2:-3.878016471862793--TestLoss:81.74922180175781--TestR2:-5.127566337585449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2500--TrainLoss:74.23747253417969--TrainR2:-3.8770928382873535--TestLoss:81.73413848876953--TestR2:-5.126435279846191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2501--TrainLoss:74.22348022460938--TrainR2:-3.876173973083496--TestLoss:81.71903991699219--TestR2:-5.125303745269775\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2502--TrainLoss:74.20941925048828--TrainR2:-3.8752503395080566--TestLoss:81.70394134521484--TestR2:-5.124172210693359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2503--TrainLoss:74.19542694091797--TrainR2:-3.874330997467041--TestLoss:81.68885803222656--TestR2:-5.12304162979126\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2504--TrainLoss:74.18138122558594--TrainR2:-3.8734078407287598--TestLoss:81.67377471923828--TestR2:-5.121910572052002\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2505--TrainLoss:74.1673812866211--TrainR2:-3.872488498687744--TestLoss:81.65867614746094--TestR2:-5.120779037475586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2506--TrainLoss:74.15338134765625--TrainR2:-3.8715691566467285--TestLoss:81.64359283447266--TestR2:-5.119648456573486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2507--TrainLoss:74.13936614990234--TrainR2:-3.87064790725708--TestLoss:81.62850189208984--TestR2:-5.1185173988342285\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2508--TrainLoss:74.12533569335938--TrainR2:-3.8697266578674316--TestLoss:81.61343383789062--TestR2:-5.117387771606445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2509--TrainLoss:74.1113510131836--TrainR2:-3.868807792663574--TestLoss:81.59833526611328--TestR2:-5.116256237030029\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2510--TrainLoss:74.09732055664062--TrainR2:-3.8678860664367676--TestLoss:81.5832748413086--TestR2:-5.1151275634765625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2511--TrainLoss:74.08334350585938--TrainR2:-3.86696720123291--TestLoss:81.56817626953125--TestR2:-5.113995552062988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2512--TrainLoss:74.0693588256836--TrainR2:-3.8660483360290527--TestLoss:81.5531234741211--TestR2:-5.11286735534668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2513--TrainLoss:74.05532836914062--TrainR2:-3.8651275634765625--TestLoss:81.53804779052734--TestR2:-5.111737251281738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2514--TrainLoss:74.04137420654297--TrainR2:-3.8642101287841797--TestLoss:81.52297973632812--TestR2:-5.110608100891113\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2515--TrainLoss:74.02734375--TrainR2:-3.863288402557373--TestLoss:81.50791931152344--TestR2:-5.109478950500488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2516--TrainLoss:74.01335906982422--TrainR2:-3.862370014190674--TestLoss:81.49284362792969--TestR2:-5.108349323272705\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2517--TrainLoss:73.99935913085938--TrainR2:-3.861449718475342--TestLoss:81.477783203125--TestR2:-5.10722017288208\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2518--TrainLoss:73.98535919189453--TrainR2:-3.860530376434326--TestLoss:81.46272277832031--TestR2:-5.106091022491455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2519--TrainLoss:73.97140502929688--TrainR2:-3.8596138954162598--TestLoss:81.44766235351562--TestR2:-5.104962348937988\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2520--TrainLoss:73.95738983154297--TrainR2:-3.8586926460266113--TestLoss:81.43260192871094--TestR2:-5.1038336753845215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2521--TrainLoss:73.94341278076172--TrainR2:-3.8577747344970703--TestLoss:81.41755676269531--TestR2:-5.102705955505371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2522--TrainLoss:73.92945098876953--TrainR2:-3.8568572998046875--TestLoss:81.40250396728516--TestR2:-5.101577281951904\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2523--TrainLoss:73.91545104980469--TrainR2:-3.8559374809265137--TestLoss:81.387451171875--TestR2:-5.100449085235596\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2524--TrainLoss:73.90147399902344--TrainR2:-3.8550195693969727--TestLoss:81.37239837646484--TestR2:-5.099321365356445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2525--TrainLoss:73.8874740600586--TrainR2:-3.854099750518799--TestLoss:81.35736083984375--TestR2:-5.098193645477295\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2526--TrainLoss:73.87353515625--TrainR2:-3.8531837463378906--TestLoss:81.34231567382812--TestR2:-5.097066402435303\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2527--TrainLoss:73.85958099365234--TrainR2:-3.852267265319824--TestLoss:81.32726287841797--TestR2:-5.095937728881836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2528--TrainLoss:73.8455810546875--TrainR2:-3.8513474464416504--TestLoss:81.31222534179688--TestR2:-5.094810485839844\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2529--TrainLoss:73.83160400390625--TrainR2:-3.850429058074951--TestLoss:81.29718780517578--TestR2:-5.09368371963501\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2530--TrainLoss:73.81769561767578--TrainR2:-3.849515438079834--TestLoss:81.28213500976562--TestR2:-5.092555046081543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2531--TrainLoss:73.80371856689453--TrainR2:-3.8485970497131348--TestLoss:81.26712036132812--TestR2:-5.091429710388184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2532--TrainLoss:73.78975677490234--TrainR2:-3.84768009185791--TestLoss:81.25208282470703--TestR2:-5.09030294418335\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2533--TrainLoss:73.77584075927734--TrainR2:-3.8467659950256348--TestLoss:81.237060546875--TestR2:-5.089176654815674\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2534--TrainLoss:73.7618408203125--TrainR2:-3.845846176147461--TestLoss:81.22201538085938--TestR2:-5.088048934936523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2535--TrainLoss:73.74788665771484--TrainR2:-3.8449296951293945--TestLoss:81.20699310302734--TestR2:-5.086923122406006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2536--TrainLoss:73.73393249511719--TrainR2:-3.84401273727417--TestLoss:81.19197845458984--TestR2:-5.085797309875488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2537--TrainLoss:73.71997833251953--TrainR2:-3.8430957794189453--TestLoss:81.17695617675781--TestR2:-5.084671497344971\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2538--TrainLoss:73.7060317993164--TrainR2:-3.842179775238037--TestLoss:81.16194915771484--TestR2:-5.0835466384887695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2539--TrainLoss:73.69209289550781--TrainR2:-3.841264247894287--TestLoss:81.14691925048828--TestR2:-5.082420349121094\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2540--TrainLoss:73.67811584472656--TrainR2:-3.840345859527588--TestLoss:81.13190460205078--TestR2:-5.081294536590576\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2541--TrainLoss:73.66419219970703--TrainR2:-3.839430809020996--TestLoss:81.11689758300781--TestR2:-5.080169677734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2542--TrainLoss:73.65026092529297--TrainR2:-3.8385157585144043--TestLoss:81.10189056396484--TestR2:-5.079044818878174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2543--TrainLoss:73.63630676269531--TrainR2:-3.837599277496338--TestLoss:81.08686828613281--TestR2:-5.077919006347656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2544--TrainLoss:73.62237548828125--TrainR2:-3.836683750152588--TestLoss:81.07186889648438--TestR2:-5.076794624328613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2545--TrainLoss:73.60845184326172--TrainR2:-3.8357691764831543--TestLoss:81.05686950683594--TestR2:-5.07567024230957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2546--TrainLoss:73.59453582763672--TrainR2:-3.834855079650879--TestLoss:81.04185485839844--TestR2:-5.074544906616211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2547--TrainLoss:73.58059692382812--TrainR2:-3.8339390754699707--TestLoss:81.02687072753906--TestR2:-5.073421478271484\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2548--TrainLoss:73.56671142578125--TrainR2:-3.833026885986328--TestLoss:81.0118637084961--TestR2:-5.072296619415283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2549--TrainLoss:73.55274200439453--TrainR2:-3.832108974456787--TestLoss:80.99684143066406--TestR2:-5.071171283721924\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2550--TrainLoss:73.53881072998047--TrainR2:-3.8311944007873535--TestLoss:80.98184967041016--TestR2:-5.070047378540039\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2551--TrainLoss:73.52490234375--TrainR2:-3.830280303955078--TestLoss:80.96685028076172--TestR2:-5.068922996520996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2552--TrainLoss:73.5109634399414--TrainR2:-3.829364776611328--TestLoss:80.95187377929688--TestR2:-5.067800045013428\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2553--TrainLoss:73.4970474243164--TrainR2:-3.8284502029418945--TestLoss:80.9368896484375--TestR2:-5.066677093505859\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2554--TrainLoss:73.48314666748047--TrainR2:-3.8275370597839355--TestLoss:80.92189025878906--TestR2:-5.065553188323975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2555--TrainLoss:73.46920013427734--TrainR2:-3.8266210556030273--TestLoss:80.90692901611328--TestR2:-5.064431190490723\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2556--TrainLoss:73.4552993774414--TrainR2:-3.8257079124450684--TestLoss:80.89193725585938--TestR2:-5.063307762145996\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2557--TrainLoss:73.44139862060547--TrainR2:-3.824794292449951--TestLoss:80.876953125--TestR2:-5.0621843338012695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2558--TrainLoss:73.42752075195312--TrainR2:-3.823883056640625--TestLoss:80.86197662353516--TestR2:-5.061062335968018\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2559--TrainLoss:73.4135971069336--TrainR2:-3.822968006134033--TestLoss:80.84699249267578--TestR2:-5.059938907623291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2560--TrainLoss:73.39969635009766--TrainR2:-3.822054862976074--TestLoss:80.83201599121094--TestR2:-5.058816432952881\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2561--TrainLoss:73.38578796386719--TrainR2:-3.821140766143799--TestLoss:80.81706237792969--TestR2:-5.057695388793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2562--TrainLoss:73.37189483642578--TrainR2:-3.8202285766601562--TestLoss:80.80208587646484--TestR2:-5.056572437286377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2563--TrainLoss:73.35797119140625--TrainR2:-3.8193135261535645--TestLoss:80.78712463378906--TestR2:-5.0554518699646\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2564--TrainLoss:73.34410858154297--TrainR2:-3.8184027671813965--TestLoss:80.77214050292969--TestR2:-5.054328441619873\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2565--TrainLoss:73.33020782470703--TrainR2:-3.8174896240234375--TestLoss:80.75718688964844--TestR2:-5.0532073974609375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2566--TrainLoss:73.31629180908203--TrainR2:-3.816575527191162--TestLoss:80.7422103881836--TestR2:-5.052084922790527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2567--TrainLoss:73.30243682861328--TrainR2:-3.8156652450561523--TestLoss:80.7272720336914--TestR2:-5.050964832305908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2568--TrainLoss:73.28854370117188--TrainR2:-3.8147525787353516--TestLoss:80.71229553222656--TestR2:-5.049842357635498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2569--TrainLoss:73.2746353149414--TrainR2:-3.8138389587402344--TestLoss:80.69734954833984--TestR2:-5.048722267150879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2570--TrainLoss:73.2607650756836--TrainR2:-3.812927722930908--TestLoss:80.6823959350586--TestR2:-5.047601699829102\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2571--TrainLoss:73.24688720703125--TrainR2:-3.8120155334472656--TestLoss:80.66744995117188--TestR2:-5.046481132507324\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2572--TrainLoss:73.23301696777344--TrainR2:-3.8111047744750977--TestLoss:80.65249633789062--TestR2:-5.045360088348389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2573--TrainLoss:73.21910858154297--TrainR2:-3.8101906776428223--TestLoss:80.6375503540039--TestR2:-5.0442399978637695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2574--TrainLoss:73.20523071289062--TrainR2:-3.809279441833496--TestLoss:80.62260437011719--TestR2:-5.043119430541992\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2575--TrainLoss:73.19139862060547--TrainR2:-3.808370590209961--TestLoss:80.60765838623047--TestR2:-5.041999340057373\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2576--TrainLoss:73.17752075195312--TrainR2:-3.8074584007263184--TestLoss:80.59272003173828--TestR2:-5.040879249572754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2577--TrainLoss:73.16365051269531--TrainR2:-3.806547164916992--TestLoss:80.57777404785156--TestR2:-5.039759635925293\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2578--TrainLoss:73.14976501464844--TrainR2:-3.805635452270508--TestLoss:80.56283569335938--TestR2:-5.038640022277832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2579--TrainLoss:73.13590240478516--TrainR2:-3.80472469329834--TestLoss:80.54790496826172--TestR2:-5.037520408630371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2580--TrainLoss:73.12203216552734--TrainR2:-3.8038134574890137--TestLoss:80.53297424316406--TestR2:-5.036401748657227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2581--TrainLoss:73.10816192626953--TrainR2:-3.8029022216796875--TestLoss:80.5180435180664--TestR2:-5.035282135009766\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2582--TrainLoss:73.09435272216797--TrainR2:-3.801994800567627--TestLoss:80.50310516357422--TestR2:-5.034162521362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2583--TrainLoss:73.08049011230469--TrainR2:-3.801084041595459--TestLoss:80.48819732666016--TestR2:-5.033044815063477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2584--TrainLoss:73.06661987304688--TrainR2:-3.800172805786133--TestLoss:80.4732666015625--TestR2:-5.031925678253174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2585--TrainLoss:73.0527572631836--TrainR2:-3.799262046813965--TestLoss:80.45834350585938--TestR2:-5.0308074951171875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2586--TrainLoss:73.03890991210938--TrainR2:-3.7983527183532715--TestLoss:80.44342041015625--TestR2:-5.029688835144043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2587--TrainLoss:73.02506256103516--TrainR2:-3.79744291305542--TestLoss:80.42850494384766--TestR2:-5.028570652008057\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2588--TrainLoss:73.01122283935547--TrainR2:-3.7965335845947266--TestLoss:80.41358184814453--TestR2:-5.027451992034912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2589--TrainLoss:72.99738311767578--TrainR2:-3.795624256134033--TestLoss:80.39866638183594--TestR2:-5.026333808898926\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2590--TrainLoss:72.98353576660156--TrainR2:-3.79471492767334--TestLoss:80.38373565673828--TestR2:-5.025215148925781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2591--TrainLoss:72.96969604492188--TrainR2:-3.7938055992126465--TestLoss:80.36884307861328--TestR2:-5.0240983963012695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2592--TrainLoss:72.95585632324219--TrainR2:-3.792896270751953--TestLoss:80.35394287109375--TestR2:-5.022981643676758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2593--TrainLoss:72.94198608398438--TrainR2:-3.791985034942627--TestLoss:80.33902740478516--TestR2:-5.021864414215088\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2594--TrainLoss:72.92820739746094--TrainR2:-3.7910799980163574--TestLoss:80.3241195678711--TestR2:-5.02074670791626\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2595--TrainLoss:72.91436004638672--TrainR2:-3.790170192718506--TestLoss:80.30921173095703--TestR2:-5.01962947845459\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2596--TrainLoss:72.90052795410156--TrainR2:-3.789261817932129--TestLoss:80.29431915283203--TestR2:-5.018513202667236\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2597--TrainLoss:72.88668823242188--TrainR2:-3.7883520126342773--TestLoss:80.2794189453125--TestR2:-5.017395973205566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2598--TrainLoss:72.87284851074219--TrainR2:-3.787443161010742--TestLoss:80.26451873779297--TestR2:-5.016279220581055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2599--TrainLoss:72.8590087890625--TrainR2:-3.786533832550049--TestLoss:80.2496337890625--TestR2:-5.015163421630859\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2600--TrainLoss:72.84526824951172--TrainR2:-3.7856311798095703--TestLoss:80.23473358154297--TestR2:-5.014046669006348\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2601--TrainLoss:72.83139038085938--TrainR2:-3.7847189903259277--TestLoss:80.21984100341797--TestR2:-5.012930393218994\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2602--TrainLoss:72.81759643554688--TrainR2:-3.783812999725342--TestLoss:80.2049560546875--TestR2:-5.011814117431641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2603--TrainLoss:72.80375671386719--TrainR2:-3.7829041481018066--TestLoss:80.1900863647461--TestR2:-5.01069974899292\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2604--TrainLoss:72.78993225097656--TrainR2:-3.7819957733154297--TestLoss:80.1751708984375--TestR2:-5.009582042694092\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2605--TrainLoss:72.77615356445312--TrainR2:-3.78109073638916--TestLoss:80.16031646728516--TestR2:-5.008468151092529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2606--TrainLoss:72.76232147216797--TrainR2:-3.780181884765625--TestLoss:80.14541625976562--TestR2:-5.007351398468018\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2607--TrainLoss:72.74850463867188--TrainR2:-3.7792744636535645--TestLoss:80.13054656982422--TestR2:-5.006237030029297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2608--TrainLoss:72.73468780517578--TrainR2:-3.7783665657043457--TestLoss:80.11566925048828--TestR2:-5.00512170791626\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2609--TrainLoss:72.72089385986328--TrainR2:-3.7774600982666016--TestLoss:80.1008071899414--TestR2:-5.004007816314697\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2610--TrainLoss:72.7071304321289--TrainR2:-3.7765560150146484--TestLoss:80.08592224121094--TestR2:-5.00289249420166\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2611--TrainLoss:72.69329833984375--TrainR2:-3.7756476402282715--TestLoss:80.0710678100586--TestR2:-5.001779079437256\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2612--TrainLoss:72.67950439453125--TrainR2:-3.7747411727905273--TestLoss:80.05618286132812--TestR2:-5.0006632804870605\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2613--TrainLoss:72.66571807861328--TrainR2:-3.7738351821899414--TestLoss:80.04132843017578--TestR2:-4.999549865722656\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2614--TrainLoss:72.65189361572266--TrainR2:-3.7729272842407227--TestLoss:80.02645874023438--TestR2:-4.9984354972839355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2615--TrainLoss:72.63817596435547--TrainR2:-3.7720260620117188--TestLoss:80.0115966796875--TestR2:-4.997321128845215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2616--TrainLoss:72.62431335449219--TrainR2:-3.771115779876709--TestLoss:79.99674224853516--TestR2:-4.9962077140808105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2617--TrainLoss:72.61052703857422--TrainR2:-3.770209789276123--TestLoss:79.98188018798828--TestR2:-4.99509334564209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2618--TrainLoss:72.59674072265625--TrainR2:-3.7693042755126953--TestLoss:79.9670181274414--TestR2:-4.993979454040527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2619--TrainLoss:72.5829849243164--TrainR2:-3.7684006690979004--TestLoss:79.95217895507812--TestR2:-4.992867469787598\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2620--TrainLoss:72.5692138671875--TrainR2:-3.767495632171631--TestLoss:79.93731689453125--TestR2:-4.991753578186035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2621--TrainLoss:72.5554428100586--TrainR2:-3.7665905952453613--TestLoss:79.92247772216797--TestR2:-4.990641117095947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2622--TrainLoss:72.54159545898438--TrainR2:-3.765681266784668--TestLoss:79.90762329101562--TestR2:-4.989527702331543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2623--TrainLoss:72.52787780761719--TrainR2:-3.764780044555664--TestLoss:79.89277648925781--TestR2:-4.988415241241455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2624--TrainLoss:72.51409149169922--TrainR2:-3.7638745307922363--TestLoss:79.87794494628906--TestR2:-4.987303256988525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2625--TrainLoss:72.50032806396484--TrainR2:-3.762969970703125--TestLoss:79.86310577392578--TestR2:-4.9861907958984375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2626--TrainLoss:72.48654174804688--TrainR2:-3.7620644569396973--TestLoss:79.84825897216797--TestR2:-4.985077857971191\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2627--TrainLoss:72.47274017333984--TrainR2:-3.761157512664795--TestLoss:79.83341217041016--TestR2:-4.9839653968811035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2628--TrainLoss:72.45901489257812--TrainR2:-3.760255813598633--TestLoss:79.81858825683594--TestR2:-4.982853889465332\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2629--TrainLoss:72.44525146484375--TrainR2:-3.7593517303466797--TestLoss:79.80374908447266--TestR2:-4.981741905212402\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2630--TrainLoss:72.43146514892578--TrainR2:-3.7584457397460938--TestLoss:79.7889175415039--TestR2:-4.980630397796631\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2631--TrainLoss:72.41771697998047--TrainR2:-3.7575430870056152--TestLoss:79.77408599853516--TestR2:-4.979518413543701\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2632--TrainLoss:72.40397644042969--TrainR2:-3.7566404342651367--TestLoss:79.75927734375--TestR2:-4.978408336639404\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2633--TrainLoss:72.39019012451172--TrainR2:-3.755734443664551--TestLoss:79.74443817138672--TestR2:-4.977295875549316\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2634--TrainLoss:72.3764419555664--TrainR2:-3.754831314086914--TestLoss:79.7296142578125--TestR2:-4.976185321807861\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2635--TrainLoss:72.36268615722656--TrainR2:-3.753927707672119--TestLoss:79.71479034423828--TestR2:-4.975074291229248\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2636--TrainLoss:72.34896850585938--TrainR2:-3.7530264854431152--TestLoss:79.6999740600586--TestR2:-4.973963260650635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2637--TrainLoss:72.33521270751953--TrainR2:-3.752122402191162--TestLoss:79.68515014648438--TestR2:-4.9728522300720215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2638--TrainLoss:72.32145690917969--TrainR2:-3.751218795776367--TestLoss:79.67033386230469--TestR2:-4.971741676330566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2639--TrainLoss:72.3077392578125--TrainR2:-3.7503175735473633--TestLoss:79.65553283691406--TestR2:-4.970632076263428\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2640--TrainLoss:72.29397583007812--TrainR2:-3.74941349029541--TestLoss:79.6407241821289--TestR2:-4.969522476196289\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2641--TrainLoss:72.28022766113281--TrainR2:-3.7485103607177734--TestLoss:79.62591552734375--TestR2:-4.968411922454834\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2642--TrainLoss:72.26648712158203--TrainR2:-3.747607707977295--TestLoss:79.61109924316406--TestR2:-4.967301845550537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2643--TrainLoss:72.25276184082031--TrainR2:-3.746706008911133--TestLoss:79.5963134765625--TestR2:-4.966193675994873\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2644--TrainLoss:72.23905181884766--TrainR2:-3.745805263519287--TestLoss:79.58148956298828--TestR2:-4.96508264541626\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2645--TrainLoss:72.22529602050781--TrainR2:-3.744901657104492--TestLoss:79.56670379638672--TestR2:-4.9639739990234375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2646--TrainLoss:72.21156311035156--TrainR2:-3.743999481201172--TestLoss:79.55189514160156--TestR2:-4.962863922119141\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2647--TrainLoss:72.19784545898438--TrainR2:-3.743098258972168--TestLoss:79.537109375--TestR2:-4.961755752563477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2648--TrainLoss:72.18411254882812--TrainR2:-3.7421960830688477--TestLoss:79.52230072021484--TestR2:-4.96064567565918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2649--TrainLoss:72.17037200927734--TrainR2:-3.741293430328369--TestLoss:79.50751495361328--TestR2:-4.959537506103516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2650--TrainLoss:72.15667724609375--TrainR2:-3.74039363861084--TestLoss:79.49272155761719--TestR2:-4.958428382873535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2651--TrainLoss:72.14297485351562--TrainR2:-3.7394933700561523--TestLoss:79.47794342041016--TestR2:-4.9573211669921875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2652--TrainLoss:72.12922668457031--TrainR2:-3.7385902404785156--TestLoss:79.46316528320312--TestR2:-4.956212997436523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2653--TrainLoss:72.1155014038086--TrainR2:-3.7376885414123535--TestLoss:79.4483642578125--TestR2:-4.955103874206543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2654--TrainLoss:72.10179901123047--TrainR2:-3.736788272857666--TestLoss:79.43357849121094--TestR2:-4.953995704650879\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2655--TrainLoss:72.08809661865234--TrainR2:-3.7358880043029785--TestLoss:79.41880798339844--TestR2:-4.952888488769531\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2656--TrainLoss:72.07435607910156--TrainR2:-3.734985828399658--TestLoss:79.40403747558594--TestR2:-4.951780796051025\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2657--TrainLoss:72.0606689453125--TrainR2:-3.734086036682129--TestLoss:79.38925170898438--TestR2:-4.9506731033325195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2658--TrainLoss:72.0469741821289--TrainR2:-3.7331862449645996--TestLoss:79.3744888305664--TestR2:-4.94956636428833\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2659--TrainLoss:72.03323364257812--TrainR2:-3.7322840690612793--TestLoss:79.35971069335938--TestR2:-4.948458671569824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2660--TrainLoss:72.01959228515625--TrainR2:-3.7313876152038574--TestLoss:79.34493255615234--TestR2:-4.947350978851318\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2661--TrainLoss:72.0058822631836--TrainR2:-3.7304868698120117--TestLoss:79.33019256591797--TestR2:-4.946246147155762\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2662--TrainLoss:71.99217987060547--TrainR2:-3.7295870780944824--TestLoss:79.3154067993164--TestR2:-4.9451375007629395\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2663--TrainLoss:71.9784927368164--TrainR2:-3.728687286376953--TestLoss:79.3006362915039--TestR2:-4.94403076171875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2664--TrainLoss:71.96478271484375--TrainR2:-3.7277870178222656--TestLoss:79.285888671875--TestR2:-4.942925453186035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2665--TrainLoss:71.95108032226562--TrainR2:-3.726886749267578--TestLoss:79.27112579345703--TestR2:-4.941818714141846\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2666--TrainLoss:71.93737030029297--TrainR2:-3.7259860038757324--TestLoss:79.25636291503906--TestR2:-4.9407124519348145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2667--TrainLoss:71.92371368408203--TrainR2:-3.725088596343994--TestLoss:79.24160766601562--TestR2:-4.939606189727783\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2668--TrainLoss:71.91001892089844--TrainR2:-3.724188804626465--TestLoss:79.22686004638672--TestR2:-4.93850040435791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2669--TrainLoss:71.89633178710938--TrainR2:-3.723289966583252--TestLoss:79.21210479736328--TestR2:-4.937394618988037\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2670--TrainLoss:71.88267517089844--TrainR2:-3.7223925590515137--TestLoss:79.19734954833984--TestR2:-4.936288356781006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2671--TrainLoss:71.86898040771484--TrainR2:-3.7214932441711426--TestLoss:79.18260192871094--TestR2:-4.935183525085449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2672--TrainLoss:71.85529327392578--TrainR2:-3.7205939292907715--TestLoss:79.16786193847656--TestR2:-4.934078693389893\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2673--TrainLoss:71.84166717529297--TrainR2:-3.719698905944824--TestLoss:79.15312957763672--TestR2:-4.932973861694336\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2674--TrainLoss:71.82794952392578--TrainR2:-3.7187976837158203--TestLoss:79.13838195800781--TestR2:-4.931869029998779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2675--TrainLoss:71.81427001953125--TrainR2:-3.7178988456726074--TestLoss:79.12364196777344--TestR2:-4.9307637214660645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2676--TrainLoss:71.80062103271484--TrainR2:-3.7170023918151855--TestLoss:79.10890197753906--TestR2:-4.929658889770508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2677--TrainLoss:71.78694152832031--TrainR2:-3.7161035537719727--TestLoss:79.09416961669922--TestR2:-4.928555011749268\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2678--TrainLoss:71.77326202392578--TrainR2:-3.715205192565918--TestLoss:79.07942962646484--TestR2:-4.927449703216553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2679--TrainLoss:71.75961303710938--TrainR2:-3.714308261871338--TestLoss:79.06470489501953--TestR2:-4.926346302032471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2680--TrainLoss:71.74593353271484--TrainR2:-3.713409423828125--TestLoss:79.04996490478516--TestR2:-4.925240993499756\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2681--TrainLoss:71.73228454589844--TrainR2:-3.712512969970703--TestLoss:79.03524780273438--TestR2:-4.92413854598999\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2682--TrainLoss:71.7186508178711--TrainR2:-3.7116174697875977--TestLoss:79.02052307128906--TestR2:-4.92303466796875\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2683--TrainLoss:71.7049789428711--TrainR2:-3.710719108581543--TestLoss:79.00580596923828--TestR2:-4.921931266784668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2684--TrainLoss:71.69134521484375--TrainR2:-3.7098231315612793--TestLoss:78.99108123779297--TestR2:-4.920827865600586\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2685--TrainLoss:71.67768096923828--TrainR2:-3.708925724029541--TestLoss:78.97636413574219--TestR2:-4.919724464416504\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2686--TrainLoss:71.66403198242188--TrainR2:-3.708028793334961--TestLoss:78.96163940429688--TestR2:-4.918621063232422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2687--TrainLoss:71.65042114257812--TrainR2:-3.70713472366333--TestLoss:78.94692993164062--TestR2:-4.917518138885498\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2688--TrainLoss:71.6367416381836--TrainR2:-3.706235885620117--TestLoss:78.93219757080078--TestR2:-4.916414260864258\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2689--TrainLoss:71.62309265136719--TrainR2:-3.7053394317626953--TestLoss:78.9175033569336--TestR2:-4.915312767028809\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2690--TrainLoss:71.6094741821289--TrainR2:-3.7044448852539062--TestLoss:78.9028091430664--TestR2:-4.914211273193359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2691--TrainLoss:71.59583282470703--TrainR2:-3.7035484313964844--TestLoss:78.88809967041016--TestR2:-4.9131083488464355\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2692--TrainLoss:71.58214569091797--TrainR2:-3.7026491165161133--TestLoss:78.87337493896484--TestR2:-4.912005424499512\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2693--TrainLoss:71.56853485107422--TrainR2:-3.7017550468444824--TestLoss:78.85868835449219--TestR2:-4.9109039306640625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2694--TrainLoss:71.55489349365234--TrainR2:-3.7008585929870605--TestLoss:78.84397888183594--TestR2:-4.909801959991455\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2695--TrainLoss:71.54129791259766--TrainR2:-3.699965476989746--TestLoss:78.82929229736328--TestR2:-4.908700466156006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2696--TrainLoss:71.52764129638672--TrainR2:-3.699068546295166--TestLoss:78.81458282470703--TestR2:-4.907598495483398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2697--TrainLoss:71.51399993896484--TrainR2:-3.698172092437744--TestLoss:78.79988861083984--TestR2:-4.906496524810791\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2698--TrainLoss:71.50040435791016--TrainR2:-3.6972789764404297--TestLoss:78.78520965576172--TestR2:-4.905396461486816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2699--TrainLoss:71.48677062988281--TrainR2:-3.696383476257324--TestLoss:78.77051544189453--TestR2:-4.904294967651367\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2700--TrainLoss:71.47317504882812--TrainR2:-3.6954903602600098--TestLoss:78.75582885742188--TestR2:-4.903194427490234\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2701--TrainLoss:71.45953369140625--TrainR2:-3.694593906402588--TestLoss:78.74114227294922--TestR2:-4.902093410491943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2702--TrainLoss:71.4458999633789--TrainR2:-3.6936984062194824--TestLoss:78.7264633178711--TestR2:-4.900993347167969\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2703--TrainLoss:71.43231201171875--TrainR2:-3.692805767059326--TestLoss:78.71176147460938--TestR2:-4.899891376495361\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2704--TrainLoss:71.41869354248047--TrainR2:-3.691910743713379--TestLoss:78.69709777832031--TestR2:-4.898791790008545\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2705--TrainLoss:71.40509796142578--TrainR2:-3.6910181045532227--TestLoss:78.68241882324219--TestR2:-4.89769172668457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2706--TrainLoss:71.39147186279297--TrainR2:-3.6901230812072754--TestLoss:78.66773986816406--TestR2:-4.8965911865234375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2707--TrainLoss:71.37786865234375--TrainR2:-3.6892290115356445--TestLoss:78.65306091308594--TestR2:-4.895491123199463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2708--TrainLoss:71.3642807006836--TrainR2:-3.6883363723754883--TestLoss:78.63838958740234--TestR2:-4.8943915367126465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2709--TrainLoss:71.35063934326172--TrainR2:-3.6874399185180664--TestLoss:78.62371063232422--TestR2:-4.893291473388672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2710--TrainLoss:71.33707427978516--TrainR2:-3.686549186706543--TestLoss:78.60904693603516--TestR2:-4.892192363739014\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2711--TrainLoss:71.32345581054688--TrainR2:-3.685654640197754--TestLoss:78.5943832397461--TestR2:-4.891092777252197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2712--TrainLoss:71.3098373413086--TrainR2:-3.6847596168518066--TestLoss:78.5797119140625--TestR2:-4.889993667602539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2713--TrainLoss:71.29627227783203--TrainR2:-3.683868408203125--TestLoss:78.5650634765625--TestR2:-4.888895511627197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2714--TrainLoss:71.28266143798828--TrainR2:-3.682974338531494--TestLoss:78.55039978027344--TestR2:-4.887796401977539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2715--TrainLoss:71.26909637451172--TrainR2:-3.6820831298828125--TestLoss:78.53573608398438--TestR2:-4.886697292327881\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2716--TrainLoss:71.25547790527344--TrainR2:-3.6811885833740234--TestLoss:78.52108764648438--TestR2:-4.885599136352539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2717--TrainLoss:71.24189758300781--TrainR2:-3.6802964210510254--TestLoss:78.50643157958984--TestR2:-4.884500980377197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2718--TrainLoss:71.22833251953125--TrainR2:-3.6794052124023438--TestLoss:78.49177551269531--TestR2:-4.883401870727539\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2719--TrainLoss:71.21471405029297--TrainR2:-3.6785106658935547--TestLoss:78.47711944580078--TestR2:-4.882303714752197\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2720--TrainLoss:71.20115661621094--TrainR2:-3.6776199340820312--TestLoss:78.46249389648438--TestR2:-4.881207466125488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2721--TrainLoss:71.18756866455078--TrainR2:-3.676727294921875--TestLoss:78.44782257080078--TestR2:-4.880107402801514\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2722--TrainLoss:71.17399597167969--TrainR2:-3.675835609436035--TestLoss:78.43318939208984--TestR2:-4.8790106773376465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2723--TrainLoss:71.16040802001953--TrainR2:-3.674942970275879--TestLoss:78.4185562133789--TestR2:-4.877913951873779\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2724--TrainLoss:71.14682006835938--TrainR2:-3.6740503311157227--TestLoss:78.40390014648438--TestR2:-4.876815319061279\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2725--TrainLoss:71.13329315185547--TrainR2:-3.673161506652832--TestLoss:78.38926696777344--TestR2:-4.875718593597412\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2726--TrainLoss:71.11969757080078--TrainR2:-3.6722683906555176--TestLoss:78.3746337890625--TestR2:-4.874621868133545\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2727--TrainLoss:71.10614776611328--TrainR2:-3.6713781356811523--TestLoss:78.35999298095703--TestR2:-4.873524188995361\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2728--TrainLoss:71.09256744384766--TrainR2:-3.6704859733581543--TestLoss:78.34536743164062--TestR2:-4.872427940368652\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2729--TrainLoss:71.0790023803711--TrainR2:-3.669595241546631--TestLoss:78.33074188232422--TestR2:-4.871331691741943\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2730--TrainLoss:71.06542205810547--TrainR2:-3.6687026023864746--TestLoss:78.31610870361328--TestR2:-4.870234489440918\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2731--TrainLoss:71.0519027709961--TrainR2:-3.6678147315979004--TestLoss:78.30149841308594--TestR2:-4.869139194488525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2732--TrainLoss:71.038330078125--TrainR2:-3.6669225692749023--TestLoss:78.28687286376953--TestR2:-4.868043422698975\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2733--TrainLoss:71.02476501464844--TrainR2:-3.666031837463379--TestLoss:78.27223205566406--TestR2:-4.866946220397949\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2734--TrainLoss:71.01122283935547--TrainR2:-3.665142059326172--TestLoss:78.25762176513672--TestR2:-4.865850925445557\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2735--TrainLoss:70.99767303466797--TrainR2:-3.6642518043518066--TestLoss:78.24301147460938--TestR2:-4.864756107330322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2736--TrainLoss:70.98412322998047--TrainR2:-3.6633620262145996--TestLoss:78.2283706665039--TestR2:-4.863658428192139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2737--TrainLoss:70.97051239013672--TrainR2:-3.6624674797058105--TestLoss:78.2137680053711--TestR2:-4.862563610076904\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2738--TrainLoss:70.95701599121094--TrainR2:-3.661581039428711--TestLoss:78.19915771484375--TestR2:-4.861468315124512\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2739--TrainLoss:70.94346618652344--TrainR2:-3.6606907844543457--TestLoss:78.18455505371094--TestR2:-4.860374450683594\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2740--TrainLoss:70.92993927001953--TrainR2:-3.659801959991455--TestLoss:78.16996002197266--TestR2:-4.859280109405518\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2741--TrainLoss:70.91638946533203--TrainR2:-3.65891170501709--TestLoss:78.15534973144531--TestR2:-4.858184814453125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2742--TrainLoss:70.90288543701172--TrainR2:-3.658024787902832--TestLoss:78.1407470703125--TestR2:-4.857089996337891\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2743--TrainLoss:70.88933563232422--TrainR2:-3.657134532928467--TestLoss:78.12616729736328--TestR2:-4.855997562408447\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2744--TrainLoss:70.87586212158203--TrainR2:-3.656249523162842--TestLoss:78.11154174804688--TestR2:-4.854901313781738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2745--TrainLoss:70.86226654052734--TrainR2:-3.6553564071655273--TestLoss:78.0969467163086--TestR2:-4.85380744934082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2746--TrainLoss:70.8487548828125--TrainR2:-3.654468536376953--TestLoss:78.08235931396484--TestR2:-4.852713584899902\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2747--TrainLoss:70.83521270751953--TrainR2:-3.653578758239746--TestLoss:78.0677490234375--TestR2:-4.851619243621826\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2748--TrainLoss:70.82170104980469--TrainR2:-3.65269136428833--TestLoss:78.05316162109375--TestR2:-4.850525379180908\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2749--TrainLoss:70.80817413330078--TrainR2:-3.6518025398254395--TestLoss:78.03857421875--TestR2:-4.849431991577148\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2750--TrainLoss:70.7946548461914--TrainR2:-3.650914192199707--TestLoss:78.02397918701172--TestR2:-4.8483381271362305\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2751--TrainLoss:70.78111267089844--TrainR2:-3.650024890899658--TestLoss:78.0093994140625--TestR2:-4.847244739532471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2752--TrainLoss:70.76762390136719--TrainR2:-3.6491384506225586--TestLoss:77.99481201171875--TestR2:-4.846151828765869\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2753--TrainLoss:70.75408172607422--TrainR2:-3.6482491493225098--TestLoss:77.98023223876953--TestR2:-4.845058917999268\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2754--TrainLoss:70.74058532714844--TrainR2:-3.647362232208252--TestLoss:77.96565246582031--TestR2:-4.843966007232666\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2755--TrainLoss:70.72706604003906--TrainR2:-3.6464743614196777--TestLoss:77.9510726928711--TestR2:-4.8428730964660645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2756--TrainLoss:70.71356964111328--TrainR2:-3.64558744430542--TestLoss:77.9365005493164--TestR2:-4.841780662536621\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2757--TrainLoss:70.7000503540039--TrainR2:-3.6446990966796875--TestLoss:77.92192077636719--TestR2:-4.840688228607178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2758--TrainLoss:70.68659210205078--TrainR2:-3.643815040588379--TestLoss:77.90735626220703--TestR2:-4.839596748352051\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2759--TrainLoss:70.67304992675781--TrainR2:-3.642925262451172--TestLoss:77.89278411865234--TestR2:-4.838504314422607\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2760--TrainLoss:70.65955352783203--TrainR2:-3.6420388221740723--TestLoss:77.87820434570312--TestR2:-4.837411403656006\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2761--TrainLoss:70.6460189819336--TrainR2:-3.6411495208740234--TestLoss:77.86365509033203--TestR2:-4.836320877075195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2762--TrainLoss:70.63255310058594--TrainR2:-3.6402649879455566--TestLoss:77.84908294677734--TestR2:-4.835228443145752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2763--TrainLoss:70.6190414428711--TrainR2:-3.6393771171569824--TestLoss:77.83453369140625--TestR2:-4.834137916564941\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2764--TrainLoss:70.60554504394531--TrainR2:-3.638490676879883--TestLoss:77.81998443603516--TestR2:-4.833047389984131\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2765--TrainLoss:70.59203338623047--TrainR2:-3.6376028060913086--TestLoss:77.805419921875--TestR2:-4.831955909729004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2766--TrainLoss:70.57857513427734--TrainR2:-3.636719226837158--TestLoss:77.79086303710938--TestR2:-4.830864906311035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2767--TrainLoss:70.5650634765625--TrainR2:-3.635831356048584--TestLoss:77.77631378173828--TestR2:-4.829773902893066\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2768--TrainLoss:70.55158996582031--TrainR2:-3.634946346282959--TestLoss:77.76175689697266--TestR2:-4.828682899475098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2769--TrainLoss:70.5380859375--TrainR2:-3.634058952331543--TestLoss:77.74720764160156--TestR2:-4.827592372894287\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2770--TrainLoss:70.52462768554688--TrainR2:-3.6331748962402344--TestLoss:77.732666015625--TestR2:-4.826502323150635\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2771--TrainLoss:70.51113891601562--TrainR2:-3.6322884559631348--TestLoss:77.71810150146484--TestR2:-4.825410842895508\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2772--TrainLoss:70.49768829345703--TrainR2:-3.6314048767089844--TestLoss:77.70358276367188--TestR2:-4.82432222366333\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2773--TrainLoss:70.48419952392578--TrainR2:-3.6305184364318848--TestLoss:77.68902587890625--TestR2:-4.8232316970825195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2774--TrainLoss:70.47071838378906--TrainR2:-3.6296329498291016--TestLoss:77.67448425292969--TestR2:-4.822141647338867\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2775--TrainLoss:70.45723724365234--TrainR2:-3.6287474632263184--TestLoss:77.65996551513672--TestR2:-4.8210530281066895\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2776--TrainLoss:70.44377899169922--TrainR2:-3.6278634071350098--TestLoss:77.64543151855469--TestR2:-4.8199639320373535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2777--TrainLoss:70.43032836914062--TrainR2:-3.626979351043701--TestLoss:77.63089752197266--TestR2:-4.818874359130859\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2778--TrainLoss:70.41683959960938--TrainR2:-3.6260933876037598--TestLoss:77.61637115478516--TestR2:-4.817785263061523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2779--TrainLoss:70.40342712402344--TrainR2:-3.6252126693725586--TestLoss:77.6018295288086--TestR2:-4.816695690155029\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2780--TrainLoss:70.38990020751953--TrainR2:-3.624323844909668--TestLoss:77.58731842041016--TestR2:-4.81560754776001\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2781--TrainLoss:70.37647247314453--TrainR2:-3.623441696166992--TestLoss:77.57279205322266--TestR2:-4.814518928527832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2782--TrainLoss:70.36299133300781--TrainR2:-3.622556209564209--TestLoss:77.55826568603516--TestR2:-4.813430309295654\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2783--TrainLoss:70.34954833984375--TrainR2:-3.621673107147217--TestLoss:77.54374694824219--TestR2:-4.812341690063477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2784--TrainLoss:70.33609008789062--TrainR2:-3.62078857421875--TestLoss:77.52921295166016--TestR2:-4.811252593994141\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2785--TrainLoss:70.32264709472656--TrainR2:-3.619905471801758--TestLoss:77.51470184326172--TestR2:-4.810164451599121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2786--TrainLoss:70.30919647216797--TrainR2:-3.619021415710449--TestLoss:77.50019073486328--TestR2:-4.80907678604126\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2787--TrainLoss:70.29574584960938--TrainR2:-3.618138313293457--TestLoss:77.48567962646484--TestR2:-4.807989120483398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2788--TrainLoss:70.2823257446289--TrainR2:-3.6172566413879395--TestLoss:77.4711685180664--TestR2:-4.806901454925537\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2789--TrainLoss:70.26885223388672--TrainR2:-3.6163716316223145--TestLoss:77.45665740966797--TestR2:-4.805813789367676\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2790--TrainLoss:70.25541687011719--TrainR2:-3.6154890060424805--TestLoss:77.44214630126953--TestR2:-4.8047261238098145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2791--TrainLoss:70.24197387695312--TrainR2:-3.61460542678833--TestLoss:77.4276351928711--TestR2:-4.803638458251953\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2792--TrainLoss:70.2285385131836--TrainR2:-3.613722801208496--TestLoss:77.41313171386719--TestR2:-4.80255126953125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2793--TrainLoss:70.21508026123047--TrainR2:-3.6128387451171875--TestLoss:77.39864349365234--TestR2:-4.8014655113220215\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2794--TrainLoss:70.2016372680664--TrainR2:-3.6119556427001953--TestLoss:77.38414764404297--TestR2:-4.800378799438477\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2795--TrainLoss:70.18826293945312--TrainR2:-3.611076831817627--TestLoss:77.36964416503906--TestR2:-4.799292087554932\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2796--TrainLoss:70.1748046875--TrainR2:-3.6101927757263184--TestLoss:77.35516357421875--TestR2:-4.798206329345703\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2797--TrainLoss:70.16138458251953--TrainR2:-3.609311580657959--TestLoss:77.34066009521484--TestR2:-4.797119140625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2798--TrainLoss:70.14794158935547--TrainR2:-3.6084280014038086--TestLoss:77.32616424560547--TestR2:-4.796032905578613\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2799--TrainLoss:70.13451385498047--TrainR2:-3.607545852661133--TestLoss:77.31168365478516--TestR2:-4.794947147369385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2800--TrainLoss:70.1211166381836--TrainR2:-3.606666088104248--TestLoss:77.29718780517578--TestR2:-4.793860912322998\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2801--TrainLoss:70.1076889038086--TrainR2:-3.605783462524414--TestLoss:77.28269958496094--TestR2:-4.7927751541137695\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2802--TrainLoss:70.0942611694336--TrainR2:-3.6049013137817383--TestLoss:77.2682113647461--TestR2:-4.791688919067383\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2803--TrainLoss:70.0808334350586--TrainR2:-3.6040191650390625--TestLoss:77.25373840332031--TestR2:-4.790604114532471\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2804--TrainLoss:70.06742858886719--TrainR2:-3.6031384468078613--TestLoss:77.23927307128906--TestR2:-4.789519786834717\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2805--TrainLoss:70.05403137207031--TrainR2:-3.6022582054138184--TestLoss:77.22479248046875--TestR2:-4.7884345054626465\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2806--TrainLoss:70.04060363769531--TrainR2:-3.6013760566711426--TestLoss:77.2103271484375--TestR2:-4.787350177764893\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2807--TrainLoss:70.02722930908203--TrainR2:-3.6004977226257324--TestLoss:77.19583129882812--TestR2:-4.786263942718506\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2808--TrainLoss:70.0137939453125--TrainR2:-3.5996150970458984--TestLoss:77.1813735961914--TestR2:-4.785179615020752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2809--TrainLoss:70.00040435791016--TrainR2:-3.5987353324890137--TestLoss:77.16690063476562--TestR2:-4.78409481048584\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2810--TrainLoss:69.98701477050781--TrainR2:-3.597855567932129--TestLoss:77.15243530273438--TestR2:-4.783010959625244\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2811--TrainLoss:69.97359466552734--TrainR2:-3.5969743728637695--TestLoss:77.13797760009766--TestR2:-4.781927108764648\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2812--TrainLoss:69.9601821899414--TrainR2:-3.59609317779541--TestLoss:77.1235122680664--TestR2:-4.7808427810668945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2813--TrainLoss:69.94676971435547--TrainR2:-3.5952115058898926--TestLoss:77.10904693603516--TestR2:-4.779758930206299\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2814--TrainLoss:69.93341064453125--TrainR2:-3.594334125518799--TestLoss:77.0946044921875--TestR2:-4.7786760330200195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2815--TrainLoss:69.92000579833984--TrainR2:-3.5934534072875977--TestLoss:77.08013153076172--TestR2:-4.777591228485107\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2816--TrainLoss:69.90660095214844--TrainR2:-3.5925731658935547--TestLoss:77.06568908691406--TestR2:-4.7765092849731445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2817--TrainLoss:69.89322662353516--TrainR2:-3.5916943550109863--TestLoss:77.05121612548828--TestR2:-4.775424003601074\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2818--TrainLoss:69.87984466552734--TrainR2:-3.5908150672912598--TestLoss:77.03677368164062--TestR2:-4.774341106414795\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2819--TrainLoss:69.86646270751953--TrainR2:-3.589935779571533--TestLoss:77.0223388671875--TestR2:-4.773259162902832\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2820--TrainLoss:69.85310363769531--TrainR2:-3.5890583992004395--TestLoss:77.00788879394531--TestR2:-4.772176265716553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2821--TrainLoss:69.8396987915039--TrainR2:-3.5881776809692383--TestLoss:76.99342346191406--TestR2:-4.771092414855957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2822--TrainLoss:69.8263168334961--TrainR2:-3.58729887008667--TestLoss:76.97898864746094--TestR2:-4.770010471343994\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2823--TrainLoss:69.81292724609375--TrainR2:-3.586419105529785--TestLoss:76.96454620361328--TestR2:-4.768927574157715\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2824--TrainLoss:69.79955291748047--TrainR2:-3.585540294647217--TestLoss:76.95011138916016--TestR2:-4.767845630645752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2825--TrainLoss:69.78619384765625--TrainR2:-3.584662437438965--TestLoss:76.93568420410156--TestR2:-4.766764163970947\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2826--TrainLoss:69.7728271484375--TrainR2:-3.583784580230713--TestLoss:76.9212417602539--TestR2:-4.765681743621826\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2827--TrainLoss:69.75940704345703--TrainR2:-3.5829029083251953--TestLoss:76.90680694580078--TestR2:-4.764599800109863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2828--TrainLoss:69.7460708618164--TrainR2:-3.582026481628418--TestLoss:76.89237976074219--TestR2:-4.763518333435059\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2829--TrainLoss:69.73273468017578--TrainR2:-3.581150531768799--TestLoss:76.87794494628906--TestR2:-4.762436389923096\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2830--TrainLoss:69.71935272216797--TrainR2:-3.5802712440490723--TestLoss:76.86351776123047--TestR2:-4.761354923248291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2831--TrainLoss:69.70599365234375--TrainR2:-3.5793943405151367--TestLoss:76.84910583496094--TestR2:-4.760274410247803\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2832--TrainLoss:69.69261932373047--TrainR2:-3.57851505279541--TestLoss:76.83467102050781--TestR2:-4.759192943572998\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2833--TrainLoss:69.67926025390625--TrainR2:-3.5776376724243164--TestLoss:76.82025909423828--TestR2:-4.758112907409668\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2834--TrainLoss:69.66595458984375--TrainR2:-3.57676362991333--TestLoss:76.80583953857422--TestR2:-4.757031440734863\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2835--TrainLoss:69.6525650024414--TrainR2:-3.5758838653564453--TestLoss:76.79141998291016--TestR2:-4.755950927734375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2836--TrainLoss:69.63920593261719--TrainR2:-3.5750064849853516--TestLoss:76.77701568603516--TestR2:-4.754870891571045\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2837--TrainLoss:69.6258773803711--TrainR2:-3.5741305351257324--TestLoss:76.7625961303711--TestR2:-4.753790378570557\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2838--TrainLoss:69.6125259399414--TrainR2:-3.573253631591797--TestLoss:76.74819946289062--TestR2:-4.752710819244385\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2839--TrainLoss:69.59918975830078--TrainR2:-3.5723772048950195--TestLoss:76.73377227783203--TestR2:-4.751629829406738\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2840--TrainLoss:69.58584594726562--TrainR2:-3.571500778198242--TestLoss:76.71936798095703--TestR2:-4.750550270080566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2841--TrainLoss:69.57247161865234--TrainR2:-3.570621967315674--TestLoss:76.70497131347656--TestR2:-4.749471187591553\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2842--TrainLoss:69.55917358398438--TrainR2:-3.5697484016418457--TestLoss:76.69056701660156--TestR2:-4.748391151428223\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2843--TrainLoss:69.54582214355469--TrainR2:-3.568871021270752--TestLoss:76.6761703491211--TestR2:-4.747312068939209\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2844--TrainLoss:69.53244018554688--TrainR2:-3.5679922103881836--TestLoss:76.66178131103516--TestR2:-4.7462334632873535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2845--TrainLoss:69.5191421508789--TrainR2:-3.5671186447143555--TestLoss:76.64736938476562--TestR2:-4.745152950286865\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2846--TrainLoss:69.50580596923828--TrainR2:-3.5662426948547363--TestLoss:76.63296508789062--TestR2:-4.744073390960693\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2847--TrainLoss:69.49250030517578--TrainR2:-3.565368175506592--TestLoss:76.61858367919922--TestR2:-4.742995738983154\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2848--TrainLoss:69.47915649414062--TrainR2:-3.5644912719726562--TestLoss:76.60419464111328--TestR2:-4.741917133331299\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2849--TrainLoss:69.4658432006836--TrainR2:-3.5636167526245117--TestLoss:76.58981323242188--TestR2:-4.740839004516602\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2850--TrainLoss:69.45252227783203--TrainR2:-3.562741756439209--TestLoss:76.57540893554688--TestR2:-4.73975944519043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2851--TrainLoss:69.43919372558594--TrainR2:-3.561866283416748--TestLoss:76.56103515625--TestR2:-4.738682270050049\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2852--TrainLoss:69.4258804321289--TrainR2:-3.5609917640686035--TestLoss:76.54666137695312--TestR2:-4.73760461807251\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2853--TrainLoss:69.41255950927734--TrainR2:-3.5601162910461426--TestLoss:76.53225708007812--TestR2:-4.736525058746338\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2854--TrainLoss:69.3991928100586--TrainR2:-3.5592379570007324--TestLoss:76.51789855957031--TestR2:-4.735448837280273\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2855--TrainLoss:69.38591766357422--TrainR2:-3.558366298675537--TestLoss:76.50350189208984--TestR2:-4.73436975479126\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2856--TrainLoss:69.37261962890625--TrainR2:-3.557492733001709--TestLoss:76.4891357421875--TestR2:-4.733293056488037\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2857--TrainLoss:69.35929107666016--TrainR2:-3.55661678314209--TestLoss:76.47476959228516--TestR2:-4.7322163581848145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2858--TrainLoss:69.34598541259766--TrainR2:-3.5557427406311035--TestLoss:76.46039581298828--TestR2:-4.731138706207275\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2859--TrainLoss:69.33271026611328--TrainR2:-3.55487060546875--TestLoss:76.44603729248047--TestR2:-4.730062484741211\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2860--TrainLoss:69.31940460205078--TrainR2:-3.5539965629577637--TestLoss:76.43165588378906--TestR2:-4.728984832763672\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2861--TrainLoss:69.30604553222656--TrainR2:-3.55311918258667--TestLoss:76.41729736328125--TestR2:-4.727908134460449\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2862--TrainLoss:69.29279327392578--TrainR2:-3.552248477935791--TestLoss:76.4029312133789--TestR2:-4.726831436157227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2863--TrainLoss:69.27951049804688--TrainR2:-3.551375389099121--TestLoss:76.3885726928711--TestR2:-4.725755214691162\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2864--TrainLoss:69.26618194580078--TrainR2:-3.55049991607666--TestLoss:76.37421417236328--TestR2:-4.724678993225098\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2865--TrainLoss:69.25288391113281--TrainR2:-3.549626350402832--TestLoss:76.3598403930664--TestR2:-4.723601341247559\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2866--TrainLoss:69.23960876464844--TrainR2:-3.5487542152404785--TestLoss:76.34551239013672--TestR2:-4.722527027130127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2867--TrainLoss:69.22632598876953--TrainR2:-3.547881603240967--TestLoss:76.33114624023438--TestR2:-4.7214508056640625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2868--TrainLoss:69.21299743652344--TrainR2:-3.547006130218506--TestLoss:76.3167953491211--TestR2:-4.720375061035156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2869--TrainLoss:69.19972229003906--TrainR2:-3.5461339950561523--TestLoss:76.30245971679688--TestR2:-4.719300270080566\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2870--TrainLoss:69.18646240234375--TrainR2:-3.5452632904052734--TestLoss:76.2881088256836--TestR2:-4.718225002288818\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2871--TrainLoss:69.17314910888672--TrainR2:-3.5443882942199707--TestLoss:76.27376556396484--TestR2:-4.717149257659912\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2872--TrainLoss:69.15988159179688--TrainR2:-3.5435166358947754--TestLoss:76.25942993164062--TestR2:-4.7160749435424805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2873--TrainLoss:69.14663696289062--TrainR2:-3.5426464080810547--TestLoss:76.24507904052734--TestR2:-4.714999675750732\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2874--TrainLoss:69.13336181640625--TrainR2:-3.541774272918701--TestLoss:76.2307357788086--TestR2:-4.713924407958984\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2875--TrainLoss:69.12005615234375--TrainR2:-3.540900230407715--TestLoss:76.2164077758789--TestR2:-4.712850570678711\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2876--TrainLoss:69.10678100585938--TrainR2:-3.540027618408203--TestLoss:76.20207214355469--TestR2:-4.711775779724121\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2877--TrainLoss:69.09355163574219--TrainR2:-3.539158821105957--TestLoss:76.187744140625--TestR2:-4.710701942443848\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2878--TrainLoss:69.08027648925781--TrainR2:-3.5382866859436035--TestLoss:76.17341613769531--TestR2:-4.709627628326416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2879--TrainLoss:69.06696319580078--TrainR2:-3.537412166595459--TestLoss:76.15909576416016--TestR2:-4.708554267883301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2880--TrainLoss:69.05374145507812--TrainR2:-3.536543846130371--TestLoss:76.14476013183594--TestR2:-4.707479953765869\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2881--TrainLoss:69.04048919677734--TrainR2:-3.535673141479492--TestLoss:76.13043212890625--TestR2:-4.706406116485596\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2882--TrainLoss:69.0272216796875--TrainR2:-3.534801483154297--TestLoss:76.1161117553711--TestR2:-4.7053327560424805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2883--TrainLoss:69.01394653320312--TrainR2:-3.5339293479919434--TestLoss:76.10179138183594--TestR2:-4.704258918762207\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2884--TrainLoss:69.00067901611328--TrainR2:-3.533057689666748--TestLoss:76.08747863769531--TestR2:-4.703186511993408\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2885--TrainLoss:68.98743438720703--TrainR2:-3.5321874618530273--TestLoss:76.07315826416016--TestR2:-4.702112674713135\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2886--TrainLoss:68.97420501708984--TrainR2:-3.5313186645507812--TestLoss:76.05883026123047--TestR2:-4.7010393142700195\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2887--TrainLoss:68.96092224121094--TrainR2:-3.5304460525512695--TestLoss:76.04452514648438--TestR2:-4.699966907501221\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2888--TrainLoss:68.94764709472656--TrainR2:-3.529573917388916--TestLoss:76.03022003173828--TestR2:-4.698894500732422\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2889--TrainLoss:68.93441009521484--TrainR2:-3.5287041664123535--TestLoss:76.01592254638672--TestR2:-4.697822570800781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2890--TrainLoss:68.92120361328125--TrainR2:-3.527836322784424--TestLoss:76.00160217285156--TestR2:-4.696749687194824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2891--TrainLoss:68.90797424316406--TrainR2:-3.5269670486450195--TestLoss:75.98729705810547--TestR2:-4.695677280426025\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2892--TrainLoss:68.89470672607422--TrainR2:-3.5260958671569824--TestLoss:75.97296905517578--TestR2:-4.694603443145752\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2893--TrainLoss:68.88148498535156--TrainR2:-3.5252270698547363--TestLoss:75.95868682861328--TestR2:-4.693532466888428\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2894--TrainLoss:68.86824798583984--TrainR2:-3.524357318878174--TestLoss:75.94438934326172--TestR2:-4.692461013793945\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2895--TrainLoss:68.85501861572266--TrainR2:-3.5234880447387695--TestLoss:75.93009185791016--TestR2:-4.691389560699463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2896--TrainLoss:68.84176635742188--TrainR2:-3.522617816925049--TestLoss:75.91580963134766--TestR2:-4.690318584442139\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2897--TrainLoss:68.82853698730469--TrainR2:-3.5217490196228027--TestLoss:75.90151977539062--TestR2:-4.6892476081848145\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2898--TrainLoss:68.8153076171875--TrainR2:-3.5208797454833984--TestLoss:75.88722229003906--TestR2:-4.688175678253174\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2899--TrainLoss:68.80209350585938--TrainR2:-3.5200109481811523--TestLoss:75.87293243408203--TestR2:-4.687105178833008\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2900--TrainLoss:68.78886413574219--TrainR2:-3.5191421508789062--TestLoss:75.858642578125--TestR2:-4.686033725738525\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2901--TrainLoss:68.77560424804688--TrainR2:-3.5182714462280273--TestLoss:75.8443603515625--TestR2:-4.684963226318359\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2902--TrainLoss:68.76240539550781--TrainR2:-3.517404079437256--TestLoss:75.83008575439453--TestR2:-4.683893203735352\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2903--TrainLoss:68.74919128417969--TrainR2:-3.516535758972168--TestLoss:75.8157958984375--TestR2:-4.682822227478027\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2904--TrainLoss:68.73599243164062--TrainR2:-3.5156688690185547--TestLoss:75.80152893066406--TestR2:-4.681752681732178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2905--TrainLoss:68.72279357910156--TrainR2:-3.514801502227783--TestLoss:75.78723907470703--TestR2:-4.6806817054748535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2906--TrainLoss:68.70953369140625--TrainR2:-3.5139307975769043--TestLoss:75.77296447753906--TestR2:-4.679611682891846\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2907--TrainLoss:68.69635009765625--TrainR2:-3.513064384460449--TestLoss:75.75870513916016--TestR2:-4.678542613983154\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2908--TrainLoss:68.68313598632812--TrainR2:-3.5121960639953613--TestLoss:75.7444076538086--TestR2:-4.67747163772583\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2909--TrainLoss:68.669921875--TrainR2:-3.5113282203674316--TestLoss:75.73015594482422--TestR2:-4.676403045654297\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2910--TrainLoss:68.65674591064453--TrainR2:-3.510462760925293--TestLoss:75.71587371826172--TestR2:-4.675332546234131\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2911--TrainLoss:68.64350128173828--TrainR2:-3.5095925331115723--TestLoss:75.70162200927734--TestR2:-4.674264430999756\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2912--TrainLoss:68.63029479980469--TrainR2:-3.508725166320801--TestLoss:75.68736267089844--TestR2:-4.6731953620910645\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2913--TrainLoss:68.61711120605469--TrainR2:-3.5078587532043457--TestLoss:75.67308807373047--TestR2:-4.672125339508057\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2914--TrainLoss:68.60393524169922--TrainR2:-3.506993293762207--TestLoss:75.65882873535156--TestR2:-4.671056747436523\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2915--TrainLoss:68.5907211303711--TrainR2:-3.5061254501342773--TestLoss:75.64457702636719--TestR2:-4.66998815536499\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2916--TrainLoss:68.57752990722656--TrainR2:-3.505258560180664--TestLoss:75.63031768798828--TestR2:-4.668919563293457\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2917--TrainLoss:68.56431579589844--TrainR2:-3.5043907165527344--TestLoss:75.6160659790039--TestR2:-4.667851448059082\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2918--TrainLoss:68.55113983154297--TrainR2:-3.5035247802734375--TestLoss:75.6018295288086--TestR2:-4.666784286499023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2919--TrainLoss:68.5379867553711--TrainR2:-3.5026602745056152--TestLoss:75.58758544921875--TestR2:-4.665716648101807\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2920--TrainLoss:68.5247802734375--TrainR2:-3.5017929077148438--TestLoss:75.57332611083984--TestR2:-4.664647579193115\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2921--TrainLoss:68.5116195678711--TrainR2:-3.5009284019470215--TestLoss:75.55907440185547--TestR2:-4.66357946395874\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2922--TrainLoss:68.49839782714844--TrainR2:-3.5000600814819336--TestLoss:75.54485321044922--TestR2:-4.662513732910156\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2923--TrainLoss:68.48521423339844--TrainR2:-3.4991941452026367--TestLoss:75.53060150146484--TestR2:-4.661445140838623\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2924--TrainLoss:68.4720458984375--TrainR2:-3.498328685760498--TestLoss:75.516357421875--TestR2:-4.660377502441406\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2925--TrainLoss:68.45887756347656--TrainR2:-3.4974632263183594--TestLoss:75.50214385986328--TestR2:-4.6593122482299805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2926--TrainLoss:68.44571685791016--TrainR2:-3.496598720550537--TestLoss:75.4878921508789--TestR2:-4.6582441329956055\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2927--TrainLoss:68.43254089355469--TrainR2:-3.4957332611083984--TestLoss:75.47364807128906--TestR2:-4.657176494598389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2928--TrainLoss:68.41931915283203--TrainR2:-3.4948644638061523--TestLoss:75.45941925048828--TestR2:-4.656109809875488\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2929--TrainLoss:68.40618896484375--TrainR2:-3.494002342224121--TestLoss:75.4451904296875--TestR2:-4.655043125152588\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2930--TrainLoss:68.39300537109375--TrainR2:-3.493135929107666--TestLoss:75.43096923828125--TestR2:-4.653977394104004\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2931--TrainLoss:68.37984466552734--TrainR2:-3.4922714233398438--TestLoss:75.41675567626953--TestR2:-4.65291166305542\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2932--TrainLoss:68.36670684814453--TrainR2:-3.491408348083496--TestLoss:75.40253448486328--TestR2:-4.651845932006836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2933--TrainLoss:68.3535385131836--TrainR2:-3.4905428886413574--TestLoss:75.38829803466797--TestR2:-4.650778770446777\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2934--TrainLoss:68.34037780761719--TrainR2:-3.4896788597106934--TestLoss:75.37407684326172--TestR2:-4.649712562561035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2935--TrainLoss:68.32720947265625--TrainR2:-3.488813877105713--TestLoss:75.35987091064453--TestR2:-4.648647785186768\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2936--TrainLoss:68.31405639648438--TrainR2:-3.4879493713378906--TestLoss:75.34564971923828--TestR2:-4.647582054138184\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2937--TrainLoss:68.30091857910156--TrainR2:-3.487086296081543--TestLoss:75.33143615722656--TestR2:-4.646516799926758\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2938--TrainLoss:68.28775024414062--TrainR2:-3.4862213134765625--TestLoss:75.31719970703125--TestR2:-4.645449638366699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2939--TrainLoss:68.27462768554688--TrainR2:-3.485358715057373--TestLoss:75.30300903320312--TestR2:-4.644385814666748\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2940--TrainLoss:68.261474609375--TrainR2:-3.484495162963867--TestLoss:75.28880310058594--TestR2:-4.6433210372924805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2941--TrainLoss:68.2483901977539--TrainR2:-3.483635425567627--TestLoss:75.27460479736328--TestR2:-4.642256736755371\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2942--TrainLoss:68.23518371582031--TrainR2:-3.4827680587768555--TestLoss:75.26041412353516--TestR2:-4.64119291305542\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2943--TrainLoss:68.2220230102539--TrainR2:-3.481903076171875--TestLoss:75.2461929321289--TestR2:-4.640127182006836\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2944--TrainLoss:68.20890808105469--TrainR2:-3.481041431427002--TestLoss:75.23199462890625--TestR2:-4.639063358306885\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2945--TrainLoss:68.19576263427734--TrainR2:-3.480177879333496--TestLoss:75.21781921386719--TestR2:-4.63800048828125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2946--TrainLoss:68.18263244628906--TrainR2:-3.4793152809143066--TestLoss:75.20359802246094--TestR2:-4.636934757232666\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2947--TrainLoss:68.16950225830078--TrainR2:-3.478452682495117--TestLoss:75.18939971923828--TestR2:-4.635869979858398\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2948--TrainLoss:68.1563720703125--TrainR2:-3.4775900840759277--TestLoss:75.17521667480469--TestR2:-4.634807109832764\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2949--TrainLoss:68.14326477050781--TrainR2:-3.476728916168213--TestLoss:75.16102600097656--TestR2:-4.633743762969971\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2950--TrainLoss:68.13009643554688--TrainR2:-3.4758639335632324--TestLoss:75.14684295654297--TestR2:-4.632680416107178\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2951--TrainLoss:68.11697387695312--TrainR2:-3.475001811981201--TestLoss:75.1326675415039--TestR2:-4.631618022918701\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2952--TrainLoss:68.10386657714844--TrainR2:-3.4741406440734863--TestLoss:75.11846160888672--TestR2:-4.630553245544434\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2953--TrainLoss:68.09075927734375--TrainR2:-3.4732794761657715--TestLoss:75.10429382324219--TestR2:-4.629490852355957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2954--TrainLoss:68.07762908935547--TrainR2:-3.472416877746582--TestLoss:75.0901107788086--TestR2:-4.628427982330322\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2955--TrainLoss:68.06451416015625--TrainR2:-3.471555709838867--TestLoss:75.07593536376953--TestR2:-4.627365589141846\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2956--TrainLoss:68.05139923095703--TrainR2:-3.470694065093994--TestLoss:75.061767578125--TestR2:-4.626303672790527\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2957--TrainLoss:68.03828430175781--TrainR2:-3.469832420349121--TestLoss:75.0475845336914--TestR2:-4.625240325927734\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2958--TrainLoss:68.02517700195312--TrainR2:-3.4689712524414062--TestLoss:75.03341674804688--TestR2:-4.624178409576416\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2959--TrainLoss:68.01207733154297--TrainR2:-3.4681105613708496--TestLoss:75.01923370361328--TestR2:-4.623115539550781\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2960--TrainLoss:67.99893188476562--TrainR2:-3.4672470092773438--TestLoss:75.00507354736328--TestR2:-4.622053623199463\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2961--TrainLoss:67.98580169677734--TrainR2:-3.4663844108581543--TestLoss:74.99089813232422--TestR2:-4.620991230010986\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2962--TrainLoss:67.97274017333984--TrainR2:-3.4655261039733887--TestLoss:74.97675323486328--TestR2:-4.619931221008301\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2963--TrainLoss:67.95962524414062--TrainR2:-3.464664936065674--TestLoss:74.96257781982422--TestR2:-4.618868827819824\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2964--TrainLoss:67.94657135009766--TrainR2:-3.4638071060180664--TestLoss:74.94843292236328--TestR2:-4.6178083419799805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2965--TrainLoss:67.9334487915039--TrainR2:-3.462944984436035--TestLoss:74.93427276611328--TestR2:-4.61674690246582\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2966--TrainLoss:67.92035675048828--TrainR2:-3.4620847702026367--TestLoss:74.92011260986328--TestR2:-4.615685939788818\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2967--TrainLoss:67.90727996826172--TrainR2:-3.4612255096435547--TestLoss:74.90596008300781--TestR2:-4.614624977111816\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2968--TrainLoss:67.89415740966797--TrainR2:-3.4603633880615234--TestLoss:74.89181518554688--TestR2:-4.613564491271973\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2969--TrainLoss:67.88106536865234--TrainR2:-3.459503650665283--TestLoss:74.87764739990234--TestR2:-4.612502574920654\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2970--TrainLoss:67.86798095703125--TrainR2:-3.458643913269043--TestLoss:74.86351013183594--TestR2:-4.611443042755127\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2971--TrainLoss:67.85491943359375--TrainR2:-3.4577860832214355--TestLoss:74.84935760498047--TestR2:-4.610382556915283\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2972--TrainLoss:67.841796875--TrainR2:-3.4569239616394043--TestLoss:74.83522033691406--TestR2:-4.609323024749756\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2973--TrainLoss:67.82872009277344--TrainR2:-3.4560647010803223--TestLoss:74.8210678100586--TestR2:-4.608262062072754\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2974--TrainLoss:67.81560516357422--TrainR2:-3.4552035331726074--TestLoss:74.80693817138672--TestR2:-4.607202529907227\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2975--TrainLoss:67.80257415771484--TrainR2:-3.4543471336364746--TestLoss:74.79280090332031--TestR2:-4.606142997741699\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2976--TrainLoss:67.78950500488281--TrainR2:-3.453488349914551--TestLoss:74.77865600585938--TestR2:-4.605082988739014\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2977--TrainLoss:67.77644348144531--TrainR2:-3.4526305198669434--TestLoss:74.7645263671875--TestR2:-4.604023456573486\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2978--TrainLoss:67.76333618164062--TrainR2:-3.4517698287963867--TestLoss:74.75040435791016--TestR2:-4.602964878082275\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2979--TrainLoss:67.75029754638672--TrainR2:-3.4509129524230957--TestLoss:74.73625183105469--TestR2:-4.601904392242432\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2980--TrainLoss:67.7372055053711--TrainR2:-3.4500527381896973--TestLoss:74.72213745117188--TestR2:-4.600846290588379\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2981--TrainLoss:67.72415924072266--TrainR2:-3.449195384979248--TestLoss:74.70799255371094--TestR2:-4.599785804748535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2982--TrainLoss:67.71107482910156--TrainR2:-3.448336124420166--TestLoss:74.69389343261719--TestR2:-4.598729133605957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2983--TrainLoss:67.69802856445312--TrainR2:-3.447479248046875--TestLoss:74.67977142333984--TestR2:-4.597670555114746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2984--TrainLoss:67.6849594116211--TrainR2:-3.446620464324951--TestLoss:74.6656494140625--TestR2:-4.596611976623535\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2985--TrainLoss:67.67192077636719--TrainR2:-3.44576358795166--TestLoss:74.65153503417969--TestR2:-4.595554351806641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2986--TrainLoss:67.65884399414062--TrainR2:-3.4449048042297363--TestLoss:74.63741302490234--TestR2:-4.59449577331543\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2987--TrainLoss:67.64583587646484--TrainR2:-3.444049835205078--TestLoss:74.62329864501953--TestR2:-4.593437671661377\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2988--TrainLoss:67.63275146484375--TrainR2:-3.443190574645996--TestLoss:74.60919189453125--TestR2:-4.592380523681641\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2989--TrainLoss:67.6197280883789--TrainR2:-3.4423346519470215--TestLoss:74.5950698852539--TestR2:-4.59132194519043\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2990--TrainLoss:67.60664367675781--TrainR2:-3.4414753913879395--TestLoss:74.58097076416016--TestR2:-4.590264797210693\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2991--TrainLoss:67.59357452392578--TrainR2:-3.440617084503174--TestLoss:74.56685638427734--TestR2:-4.589207172393799\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2992--TrainLoss:67.58052062988281--TrainR2:-3.439758777618408--TestLoss:74.55274963378906--TestR2:-4.5881500244140625\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2993--TrainLoss:67.5674819946289--TrainR2:-3.4389028549194336--TestLoss:74.53865051269531--TestR2:-4.587092876434326\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2994--TrainLoss:67.55443572998047--TrainR2:-3.4380455017089844--TestLoss:74.5245590209961--TestR2:-4.586036682128906\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2995--TrainLoss:67.54141998291016--TrainR2:-3.437190055847168--TestLoss:74.51045227050781--TestR2:-4.58497953414917\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2996--TrainLoss:67.52837371826172--TrainR2:-3.436333179473877--TestLoss:74.4963607788086--TestR2:-4.58392333984375\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2997--TrainLoss:67.51533508300781--TrainR2:-3.435476779937744--TestLoss:74.48226165771484--TestR2:-4.582866668701172\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2998--TrainLoss:67.5022964477539--TrainR2:-3.4346203804016113--TestLoss:74.46817779541016--TestR2:-4.58181095123291\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:2999--TrainLoss:67.4892807006836--TrainR2:-3.433765411376953--TestLoss:74.45407104492188--TestR2:-4.580753326416016\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3000--TrainLoss:67.47621154785156--TrainR2:-3.4329066276550293--TestLoss:74.43997955322266--TestR2:-4.579697132110596\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3001--TrainLoss:67.46321868896484--TrainR2:-3.4320526123046875--TestLoss:74.4259033203125--TestR2:-4.578641891479492\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3002--TrainLoss:67.4501953125--TrainR2:-3.431197166442871--TestLoss:74.41182708740234--TestR2:-4.577586650848389\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3003--TrainLoss:67.43716430664062--TrainR2:-3.4303412437438965--TestLoss:74.3977279663086--TestR2:-4.5765299797058105\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3004--TrainLoss:67.42414855957031--TrainR2:-3.4294862747192383--TestLoss:74.3836669921875--TestR2:-4.575475692749023\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3005--TrainLoss:67.4111328125--TrainR2:-3.428630828857422--TestLoss:74.36957550048828--TestR2:-4.574419975280762\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3006--TrainLoss:67.39807891845703--TrainR2:-3.4277734756469727--TestLoss:74.3554916381836--TestR2:-4.5733642578125\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3007--TrainLoss:67.38509368896484--TrainR2:-3.4269204139709473--TestLoss:74.34142303466797--TestR2:-4.572309494018555\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3008--TrainLoss:67.3720703125--TrainR2:-3.426064968109131--TestLoss:74.32734680175781--TestR2:-4.571254730224609\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3009--TrainLoss:67.35905456542969--TrainR2:-3.4252099990844727--TestLoss:74.31328582763672--TestR2:-4.5702009201049805\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3010--TrainLoss:67.3460693359375--TrainR2:-3.424356460571289--TestLoss:74.2992172241211--TestR2:-4.569146156311035\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3011--TrainLoss:67.33305358886719--TrainR2:-3.423501491546631--TestLoss:74.28514099121094--TestR2:-4.56809139251709\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3012--TrainLoss:67.3200454711914--TrainR2:-3.422646999359131--TestLoss:74.27108001708984--TestR2:-4.5670366287231445\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3013--TrainLoss:67.30701446533203--TrainR2:-3.4217910766601562--TestLoss:74.25701904296875--TestR2:-4.565982818603516\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3014--TrainLoss:67.29402923583984--TrainR2:-3.420938014984131--TestLoss:74.2429428100586--TestR2:-4.56492805480957\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3015--TrainLoss:67.2810287475586--TrainR2:-3.420083522796631--TestLoss:74.22888946533203--TestR2:-4.5638747215271\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3016--TrainLoss:67.26802825927734--TrainR2:-3.4192299842834473--TestLoss:74.21484375--TestR2:-4.562821865081787\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3017--TrainLoss:67.2550277709961--TrainR2:-3.4183759689331055--TestLoss:74.2007827758789--TestR2:-4.561768054962158\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3018--TrainLoss:67.24203491210938--TrainR2:-3.4175219535827637--TestLoss:74.18672180175781--TestR2:-4.560714244842529\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3019--TrainLoss:67.22900390625--TrainR2:-3.416666030883789--TestLoss:74.17268371582031--TestR2:-4.559661388397217\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3020--TrainLoss:67.21607208251953--TrainR2:-3.415816307067871--TestLoss:74.15863037109375--TestR2:-4.558608055114746\n",
      "Evaluating Network.....\n",
      "==================================================\n",
      "Epoch:3021--TrainLoss:67.20306396484375--TrainR2:-3.414961814880371--TestLoss:74.14458465576172--TestR2:-4.557555198669434\n",
      "Evaluating Network....."
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 10_000\n",
    "\n",
    "# stuff to store\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "train_r2s = np.zeros(epochs)\n",
    "test_r2s = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #move data to gpu\n",
    "    inputs, targets = trainx.to(device), trainy.to(device)\n",
    "\n",
    "    # zero the grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forwards pass and get losses\n",
    "    logits = model(inputs)\n",
    "    train_loss = criterion(logits, targets)\n",
    "    train_r2 = coeff_determination(targets, logits)\n",
    "\n",
    "    # step thru optimizer\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Now validate\n",
    "    print(\"Evaluating Network.....\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        #move data to gpu\n",
    "        inputs, targets = testx.to(device), testy.to(device)\n",
    "\n",
    "        # forwards pass and get losses\n",
    "        logits = model(inputs)\n",
    "        test_loss = criterion(logits, targets)\n",
    "        test_r2 = coeff_determination(targets, logits)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    print(\"=\" * 50)\n",
    "    # now getting out to the next epoch - all batches done - store epoch\n",
    "    # loss and accus\n",
    "    train_losses[epoch] = train_loss.item()\n",
    "    test_losses[epoch] = test_loss.item()\n",
    "    train_r2s[epoch] = train_r2\n",
    "    test_r2s[epoch] = test_r2\n",
    "\n",
    "\n",
    "    print(f\"Epoch:{epoch}--TrainLoss:{train_losses[epoch]}--TrainR2\"\n",
    "          f\":{train_r2s[epoch]}--TestLoss:{test_losses[epoch]}--TestR2\"\n",
    "          f\":{test_r2s[epoch]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# plot the performance\n",
    "epoch_num = range(1,epochs+1)\n",
    "loss = train_losses\n",
    "train_r2 = train_r2s\n",
    "val_loss = test_losses\n",
    "test_r2 = test_r2s\n",
    "plot_df = pd.DataFrame(data=np.c_[epoch_num,loss,train_r2,val_loss,\n",
    "                                  test_r2],\n",
    "                       columns=['epochs','loss', 'train_r2', 'val_loss',\n",
    "                                'test_r2'])\n",
    "\n",
    "# do the actual plots\n",
    "sns.set(font_scale=1)\n",
    "f, ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "# sns.lineplot(data=plot_df, x='epochs', y='loss', ax=ax, label='train loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='train_r2', ax=ax, label='train_r2',\n",
    "             linewidth=3)\n",
    "# sns.lineplot(data=plot_df, x='epochs', y='val_loss', ax=ax, label='val loss', linewidth=3)\n",
    "sns.lineplot(data=plot_df, x='epochs', y='test_r2', ax=ax,\n",
    "             label='test_r2', linewidth=3)\n",
    "ax.set_ylabel('Loss or R2')\n",
    "ax.set_xlabel('Epochs')\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='18'); # for legend text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}